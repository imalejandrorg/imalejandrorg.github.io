---
output:
  bookdown::html_document2:
    fig.caption: yes
editor_options: 
  markdown: 
    wrap: sentence
---

# Probabilidad

```{r include=FALSE}
library(knitr)
library(bookdown)
library(tidyverse)
library(kableExtra)
library(ggthemes)
library(cowplot)
library(ggpubr)
library(latex2exp)
```

La **probabilidad** es una cantidad numérica que expresa qué tan factible es que ocurra un evento.
Normalmente es expresada como P(A), donde A expresa un evento aleatorio.
Siempre se encuentra entre un rango de 0 y 1, o expresado en porcentaje, entre 0% y 100%.
Existen distintas interpretaciones de la probabilidad como la **frecuentista** o la **bayesiana.** En este caso, aprenderemos el enfoque frecuentista como una forma de asignar una probabilidad mensurable a un evento, es decir, la ocurrencia de el evento a la larga.

## Probabilidad frecuentista

En este enfoque, la probabilidad de un evento se determina a través por el número de veces que el evento A ocurre en una serie de repeticiones indefinidamente largas.


\begin{equation}
P(A) = \frac{num_{A}}{num_{T}}
(\#eq:prob)
\end{equation}

Donde $num_{A}$ es el número de veces que se repite el evento A y $num_{T}$ es el número total de repeticiones.

## Reglas de probabilidad

Estas son algunas reglas que nos pueden servir para determinar la probabilidad de algunos eventos.

1.  La probabilidad de un evento A siempre se encuentra entre 0 y 1.

2.  La suma de probabilidades de los eventos tiene que ser igual a 1.

3.  La probabilidad de que el evento A no ocurra es $1 - P(A)$ y se denota con el símbolo $A_1^C$, indicando que es el complemento de $A_1$.

4.  Si dos eventos $A_1$ y $A_2$ son eventos que no ocurren en conjunto, entonces $P(A_1 \cup A_2) = P(A_1) + P(A_2)$.

5.  Para dos eventos que ocurren en conjunto, $P(A_1 \cup A_2) = P(A_1) + P(A_2) - P(A_1 \cap A_2)$.

+-----------------+--------------+---------------+--------------+------------+
|                 | Cabello Café | Cabello Negro | Cabello Rojo | Total      |
+=================+==============+===============+==============+============+
| **Ojos Cafés**  | 400          | 300           | 20           | 720        |
+-----------------+--------------+---------------+--------------+------------+
| **Ojos Azules** | 800          | 200           | 50           | 1050       |
+-----------------+--------------+---------------+--------------+------------+
| **Total**       | 1200         | 500           | 70           | 1770       |
+-----------------+--------------+---------------+--------------+------------+

En este caso, la probabilidad de que alguien tenga el cabello café (CC) o rojo (CR), $P(CC \cup CR) = P(CC) + P(CR) = 500/1770 + 70 / 1770$.

La probabilidad de que alguien tenga el cabello café $P(CC) = 1200/1770$.

La probabilidad de tener ojos azules $P(OA) = 1050/1770$.

La probabilidad de tener cabello negro y ojos azules $P(CN \cap OA)$ pueden ocurrir en conjunto ya que hay 200 personas con ojos azules y cabello negro.
Entonces $P(CN \cup OA) = P(CN) + P(OA) - P(CN \cap OA) = 500/1770 + 1050/1770 - 200/1770 = 1350/1770$.

La fórmula de **probabilidad condicional** es la siguiente:


\begin{equation}
P(A_1 | A_2) = \frac{P(A_1 \cap A_2)}{P(A_2)}
(\#eq:prob)
\end{equation}

6.  Si dos eventos, $A_1$ y $A_2$ son **independientes,** entonces $P(A_1 \cap A_2) = P(A_1)*P(A_2)$.

7.  Para cualquier evento $A_1$ y $A_2$, $P(A_1 \cap A_2) = P(A_1) * P(A_2 | A_1)$.

Por ejemplo, de la tabla anterior, ¿Cuál es la probabilidad de que una persona tenga cabello rojo y ojos cafés?
En este caso, $P(CR \cap OA) = P(CR) * P(OA | CR) = 70/1770*20/70 = 20/1770$.

8.  Para dos eventos cualesquiera, $A_1$ y $A_2$, $P(A_1) = P(A_2) * P(A_1 | A_2) + P(A_2^C) * P(A_1 | A_2^C )$ donde $A_2^C$ es el complemento de $A_2$, es decir $1 - A_2$.

## Curvas de densidad

Se utilizan para variables continuas.
Los **histogramas de frecuencias relativas** representan las proporciones de las observaciones en cada categoría, en lugar del total de observaciones.
Para variables continuas normalmente se utilizan clases muy estrechas para representar al histograma como una **curva de densidad.** La coordenada y de una curva de densidad representa la escala de densidad y a menudo las frecuencias relativas se representan como áreas debajo de la curva.
El área total bajo la curva debe ser igual a 1.

```{r echo=FALSE, message=TRUE, warning=TRUE, paged.print=FALSE, fig.cap = "Curva de densidad.", fig.align = "center"}
data <- data.frame("x" = rnorm(100000, mean = 0, sd = 1))
ggplot(data, aes(x = x)) +
  geom_density(size = 0.6) +
  ylab("") +
  xlab("") +
  scale_y_continuous(breaks = NULL) +
  theme_minimal_hgrid()
```

Una probabilidad para una variable continua equivale al área debajo de la curva de densidad entre dos puntos.

## Distribución binomial

La **distribución binomial** de una variable aleatoria es una distribución de probabilidad discreta, que cuenta el número de éxitos tras realizar $n$ veces un experimento, cada *intento* debe ser independiente entre sí.
La probabilidad de éxito es fija entre cada intento y se denota con la letra $p$.
Lo primero que se nos viene a la mente con este tipo de distribuciones son *lanzamientos de moneda*, sin embargo, el comportamiento de muchos genes también sigue una distribución binomial.

Normalmente, para que una variable aleatoria sea una **variable aleatoria binomial** se requieren los siguientes requisitos:

1.  **Resultados binarios:** Solamente existen dos posibilidades para cada intento (éxito o fracaso, cara o cruz, dominante o recesivo, muerto o vivo, niño o niña, etc.).

2.  **Intentos independientes:** Cada prueba o intento deben ser independientes del anterior.

3.  **El valor de** $n$ es fijo: Se sabe con antelación el número de pruebas $n$.

4.  **Mismo valor** $p$: En todos los casos, la probabilidad de éxito o fracaso no debe cambiar, es decir $p$ debe permanecer constante.

5.  **Resultados mutuamente excluyentes:** Es decir, no se puede tener éxito y fracaso al mismo tiempo.

6.  **Resultados colectivamente exhaustivos:** Al menos uno de los dos resultados debe de ocurrir.

Ahora bien, la **función de masa de probabilidad (PMF)** o fórmula de la distribución binomial se indica en la ecuación \@ref(eq:binom).


\begin{equation}
P(X = k) = {\binom{n}{k}}{p^{k}}{q^{n-k}}
(\#eq:binom)
\end{equation}

Donde:

-   $P(X = k)$ = probabilidad de obtener $k$ éxitos.

-   $\binom{n}{k}$ = coeficiente binomial, que se calcula con la fórmula \@ref(eq:binc).

-   $k$ = número de éxitos.

-   $p$ = probabilidad de éxito.

-   $q$ = probabilidad de fracaso.

-   $n$ = número de pruebas.


\begin{equation}
_{n}C_{k} = \binom{n}{k} = \frac{n!}{k!(n-k)!}
(\#eq:binc)
\end{equation}

### Media y desviación estándar de la distribución binomial

Se pueden calcular tanto la **media** como la **desviación estándar** de una distribución binomial.
La media es el número promedio de éxitos, y la desviación estándar es qué tanto se desvían de la media los valores.

La media de una distribución binomial se calcula con la ecuación \@ref(eq:meanbinom).


\begin{equation}
\mu = {n}{p}
(\#eq:meanbinom)
\end{equation}

En cuanto a la desviación estándar se calcula con la siguiente ecuación \@ref(eq:sdbinom).


\begin{equation}
\sigma = \sqrt{{n}{p}{(1-p)}} = \sqrt{{n}{p}{q}}
(\#eq:sdbinom)
\end{equation}

En caso de querer obtener la **varianza**, simplemente tomamos la ecuación \@ref(eq:sdbinom) sin aplicar la raíz cuadrada.

\begin{equation}
\sigma^2 = {n}{p}{(1-p)} = {n}{p}{q}
(\#eq:varbinom)
\end{equation}

### Distribución binomial en R

Para utilizar la distribución binomial en R es bastante sencillo.
Veamos el siguiente ejemplo.

> **Ejemplo:** Supongamos que analizamos a 5 individuos de una población en la que el 37% de las personas presentan un alelo mutante.
> Las probabilidades de las distintas configuraciones están dadas por la distribución binomial, en donde $n$ = 5 y $p$ = 0.37.
> ¿Cuál es la probabilidad de que exactamente 2 personas sean mutantes?

Antes de continuar, hay que saber que existen 4 distintos comandos de la función binomial:

\-`dbinom()` nos da un valor exacto de la distribución binomial en el punto indicado.

\-`pbinom()` nos da la probabilidad acumulada de un evento.

\-`qbinom()` toma el valor de probabilidad que le ponemos como primer argumento y nos da como regreso un número cuya probabilidad acumulada empate con el valor de la probabilidad.

\-`rbinom()` genera cierta cantidad de número aleatorios de acuerdo con la probabilidad y el número de pruebas realizadas.

En este caso, queremos conocer la probabilidad exacta de que 2 personas sean mutantes.
Para esto necesitamos la función `dbinom(x, size, prob)`, donde el argumento `x` equivale al número de éxitos, `size` al número de ensayos y `prob` a la probabilidad de éxito.

```{r echo=TRUE, message=FALSE, warning=FALSE, paged.print=FALSE}
dbinom(2, 5, 0.37)
```

> **Ejemplo**: En Estados Unidos, 85% de la población tiene sangre Rh positivo.
> Supongamos que tomamos 6 personas y contamos cuántos tiene Rh positivo.
> En este caso $Y$ representará cuántas personas tienen Rh positivo dentro del grupo de 6.
> ¿Cuál es la probabilidad de $Y$ = 4?
> ¿Y la probabilidad de que *al menos* 4 personas sean Rh positivo?
> ¿Y la probabilidad de que haya *al menos* 1 persona con Rh negativo?

En este caso debemos utilizar dos comandos, `dbinom()` y `pbinom()`, con uno calcularemos $Y = 4$ y con el otro $Y \ge 4$ (4 o más).
Como queremos obtener $P(Y \ge 4)$, entonces $P(Y \ge 4) = P(Y = 4) + P(Y=5) + P(Y=6)$, utilizamos el argumento `lower.tail` y lo ponemos en `FALSE` para indicar que vamos a trabajar con un valor mínimo de 4, para arriba.
Para el caso de *al menos* una persona con Rh negativo $P(Y < 6)$, tenemos dos opciones.
Usar `dbinom()` cinco veces para calcular el valor individual de $P(Y = 0) + P(Y = 1) + P(Y = 2) \cdots + P(Y = 5)$, o calcular $P(Y = 6)$ y restarle a 1 este valor.
Como vemos ambas operaciones dan los mismos resultados, pero es más sencilla la segunda.

```{r}
#Probabilidad de que 4 personas tengan Rh positivo.
dbinom(4, 6, 0.85)

#Probabilidad de que al menos 4 personas (pueden ser 4, 5 o 6) tengan Rh positivo.
pbinom(3, 6, 0.85, lower.tail = FALSE)

#Probabilidad de que al menos 1 persona tenga Rh positivo.
dbinom(0, 6, 0.85) + dbinom(1, 6, 0.85) + dbinom(2, 6, 0.85) + dbinom(3, 6, 0.85) + dbinom(4, 6, 0.85) + dbinom(5, 6, 0.85)
1 - dbinom(6, 6, 0.85)
```

## Distribución normal

Una **distribución normal** corresponde a una curva *en forma de campana*, con ciertas características específicas.
Se utiliza para representar la distribución de los valores de una variable $X$, de dos maneras distintas: (1) como una aproximación a un histograma basado en los valores muestreados de la variable $X$ o; (2) como una representación idealizada de la distribución poblacional de $X$.

Las curvas con distribución normal toman su forma por dos elementos muy importantes: la **media** $\mu$ y su **desviación estándar** $\sigma$.
Cuando se tiene una curva con distribución normal, se expresa de la siguiente manera $X \sim N(\mu, \sigma)$.
La fórmula de la distribución normal se encuentra en la ecuación \@ref(eq:norm).
No se trata de cualquier curva simétrica, si no de una curva simétrica *específica*.


\begin{equation}
f(x) = \frac{1}{{\sigma}{\sqrt{2 \pi}}}e^{-\frac{1}{2}(\frac{x-\mu}{\sigma})^2}
(\#eq:norm)
\end{equation}

La función $f(x)$ se conoce como **función de densidad de probabilidad (PDF)** y expresa la altura de la curva como una función de la posición en el eje horizontal.
El centro de una curva normal es $x = \mu$, los puntos de inflexión están en $x = \mu + \sigma$ y $x = \mu - \sigma$.
En principio la curva se extiende hasta el infinito, pero tres desviaciones estándar de la media hacia el valor negativo o positivo da como resultado valores demasiado pequeños.
El ancho y alto de una curva normal están determinados por la desviación estándar $\sigma$.

```{r echo=FALSE, message=FALSE, warning=FALSE, paged.print=FALSE, fig.align = "center", fig.cap = "Forma de la distribución normal."}
ggplot(data.frame(x = c(-4, 4)), aes(x = x)) +
  stat_function(fun = dnorm, size = 0.6) +
  scale_y_continuous(breaks = NULL) +
  ylab("") +
  xlab("") +
  scale_y_continuous(breaks = NULL) +
  scale_x_discrete(limit = c(-3:3)) +
  theme_minimal_hgrid()
```

Normalmente lo que nos interesa de una curva con distribución normal es el **área debajo de la curva**.
Para esto utilizamos la **escala estandarizada**, en el cual el valor del eje horizontal se denomina **valor Z**.
La escala de Z mide las desviaciones estándar a partir de la media, por ejemplo, $z = 1$ corresponde a una desviación estándar de la media.
Para transformar nuestros datos a la escala Z simplemente aplicamos la ecuación \@ref(eq:Z) y a la variable Z se le conoce como una variable de **distribución normal estándar**, ya que se encuentra estandarizada y no importa en que valor se encuentren los datos originales (kg, °C, cm, mmHg, etc.), la variable Z es *adimensional*.


\begin{equation}
Z = \frac{X - \mu}{\sigma}
(\#eq:Z)
\end{equation}

Una vez que nuestras variables se encuentran estandarizadas en el valor Z, podemos utilizar **tablas de Z** para realizar el cálculo debajo del área que corresponde al valor Z obtenido, aunque claro, también podemos hacer estos cálculos en `R`.
Hay que tener en cuenta que para una curva de distribución normal estándar:

-   68% del área se encuentra entre $\pm$ 1 distribución estándar.

-   95% del área se encuentra entre $\pm$ 2 distribuciones estándar.

-   99.7% del área se encuentra entre $\pm$ 3 distribuciones estándar.

### Distribución normal en R

De manera similar a la distribución binomial, en `R` encontramos comandos similares para la distribución normal, donde tenemos:

\-`dnorm()` nos da un valor de densidad normal en determinado punto (valor puntual de la función de densidad).

\-`pnorm()` nos da un valor de densidad normal acumulado hasta cierto punto (área debajo de la curva).

\-`qnorm()` toma el valor de densidad normal que le ponemos como primer argumento y nos da como regreso un número cuya densidad normal acumulada empate con el valor de densidad normal ingresado.

\-`rnorm()` genera cierta cantidad de número aleatorios de acuerdo al valor de densidad normal.

Intentemos resolver algunos ejercicios.

> **Ejemplo:** En una población de peces de la especie *Pomolobus aestivalis*, la longitud de los individuos sigue una distribución normal. La media de la longitud es de 54.0 mm, y la desviación estándar es de 4.5 mm^2^. ¿Qué porcentaje de los peces mide menos de 60 mm? ¿Qué porcentaje de los peces mide más de 51 mm? ¿Qué porcentaje de los peces miden entre 51 mm y 60 mm?

Para responder la primer pregunta, debemos transformar nuestros datos a valores Z, ya que se encuentran en mm. Para esto aplicamos la ecuación \@ref(eq:Z). En `R` es una operación relativamente sencilla de hacer.

```{r}
(60 - 54)/(4.5)
```

Como podemos ver, el valor Z de 60 mm es igual a 1.33. Lo siguiente sería encontrar *el área bajo la curva* que corresponda a este valor Z. Para esto utilizamos la función `pnorm()`.

```{r}
pnorm(1.33, 0, 1, lower.tail = TRUE)
```

```{r echo=FALSE, message=FALSE, warning=FALSE, paged.print=FALSE, fig.align='center', fig.cap="Curva de distribución normal con el área sombreada correspondiente a un valor Z = 1.33"}

ggplot(data.frame(x = c(-4, 4)), aes(x = x)) +
  stat_function(fun = dnorm, geom = "area", fill = "lightblue", xlim = c(-3, 1.33)) +
  stat_function(fun = dnorm, size = 0.6) +
  geom_vline(aes(xintercept = 1.33), linetype = "dashed", size = 0.35) +
  geom_text(aes(x = 2.2, label = "Área = 0.9082", y = 0.26), color = "red", vjust = -1.2, size = 4.5) +
  geom_text(aes(x = 2.25, label = "Z = 1.33", y = 0.26), color = "black", vjust = -3, size = 4.5) +
  xlim(-4, 4) +
  ylab("") +
  xlab("") +
  scale_y_continuous(breaks = NULL) +
  scale_x_discrete(limit = c(-3:3)) +
  theme_minimal_hgrid()
```


De nuevo, el argumento `lower.tail = TRUE` significa que encontrará la probabilidad acumulada de valores Z menores a 1.33 hasta 1.33. Como podemos ver la probabilidad de que un pez mida menos de 60 mm es de 90.82%.

Para la segunda pregunta, primero debemos encontrar el valor de Z correspondiente a 51 mm y después basta con cambiar el argumento `lower.tail` a `FALSE`. 

```{r}
(51 - 54)/(4.5)

pnorm(-0.67, 0, 1, lower.tail = FALSE)
```

Como podemos ver el resultado indica que 75.86% de los peces miden más de 51 mm. Bastante sencillo, ¿no?

```{r echo=FALSE, message=FALSE, warning=FALSE, paged.print=FALSE, fig.align='center', fig.cap="Curva de distribución normal con el área sombreada correspondiente a un valor Z = -0.67, pero partiendo desde el lado positivo de la curva."}
ggplot(data.frame(x = c(-4, 4)), aes(x = x)) +
  stat_function(fun = dnorm, geom = "area", fill = "lightblue", xlim = c(-0.67, 3)) +
  stat_function(fun = dnorm, size = 0.6) +
  geom_vline(aes(xintercept = -0.67), linetype = "dashed", size = 0.35) +
  geom_text(aes(x = -1.8, label = "Área = 0.7486", y = 0.26), color = "red", vjust = -1.2, size = 4.5) +
  geom_text(aes(x = -1.7, label = "Z = -0.67", y = 0.26), color = "black", vjust = -3, size = 4.5) +
  xlim(-4, 4) +
  ylab("") +
  xlab("") +
  scale_y_continuous(breaks = NULL) +
    scale_x_discrete(limit = c(-3:3)) +
  theme_minimal_hgrid()
```

Ahora para la última pregunta, simplemente calculamos la probabilidad acumulada hasta nuestro valor Z más grande, que en este caso corresponde a 1.33 y le restamos la probabilidad acumulada del valor Z más pequeño, que corresponde -0.67.

```{r}
pnorm(1.33, 0, 1) - pnorm(-0.67, 0, 1)
```

Como resultado obtenemos que el 65.68% de los peces se encuentran en longitudes de entre 51 mm y 60 mm. Si nos damos cuenta, la distribución normal puede, de cierta manera, interpretarse como una distribución de probabilidad continua.

```{r echo=FALSE, message=FALSE, warning=FALSE, paged.print=FALSE, fig.align='center', fig.cap="Curva de distribución normal con el área sombreada correspondiente a un intervalo entre Z = -0.67 y Z = 1.33."}
ggplot(data.frame(x = c(-4, 4)), aes(x = x)) +
  stat_function(fun = dnorm, geom = "area", fill = "lightblue", xlim = c(-0.67, 1.33)) +
  stat_function(fun = dnorm, size = 0.6) +
  geom_vline(aes(xintercept = -0.67), linetype = "dashed", size = 0.35) +
  geom_vline(aes(xintercept = 1.33), linetype = "dashed", size = 0.35) +
  geom_text(aes(x = -1.8, label = "Área = 0.6568", y = 0.26), color = "red", vjust = -1.2, size = 4.5) +
  geom_text(aes(x = -1.7, label = "Z = -0.67", y = 0.26), color = "black", vjust = -3, size = 4.5) +
  geom_text(aes(x = 1.9, label = "Z = 1.33", y = 0.26), color = "black", vjust = -3, size = 4.5) +
  xlim(-4, 4) +
  ylab("") +
  xlab("") +
  scale_y_continuous(breaks = NULL) +
    scale_x_discrete(limit = c(-3:3)) +
  theme_minimal_hgrid()
```


En algunas ocasiones, queremos encontrar el valor Z correspondiente a un área bajo la curva determinada, para este tipo de ocasiones, utilizamos la función `qnorm()`.

Del ejemplo anterior, supongamos que queremos encontrar el percentil 70 de la distribución de la longitud de los peces. Supongamos que este valor está representado por la variable $y$. En otras palabras, queremos encontrar el valor tal que el 70% de las longitudes de los peces son menores que $y$ y el 30% son mayores.

```{r}
qnorm(0.7, 0, 1)
```

Como podemos ver, el valor Z correspondiente es 0.5244. Ahora, utilizando la ecuación \@ref(eq:Z) podemos realizar un despeje muy sencillo y obtener la fórmula $y = Z* \sigma + \mu = 0.5244 * 4.5 + 54 = 56.3$. Esto quiere decir que 56.3 mm es el percentil 70 de la distribución de nuestros datos.

```{r echo=FALSE, message=FALSE, warning=FALSE, paged.print=FALSE, fig.align='center', fig.cap="Curva de distribución normal con el área sombreada correspondiente a un valor Z = -0.67, pero partiendo desde el lado positivo de la curva."}
ggplot(data.frame(x = c(-4, 4)), aes(x = x)) +
  stat_function(fun = dnorm, geom = "area", fill = "lightblue", xlim = c(0.5244, 3)) +
  stat_function(fun = dnorm, size = 0.6) +
  geom_vline(aes(xintercept = 0.5244), linetype = "dashed", size = 0.35) +
  geom_text(aes(x = 1.8, label = "Área = 0.3", y = 0.26), color = "red", vjust = -1.2, size = 4.5) +
  geom_text(aes(x = 2.18, label = "Z = 0.5244", y = 0.26), color = "black", vjust = -3, size = 4.5) +
  xlim(-4, 4) +
  ylab("") +
  xlab("") +
  scale_y_continuous(breaks = NULL) +
  scale_x_discrete(limit = c(-3:3)) +
  theme_minimal_hgrid()
```

### Pruebas de normalidad en R

Ya que muchos procedimientos estadísticos se basan en datos provenientes de una población con distribución normal, es importante saber si nuestros datos siguen está distribución.

Uno de los métodos más utilizados son los **gráficos cuantil-cuantil**, **gráficos Q-Q** o **Q-Q plot**. Veamos un ejemplo con datos de plantas que vienen incluidas en `ggplot2`. 

```{r}
data("PlantGrowth")
PlantGrowth
```

Como podemos ver, tenemos tres grupos, el control, tratamiento 1 y tratamiento 2. Vamos a enfocarnos solamente en los datos del grupo del tratamiento 1.

```{r}
Datos <- PlantGrowth %>% dplyr::select(starts_with("trt1"), weight)
Datos
```

Recordemos que podemos utilizar la librería `dplyr` para extraer ciertos datos de nuestras matrices de datos. Lo que haremos ahora es utilizar el gráfico Q-Q para comprar los datos de nuestra muestra con  unos datos teóricos que siguen una distribución normal. En caso de que nuestros datos se comporten de manera normal, deberíamos de tener casi una línea recta. Para esto necesitaremos la librería `ggpubr`.

```{r message=FALSE, warning=FALSE, paged.print=FALSE, fig.align='center', fig.cap="Gráfico Q-Q comparando una muestra de datos contra una muestra teórica con distribución normal."}
library(ggpubr)

ggqqplot(Datos$weight) +
  xlab("Teórico") +
  ylab("Muestra")
```

Otra opción sería realizar el gráfico de densidad, pero muchas veces es difícil identificar la forma de campana en cierto conjunto de datos, así que los gráficos Q-Q son de gran ayuda.

Otra opción para realizar una prueba de normalidad conocida como **prueba de Shapiro-Wilks** (aunque no se recomienda que para $n > 50$), pero es muy sensible a ligeras desviaciones de la normalidad, sobre todo con un tamaño de muestra grande. Es muy sencillo de realizar en `R`, simplemente utilizamos el comando `shapiro_test()` del paquete `rstatix`. En este caso, vamos a analizar si nuestros dos tratamientos y el control siguen una distribución normal. Para esto, vamos a hacer uso de la librería `dplyr`.

```{r}
library(rstatix)

PlantGrowth %>% group_by(group) %>% shapiro_test(weight) #Agrupamos los datos por tipo de tratamiento y hacemos la prueba a la variable "weight".
```

¿Cómo interpretamos los resultados y qué significa el valor-*p*? Bueno, ya que el control y los dos tratamientos tienen un valor-*p* > 0.05 decimos que se distribuyen de manera normal. Valores-*p* menores a 0.05 son indicativos fuertes de no-normalidad. Después de obtener un valor menor a 0.05, podríamos corroborar esto con un gráfico Q-Q o con un histograma para ver la forma de la distribución de nuestros datos.


## Distribución muestral

La variabilidad entre muestras aleatorias que provienen de una misma población se conoce como **variabilidad de muestreo**. Una distribución de probabilidad que caracteriza algún aspecto de la variabilidad de muestreo se conoce como **distribución muestral**. Usualmente los valores de una muestra se parecen a los de la población de la cuál provienen. Una distribución muestral nos indica qué tan cerca la resemblanza entre entre la muestra y la población es probable que sea.

Normalmente tomamos solamente una muestra aleatoria de una población. Pero para visualizar la distribución muestral, necesitamos realizar un **meta-estudio**, que consiste en repetir de manera indefinida, réplicas del mismo estudio. Por ejemplo, si un estudio consiste en extraer una muestra aleatoria de tamaño $n$ de una población, un meta-estudio consiste en repetir varias veces la extracción de una muestra de tamaño $n$ de una población. Por lo tanto, las probabilidades relativas de una muestra aleatoria se pueden interpretar como frecuencias relativas en un meta-estudio. Conocer la distribución muestral nos permite hacer afirmaciones de probabilidad de otras posibles muestras.

Una pregunta natural a realizar es, ¿Qué tan parecida es la media de la muestra $\overline{x}$ de la media de la población $\mu$? Aunque con una sola muestra no podemos responder esta pregunta, si pensamos en un modelo de muestreo aleatorio y tomamos la media muestral como una variable $\overline{X}$, podemos hacer ciertas inferencias. Reformulamos nuestra pregunta a ¿Qué tan cerca de $\mu$ es probable que este $\overline{X}$? Nuestra respuesta la encontramos en la **distribución muestral de $\overline{X}$**.

Tenemos que tener en cuenta que, en promedio, la **media de la distribución muestral** $\overline{X}$ equivale a la media de la población $\mu$. Esto se ve mejor en la fórmula \@ref(eq:samplemean).

\begin{equation}
\mu_{\overline{X}} = \mu
(\#eq:samplemean)
\end{equation}

La fórmula de la **desviación estándar de la muestra** es un poco menos intuitiva, aunque si se analiza de manera detallada tiene sentido.

\begin{equation}
\sigma_{\overline{X}} = \frac{\sigma}{\sqrt{n}}
(\#eq:samplesd)
\end{equation}

Mientras el tamaño de muestra incrementa, la desviación estándar de $\overline{X}$ disminuye. Es decir para muestras más grandes existe menos variación.

La **forma** está determinada por el tamaño de muestra y la naturaleza de la población. Si la población $X$ se distribuye de manera normal, entonces la distribución muestral de $\overline{X}$ será también normal, sin importar el tamaño de nuestra $n$. Además el **teorema del límite central** indica que si obtenemos una $n$ suficientemente grande, la distribución muestral de $\overline{X}$ será aproximadamente normal incluso para muestras cuya población $X$ no se distribuye de manera normal.

>**Ejemplo:** Supongamos que tenemos una población del Carbonerito Mexicano (*Poecile sclateri*) en la cuál el peso medio es de $\mu = 11 g$ y la desviación estándar $\sigma = 1.2 g$. Supongamos que tomamos una muestra aleatoria de seis aves. Dejemos que $\overline{x}$ represente la media del peso de las seis aves. Ya que sabemos que el peso de esta ave sigue una distribución normal en la población, también nuestras muestras seguirán una distribución normal.

Para este caso, la media y la desviación estándar de nuestra muestra serán las siguientes:

\begin{equation}
\mu_{\overline{X}} = \mu = 11 g\\
\sigma_{\overline{X}} = \frac{\sigma}{\sqrt{n}} = \frac{1.2}{\sqrt{6}} = 0.49 g
\end{equation}

En este caso $\mu_{\overline{X}} = 11 g$ y $\sigma_{\overline{X}} = 0.49 g$. De tal manera que, en promedio la media de la muestra será 11 g, sin embargo, el 68% de las veces $\overline{X}$ se encontrará entre $11g \pm 0.49g$ y el 95% de las veces se encontrará entre $11g \pm 0.98g$.

```{r echo=FALSE, message=FALSE, warning=FALSE, paged.print=FALSE, fig.align='center', fig.cap="Distribución muestral proveniente de una población de *Sclateri poecile*."}
ggplot(data.frame(x = c(9.53, 12.47)), aes(x = x)) +
  stat_function(fun = dnorm, args = list(mean = 11, sd = 0.49), size = 0.6) +
  xlim(9, 13) +
  ylab("") +
  xlab("Media de la muestra (g)") +
  scale_y_continuous(breaks = NULL) +
  scale_x_discrete(limit = c(9.53, 10.02, 10.51, 11, 11.49, 11.98, 12.47)) +
  theme_minimal_hgrid()
```

Esta distribución muestral expresa distintas posibilidades para los valores de $\overline{X}$. Supongamos que quisiéramos saber la probabilidad de que la media de una muestra de seis aves sea mayor a 11.5 g. Ya que nuestros datos son normales, podemos usar la transformación a valores Z para obtener nuestro resultado.

\begin{equation}
Z = \frac{\overline{x}-\mu_\overline{X}}{\sigma_{\overline{X}}} = \frac{11.5 - 11}{0.49} = 1.0204
\end{equation}

Ya que nuestro valor Z = 1.0204, usamos la función `pnorm()` para encontrar nuestra área bajo la curva.

```{r}
pnorm(1.0204, mean = 0, sd = 1, lower.tail = FALSE)
```

De hecho, ni siquiera es necesario realizar la transformación a valores Z en `R` ya que podemos modificar los parámetros de la función `pnorm()`.

```{r}
pnorm(11.5, mean = 11, sd = 0.49, lower.tail = FALSE)
```

Debido a que hemos redondeado la desviación estándar el valor es ligeramente distinto, pero en esencia el resultado es el mismo. Entonces podemos concluir que

\begin{equation}
P(\overline{X} > 11.5) = P(Z > 1.0204) = 0.1538 \approx 0.15
\end{equation}

```{r echo=FALSE, message=FALSE, warning=FALSE, paged.print=FALSE, fig.align='center', fig.cap="Probabilidad de que la media de una muestra de seis aves de la especie *Poecile sclateri* sea mayor a 11.5."}

ggplot(data.frame(x = c(9.53, 12.47)), aes(x = x)) +
  stat_function(fun = dnorm, args = list(mean = 11, sd = 0.49), geom = "area", fill = "lightblue", xlim = c(11.5,12.47)) +
  stat_function(fun = dnorm, args = list(mean = 11, sd = 0.49), size = 0.6) +
  geom_vline(aes(xintercept = 11.5), linetype = "dashed", size = 0.35) +
  geom_text(aes(x = 12, label = "Área = 0.1538", y = 0.4), color = "red", vjust = -0.8, size = 4.5) +
  geom_text(aes(x = 12.08, label = "Z = 1.0204", y = 0.4), color = "black", vjust = -2.6, size = 4.5) +
  xlim(9, 13) +
  ylab("") +
  xlab("Media de la muestra (g)") +
  scale_y_continuous(breaks = NULL) +
  scale_x_discrete(limit = c(9.53, 10.02, 10.51, 11, 11.49, 11.98, 12.47)) +
  theme_minimal_hgrid()
```

Si eligiéramos muchas muestras aleatorias provenientes de esta población cerca del 15% de las muestras tendrían una media mayor a 11.5 g. 

El **tamaño de la muestra** tiene un efecto directo sobre la forma de nuestra curva. Básicamente, muestras más grandes dan un $\sigma_\overline{X}$ menor, y por ende dan un menor error de muestreo. En seguida se muestran distintas gráficas con distintas $\mu_\overline{X}$. Para este caso hipotético, $\mu = 100, \sigma = 40$.

```{r echo=FALSE, message=FALSE, warning=FALSE, paged.print=FALSE, fig.align='center', fig.cap= "Cambios en la forma de la curva dependientes de la desviación estándar muestral."}

ggplot(data.frame(x = c(40, 160)), aes(x = x)) +
  stat_function(fun = dnorm, args = list(mean = 100, sd = 20), size = 0.6) +
  geom_text(aes(x = 150, y = 0.015, label = "n == 4"), parse = TRUE) +
  geom_text(aes(x = 150, y = 0.0135, label = "sigma[bar(X)] == 20"), parse = TRUE) +
  xlim(40, 160) +
  ylab("") +
  xlab("") +
  scale_y_continuous(breaks = NULL) +
  theme_minimal_hgrid()

ggplot(data.frame(x = c(40, 160)), aes(x = x)) +
  stat_function(fun = dnorm, args = list(mean = 100, sd = 10), size = 0.6) +
  geom_text(aes(x = 130, y = 0.015, label = "n == 16"), parse = TRUE) +
  geom_text(aes(x = 130, y = 0.012, label = "sigma[bar(X)] == 10"), parse = TRUE) +
  xlim(40, 160) +
  ylab("") +
  xlab("") +
  scale_y_continuous(breaks = NULL) +
  theme_minimal_hgrid()
```

Qué tan cerca esta $\overline{X}$ de $\mu$ depende del tamaño de la muestra $n$. La media de una muestra grande no necesariamente está más cerca a la media poblacional que la media de una muestra pequeña, pero existe mayor probabilidad de que lo este.

### Poblaciones, muestras y distribuciones muestrales

Una vez llegado a este punto puede que exista confusión entre los valores de una población, de una muestra y de una distribución muestral. Para esto aclaremos los siguientes puntos, en torno a una variable $X$.

En una **población**, los estadísticos descriptivos como la media y la desviación estándar se representan por los siguientes símbolos:

- $\mu$: media poblacional.   

- $\sigma$: desviación estándar poblacional.

En una **muestra**, los mismos estadísticos se representan por los siguientes símbolos:

- $\overline{x}$: media muestral.

- $s$: desviación estándar poblacional.

En una **distribución muestral** lo que nosotros hacemos es repetir un muestreo indefinidas veces (meta-estudio) y de cada muestreo extraer la media muestral $\overline{x}$. Lo que representa la distribución muestral es una distribución de medias, en lugar de observaciones individuales.

- $\mu_\overline{X}$: media de una distribución muestral.

- $\sigma_\overline{X}$: desviación estándar de una distribución muestral.