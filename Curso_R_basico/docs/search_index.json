[["index.html", "Curso de R básico Requisitos Descarga de R y RStudio", " Curso de R básico Requisitos Antes de comenzar con el curso es necesario que descarguemos ciertos programas entre ellos están incluidos R y RStudio. Descarga de R y RStudio Tenemos que distinguir entre R y RStudio. Como tal, R es el lenguaje de programación y RStudio es el ambiente de trabajo donde utilizamos este lenguaje de programación. Para descargar R podemos acceder al link de esta página. Una vez descargado e instalado R, procedemos a descargar RStudio desde esta página. Con estas herramientas podemos comenzar a trabajar en RStudio. "],["introducción-a-r.html", "Lección 1 Introducción a R 1.1 Interfaz 1.2 Creando nuestro primer proyecto 1.3 Objetos y variables 1.4 Instalar paquetes 1.5 Importar datos a R 1.6 Comentarios 1.7 Exportar datos 1.8 Pedir ayuda", " Lección 1 Introducción a R 1.1 Interfaz R Studio cuenta con 4 ventanas esenciales: La consola, que es donde se ejecutan los comandos y se visualizan los outputs de nuestros análisis. Los scripts, que corresponde a la ventana superior izquierda. No solamente se visualizan scripts, también se pueden trabajar otros formatos, aunque generalmente será donde nosotros escribamos los comandos que la consola va a ejecutar. El Envirnoment y el History que corresponden a la ventana superior derecha, aunque también cuenta con otras herramientas útiles como Build y Git que se utilizan en cuestiones más avanzadas. Los Files, Packages y Plots así como la ventana de Help que corresponden a la esquina inferior derecha. Básicamente estas son las 4 pestañas de esta ventana que estaremos utilizando. Figura 1.1: Interfaz de RStudio Adicionalmente, en la ventana de Tools -&gt; Global Options -&gt; Appearance podemos configurar los colores de nuestra interfaz, el tamaño de la fuente, el zoom, el tema del editor y la tipografía. Figura 1.2: Para abrir las opciones globales nos vamos a Tools -&gt; Global Options. Figura 1.3: Ventana de Appearance dentro de las Opciones Globales de RStudio Personalmente encuentro los colores oscuros más cómodos para trabajar, por eso elegí el tema Material. 1.2 Creando nuestro primer proyecto Una vez que nos familiarizamos con la interfaz, es momento de crear nuestro primer proyecto. Para esto, debemos dar clic en File -&gt; New Project para que nos aparezca una ventana como la siguiente. Figura 1.4: Ventana para crear un nuevo proyecto a partir de distintas opciones: Nuevo directorio, directorio existente y versión de control. Creamos un nuevo directorio que se guarda por defecto en la carpeta de Documentos de nuestro ordenador. SE RECOMIENDA NO DEJAR ESPACIOS EN EL NOMBRE. En su lugar, podemos utilizar guión bajo (_), el símbolo de menos (-) o un punto (.). Una vez creado nuestro proyecto, vamos a crear un nuevo archivo script tecleando Ctrl + Shift + N. Deberíamos tener un ambiente de trabajo más o menos así. Figura 1.5: Ventana con el ambiente de trabajo básico en RStudio. Otra recomendación es guardar nuestro script en la misma carpeta del proyecto, así como las bases de datos y demás archivos que vayamos a utilizar. Para saber cuál es la carpeta de nuestro proyecto, podemos teclear el siguiente comando en la consola. getwd() ## [1] &quot;O:/Tesis/R/Webpage/imalejandrorg.github.io/Curso_R_basico&quot; Para cambiar el directorio de trabajo, simplemente tecleamos en la consola la siguiente función. setwd(&quot;O:/Documentos/R_Basico&quot;) getwd() ## [1] &quot;O:/Documentos/R_Basico&quot; De esta manera cambiamos el directorio de nuestro proyecto actual y sabemos en qué carpeta ingresar nuestros archivos. Para guardar nuestro script simplemente hacemos clic en icono del disquete o con las teclas Ctrl + S. Recuerda que es recomendable guardar el script en la misma carpeta que nuestro proyecto. 1.3 Objetos y variables Para que un comando que queramos ejecutar permanezca almacenado, debemos asignar un nombre al resultado. La manera en la que R hace esto es a través de los símbolos &lt;- o = (cuyo atajo de escritura es Alt + -). Por ejemplo, si queremos realizar una suma de 5 + 5 pondríamos el siguiente código en la consola. 5 + 5 ## [1] 10 Sin embargo, si queremos almacenar este resultado necesitamos nombrar a una variable con este resultado. Por ejemplo, una variable llamada suma. suma &lt;- 5 + 5 suma ## [1] 10 Como podemos ver en ambos casos obtenemos los mismos resultados. Sin embargo, en el segundo caso encontraremos una variable llamada suma en nuestro Environment en el panel superior derecho. Figura 1.6: Panel Environment en el cual podremos encontrar las variables que vayamos generando. En caso de que estemos interesados en asignar valores categóricos, estos deben estar encomillados. letraA &lt;- &quot;A&quot; De nuevo se agregará esta variable a nuestro Environment. Figura 1.7: Tras definir la variable A esta se suma a la ventana de Environment. Para corroborar el tipo de dato que tenemos podemos utilizar la función class(x) donde x es el nombre de nuestra variable. class(letraA) ## [1] &quot;character&quot; class(suma) ## [1] &quot;numeric&quot; Para los análisis de datos de naturaleza biológica normalmente se trabaja con matrices de datos con múltiples caracteres tanto categóricos como cuantitativos, que en R corresponde a un data frame. Por ejemplo, los datos de una matriz llamada dune del paquete vegan corresponde a una matriz de datos. data(&quot;dune&quot;) class(dune) ## [1] &quot;data.frame&quot; Achimill Agrostol Airaprae Alopgeni Anthodor Bellpere Bromhord Chenalbu Cirsarve Comapalu Eleopalu Elymrepe Empenigr Hyporadi Juncarti Juncbufo Lolipere Planlanc Poaprat Poatriv Ranuflam Rumeacet Sagiproc Salirepe Scorautu Trifprat Trifrepe Vicilath Bracruta Callcusp 1 0 0 0 0 0 0 0 0 0 0 4 0 0 0 0 7 0 4 2 0 0 0 0 0 0 0 0 0 0 3 0 0 2 0 3 4 0 0 0 0 4 0 0 0 0 5 0 4 7 0 0 0 0 5 0 5 0 0 0 0 4 0 7 0 2 0 0 0 0 0 4 0 0 0 0 6 0 5 6 0 0 0 0 2 0 2 0 2 0 0 8 0 2 0 2 3 0 2 0 0 4 0 0 0 0 5 0 4 5 0 0 5 0 2 0 1 0 2 0 2 0 0 0 4 2 2 0 0 0 0 4 0 0 0 0 2 5 2 6 0 5 0 0 3 2 2 0 2 0 2 0 0 0 3 0 0 0 0 0 0 0 0 0 0 0 6 5 3 4 0 6 0 0 3 5 5 0 6 0 2 0 0 0 2 0 2 0 0 0 0 0 0 0 0 2 6 5 4 5 0 3 0 0 3 2 2 0 2 0 0 4 0 5 0 0 0 0 0 0 4 0 0 0 4 0 4 0 4 4 2 0 2 0 3 0 2 0 2 0 0 3 0 3 0 0 0 0 0 0 0 6 0 0 4 4 2 0 4 5 0 2 2 0 2 0 3 0 2 0 4 0 0 0 4 2 4 0 0 0 0 0 0 0 0 0 6 3 4 4 0 0 0 0 3 0 6 1 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0 0 7 3 4 0 0 0 2 0 5 0 3 2 4 0 0 4 0 8 0 0 0 0 0 0 0 0 0 0 0 4 0 0 0 4 0 2 4 0 2 0 3 0 4 0 0 5 0 5 0 0 0 1 0 0 0 0 0 0 0 3 0 0 2 9 2 0 2 0 2 0 2 0 0 0 0 4 0 0 0 0 0 0 0 2 4 0 0 0 0 0 0 0 0 0 2 0 0 0 2 0 6 0 0 4 0 4 0 0 0 0 0 0 0 2 5 0 0 0 3 0 0 0 0 0 2 0 0 0 2 0 1 0 4 0 0 7 0 4 0 0 0 0 0 0 8 0 0 0 3 0 0 0 0 2 2 0 0 0 0 0 0 0 4 3 2 0 2 0 4 0 0 0 0 0 0 0 0 2 0 0 0 2 1 0 0 0 0 0 2 0 0 0 0 0 0 0 0 0 0 2 0 0 0 0 0 0 0 0 0 0 2 3 3 0 0 0 0 3 5 0 2 1 6 0 0 0 3 0 4 0 0 0 0 0 0 0 2 5 0 0 0 0 0 0 0 0 3 3 6 0 2 0 3 0 0 5 0 0 0 0 0 0 0 0 4 0 0 0 4 0 0 0 0 0 4 0 0 5 2 0 0 0 4 3 1.4 Instalar paquetes Una de las herramientas más útiles de R es la instalación de paquetes. No tenemos la necesidad de escribir nuestras propias funciones cada que queramos realizar un análisis. En su lugar, bajamos e instalamos paquetes que nos sirven para ciertas funciones. Por ejemplo, uno de los paquetes más utilizados es el de vegan, útil para realizar análisis ecológicos. Para instalar los paquetes escribimos la siguiente función en la consola. install.packages(&quot;vegan&quot;) Tenemos que conocer el nombre exacto del paquete, en este caso, vegan está escrito con una v minúscula. Tenemos que entender que instalar un paquete no es lo mismo que llamarlo. Para esto último utilizamos la siguiente función. library(vegan) Vemos que en esta ocasión llamamos al paquete sin necesidad de poner el entrecomillado. En ocasiones la función install.package() puede arrojarnos error. Una alternativa es utilizar la ventana de Packages del panel inferior derecho. Figura 1.8: Ventana de Packages donde podremos encontrar los paquetes instalados. Damos clic en Install y escribimos el nombre del paquete. Figura 1.9: Al hacer clic sobre el botón Install en la ventana de Packages aparece esta pequeña ventana donde podremos buscar la librería deseada. Nos irán apareciendo opciones con el nombre del paquete de interés. Lo seleccionamos y damos clic en Install. Figura 1.10: En este caso, solo con escribir las primeras letras del paquete deseado se despliega una lista de librerías con nombres similares. 1.5 Importar datos a R La mayor parte del tiempo lo que queremos hacer es importar nuestros propios datos a R. En este caso utilizaremos las matrices de datos presentadas en el libro de Palacio et al. (2020). Utilizaremos una base de datos de aves del capítulo 6 del libro de Palacio et al. (2020). Descargar: Aves.txt Una vez que tenemos nuestra base de datos en la carpeta de nuestro proyecto, la importamos a través de la función read.table(). Aves &lt;- read.table(&quot;Aves.txt&quot;, header = TRUE) View(Aves) sitio estacion ambiente agebad amabra rupmag spimag chlluc colmel patpic cycguj elapar furruf geoaeq ictcay lepang mimsat molbon myimac myimon pacpol phastr pitsul poldum poomel rossoc sicfla siclut spocae synspi troaed turruf viroli zenaur zoncap 2 inv bosque 1 0 0 0 0 2 2 1 0 6 1 1 1 1 0 0 0 0 0 2 0 0 1 0 0 0 0 5 3 0 2 5 2 oto bosque 1 0 1 0 0 3 2 0 0 2 0 0 2 5 0 0 0 0 0 3 0 0 1 0 0 0 0 3 4 0 1 0 2 pri bosque 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0 1 0 0 0 0 0 0 0 0 4 0 1 6 2 ver bosque 1 0 0 0 0 3 0 0 1 6 0 0 4 0 0 0 0 1 1 6 0 0 0 0 0 0 0 1 1 0 8 2 3 inv arbustal 1 0 0 0 0 1 0 0 0 6 0 0 1 0 3 0 0 0 1 2 0 0 3 0 0 0 2 6 3 0 2 6 3 oto arbustal 0 0 0 0 0 0 0 0 0 8 0 0 2 0 0 0 0 0 1 3 0 0 3 0 0 0 0 3 6 0 1 1 3 pri arbustal 0 0 0 0 0 2 1 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3 2 0 4 3 3 ver arbustal 3 0 0 0 2 0 0 0 0 2 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 1 0 1 1 0 5 4 4 inv arbustal 8 0 0 10 0 0 0 0 0 1 2 2 1 0 1 0 14 0 2 1 0 2 0 0 0 0 1 3 1 0 4 6 4 oto arbustal 0 0 0 0 0 1 0 0 0 1 0 1 0 0 0 0 0 0 0 1 3 3 0 0 0 0 0 4 3 0 0 1 4 pri arbustal 0 0 0 0 0 0 0 0 0 0 3 0 0 0 2 0 0 0 1 2 0 0 0 0 4 0 0 4 1 0 1 4 4 ver arbustal 1 0 0 0 0 0 0 0 0 3 1 0 0 0 0 1 0 0 1 2 0 0 0 0 0 3 2 2 0 0 0 4 5 inv bosque 0 0 0 0 0 3 2 3 0 4 1 2 1 0 1 0 0 0 0 9 0 0 0 0 0 0 0 2 0 0 2 3 5 oto bosque 37 0 2 0 0 2 2 0 0 2 0 0 1 0 0 0 0 0 0 3 2 0 0 0 0 0 0 3 2 0 0 3 5 pri bosque 0 0 0 1 0 0 0 0 6 0 0 0 0 0 0 0 0 0 0 2 0 0 0 0 0 0 0 0 3 1 1 4 5 ver bosque 10 0 2 0 0 2 0 2 2 0 0 0 0 0 0 1 0 3 0 5 0 0 0 0 0 0 1 1 2 2 1 5 6 inv bosque 0 0 5 0 0 1 2 1 0 4 0 0 1 0 0 0 0 0 0 8 2 0 0 0 0 0 0 0 3 0 4 6 6 oto bosque 0 1 4 0 0 0 2 0 0 3 0 0 1 0 0 0 0 0 0 5 0 0 0 0 0 0 0 0 0 0 2 0 6 pri bosque 2 0 2 0 0 0 0 1 4 0 0 0 0 0 0 0 0 0 0 5 0 0 0 0 0 0 0 0 2 2 0 3 6 ver bosque 0 4 3 0 1 1 0 0 2 1 0 0 1 0 0 4 0 4 0 1 0 0 0 0 0 0 0 0 1 1 4 3 7 inv bosque 0 0 1 0 1 0 6 0 2 0 0 1 0 0 0 0 0 0 0 4 0 0 0 0 0 0 0 0 16 0 4 2 7 oto bosque 0 0 0 0 0 1 2 1 0 2 0 2 0 1 0 0 0 0 0 1 0 0 0 0 0 0 0 0 3 0 1 0 7 pri bosque 0 0 1 0 0 0 0 0 3 0 1 0 0 0 0 0 0 0 0 2 0 0 0 0 0 0 0 0 2 2 2 3 7 ver bosque 1 0 0 0 0 1 7 0 3 2 0 0 0 0 0 0 0 0 0 2 0 0 0 0 0 0 0 0 1 0 5 2 8 inv arbustal 0 0 0 0 0 0 0 2 0 1 1 0 0 2 2 0 0 0 0 3 0 0 1 2 2 0 0 1 1 0 3 9 8 oto arbustal 0 0 0 1 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 3 8 pri arbustal 0 0 0 0 0 0 0 0 1 0 1 0 0 0 0 0 0 0 0 4 0 0 0 4 2 1 0 1 0 0 0 8 8 ver arbustal 0 0 0 0 1 0 0 0 2 2 0 0 1 0 0 0 0 0 0 2 1 0 0 0 0 7 1 4 4 0 0 5 9 inv arbustal 0 0 0 0 0 4 0 0 0 0 0 0 0 0 1 0 3 0 0 0 1 1 0 0 4 0 0 5 1 0 2 13 9 oto arbustal 0 0 0 0 0 0 0 0 0 2 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 1 6 0 1 2 9 pri arbustal 0 0 0 0 0 0 0 0 0 0 3 0 0 0 1 0 0 0 0 2 0 0 0 0 5 0 0 0 1 0 0 6 9 ver arbustal 3 0 0 0 0 0 0 0 0 2 0 0 0 0 0 0 0 0 1 0 0 0 0 3 11 6 1 2 0 0 0 31 10 inv arbustal 0 0 0 1 0 0 0 0 0 0 1 0 2 0 8 0 0 0 2 4 0 0 0 0 3 0 1 0 1 0 1 14 10 oto arbustal 0 0 0 0 0 1 1 0 0 1 0 0 0 0 0 0 0 0 0 3 0 0 1 0 0 0 3 0 4 0 0 7 10 pri arbustal 0 0 0 0 0 0 0 0 0 2 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 11 10 ver arbustal 1 0 0 0 0 1 0 0 0 7 2 0 0 0 0 0 0 0 0 2 0 0 0 0 3 8 1 2 0 0 0 12 También podemos importar archivos .csv utilizando la función read.csv(). Aves_csv &lt;- read.csv(&quot;Aves.csv&quot;, header = TRUE) Así obtendríamos la misma tabla anterior. En caso de que queramos leer archivos excel, podemos hacerlo utilizando la librería readxl. En caso de ya tener instalado el paquete no es necesario que pongan la función install.packages(\"readxl\"), al ya tenerlo instalado yo omití esta parte poniéndole el símbolo de gato #. #install.packages(&quot;readxl&quot;) library(readxl) Aves_xlsx &lt;- read_excel(&quot;Aves.xlsx&quot;) Estas tres formas de importar los datos nos producen la misma matriz. Sin embargo podemos ver que la clase del archivo importado desde excel pertenece a una subclase de los archivos de tipo data frame llamada tibble. Sin embargo, se recomienda que se cambie el tipo de archivo a solamente data.frame. Para hacer esto podemos escribir el siguiente código Aves_xlsx &lt;- data.frame(Aves_xlsx) class(Aves_xlsx) ## [1] &quot;data.frame&quot; Básicamente le estamos diciendo a R que escriba un data frame con el mismo nombre que el data frame de subtipo tibble. De esta manera sobrescribimos el primer archivo y mantenemos limpio nuestro espacio de trabajo. También se puede importar desde el botón Import Dataset que aparece en la ventana de nuestro Environment. Figura 1.11: En el panel Environment podemos hacer clic sobre la ventanaImport Dataset para importar archivos de distintos formatos. De igual manera se recomienda copiar el código al script para saber exactamente qué hacemos. 1.6 Comentarios Algo muy útil que podemos agregar a nuestro código o script son los comentarios. Estos pueden ayudarnos a recordar qué realiza alguna línea de código o alguna función o argumento en particular. La manera en la que agregamos comentarios es con el símbolo de gato #. #Esta función sirve para importar archivos. Aves &lt;- read.table(&quot;Aves.txt&quot;, header = TRUE) Podemos ver como al poner un # antes de una línea de código esta cambia a color gris. Lo que sea que realice esta línea no será leída. 1.7 Exportar datos En algunas ocasiones queremos trabajar con otro software o queremos exportar nuestra tabla modificada a Excel. Para hacer esto, utilizamos la función write.table(), por ejemplo. #Filtramos nuestro archivo para tener solo las abundancias. Abun &lt;- Aves[,4:35] #Exportamos nuestro archivo en formato .csv write.table(Abun, file = &quot;Abundancias.csv&quot;, sep = &quot;,&quot;, row.names = FALSE) De esta manera omitimos los nombres de la columnas y de las filas, además especificamos que queremos un documento en formato .csv, y que la separación sea por comas. Si no especificamos la ruta, guardaremos nuestro archivo en nuestro directorio de trabajo, si queremos guardar el archivo en un directorio específico, tendremos que dar la ruta antes de poner el nombre. write.table(Abun, file = &quot;O:/Documentos/R_Basico/Lección_1/Abundancias.csv&quot;, sep = &quot;,&quot;, row.names = FALSE) Como podemos ver, el archivo fue escrito en la dirección indicada. Figura 1.12: Dirección a en la cual hemos decidido guardar el archivo Abundancias.csv. Figura 1.13: Archivo Abundancias.csv visto en Excel. 1.8 Pedir ayuda En caso de que no sepamos que argumentos van en alguna función, podemos pedir ayuda en la consola utilizando el símbolo de interrogación ? seguido de la función sobre la cuál tengamos una duda. ?specaccum Cuando nosotros escribimos esto, se abrirá la ventana de Help en el panel inferior derecho y nos mostrará una pequeña descripción de la función, así como sus usos y qué argumentos utilizar. En ocasiones incluso podemos encontrar ejemplos. Figura 1.14: Ventana de ayuda en la pestaña Help para la función que en este caso fue ?specaccum. "],["estadística-descriptiva.html", "Lección 2 Estadística descriptiva 2.1 Medidas de tendencia central 2.2 Medidas de dispersión 2.3 Medidas de posición relativa 2.4 El uso de la librería dplyr", " Lección 2 Estadística descriptiva En esta lección veremos las nociones básicas de estadística descriptiva haciendo uso de R y RStudio. Veremos medidas de tendencia central como la media, mediana y moda, medidas de dispersión como el rango o la desviación estándar y medidas de posición relativa como los valores Z. 2.1 Medidas de tendencia central Sirven para describir los valores del centro o valores medios de algún conjunto de datos. Las tres medidas de tendencia central más utilizadas son: media, mediana y moda. 2.1.1 Media Mide el promedio del valor de nuestros datos. Se calcula como la suma de los valores de las observaciones dividida entre el número de observaciones. Es representada como \\(\\overline{x}\\) (pronunciado \\(x\\) barra) para datos muestrales y \\(\\mu\\) (pronunciado mu) para datos poblacionales. Para obtener la media muestral utilizamos la ecuación (2.1) y para obtener la media poblacional se utiliza la ecuación (2.2). \\[\\begin{equation} \\overline{x} = \\frac{\\sum_{i = 1}^{n}{x_i}}{n} \\tag{2.1} \\end{equation}\\] \\[\\begin{equation} \\mu = \\frac{\\sum_{i = 1}^{N}{x_i}}{N} \\tag{2.2} \\end{equation}\\] Donde \\(\\sum_{i=1}^{n}{x_i}\\) y \\(\\sum_{i=1}^{N}{x_i}\\) representan la sumatoria de los valores de todas nuestras observaciones; \\(n\\) y \\(N\\) representan el número de valores de datos en una muestra y el número de valores totales en una población, respectivamente. Propiedades importantes de la media: No es un estadístico robusto, ya que es sensible a valores extremadamente grandes o pequeños. Utiliza todos los datos disponibles. Las medias muestrales (\\(\\overline{x}\\)) de una población tienden a variar menos que otras medidas. 2.1.1.1 Media en R El calculo de la media en R es algo sencillo. Para esto, utilizamos la función mean(). Ejemplo: En un experimento se administraron distintos tratamientos a un grupo de plantas y se midió el peso seco de todas ellas. Supongamos que queremos saber cuál fue la media global para todos los pesos (independientemente del tratamiento). Se utilizará la base de datos PlantGrowth para este ejemplo. data(&quot;PlantGrowth&quot;) Es importante que conozcamos los datos que incluye nuestra matriz. En este caso tenemos dos columnas, weight que hace referencia al peso seco y group que hace referencia al tratamiento. Para sacar la media global (independiente del tratamiento) simplemente escribimos la función mean() y hacemos referencia al peso con el símbolo $. mean(PlantGrowth$weight) ## [1] 5.073 Lo que nos arroja una media de 5.073. Ahora en caso de que quisiéramos sacar la media de por cada uno de los tratamientos tenemos que usar filtros y agrupaciones, para esto podemos utilizar la librería dplyr que viene incluida en el paquete de tidyverse. Vamos a usar el pipeline %&gt;% de dplyr para hacer varias operaciones en cadena: 1) Utilizamos la función group_by(group) para agrupar nuestros datos por el tipo de tratamiento; 2) utilizamos la función summarise(Media = mean(weight), n = n()) para indicar que saque la media de nuestro datos YA agrupados y el número de observaciones de cada grupo. library(tidyverse) resumen &lt;- PlantGrowth %&gt;% group_by(group) %&gt;% summarise(Media = mean(weight), n = n()) resumen ## # A tibble: 3 x 3 ## group Media n ## &lt;fct&gt; &lt;dbl&gt; &lt;int&gt; ## 1 ctrl 5.03 10 ## 2 trt1 4.66 10 ## 3 trt2 5.53 10 Al final obtenemos una matriz nueva con 3 columnas: tipo de tratamiento, media para cada tratamiento y número de observaciones para cada tipo de tratamiento. 2.1.2 Mediana Es el valor que se encuentra justo por la mitad de la distribución de nuestros datos. Se respresenta por el símbolo \\(\\tilde{x}\\) (pronunciado \\(x\\) tilde). \\[ 1, 2, 3, 4, 5 \\] Para el conjunto de datos anteriores la mediana es igual al número 3 que se encuentra justo por la mitad de los datos. En caso de tener un número par de observaciones, la mediana se puede calcular como un promedio de los dos valores centrales. Por ejemplo. \\[ 5, 6, 7, 8, 9, 10 \\] Para este caso nuestros valores centrales son 7 y 8. Estos simplemente se promedian. \\[ \\tilde{x} = \\frac{7 + 8}{2} = 7.5 \\] Propiedades importantes de la mediana: No cambia al añadir valores extremos por lo que se le considera un estadístico robusto. No utiliza todo el conjunto de datos. 2.1.2.1 Mediana en R Para el calculo de la mediana se utiliza la función median(). Ejemplo: Supongamos que con los datos del ejemplo anterior queremos ahora calcular la mediana global y la mediana para cada uno de nuestros tratamientos. median(PlantGrowth$weight) ## [1] 5.155 El valor obtenido es de 5.155. De manera similar podemos calcular la mediana para cada uno de nuestros datos utilizando dplyr. resumen &lt;- PlantGrowth %&gt;% group_by(group) %&gt;% summarise(Mediana = median(weight), n = n()) resumen ## # A tibble: 3 x 3 ## group Mediana n ## &lt;fct&gt; &lt;dbl&gt; &lt;int&gt; ## 1 ctrl 5.15 10 ## 2 trt1 4.55 10 ## 3 trt2 5.44 10 Si lo deséamos, podemos obtener la media y la mediana en una misma tabla. resumen &lt;- PlantGrowth %&gt;% group_by(group) %&gt;% summarise(Media = mean(weight), Mediana = median(weight), n = n()) resumen ## # A tibble: 3 x 4 ## group Media Mediana n ## &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; ## 1 ctrl 5.03 5.15 10 ## 2 trt1 4.66 4.55 10 ## 3 trt2 5.53 5.44 10 2.1.3 Moda Es el valor que más se repite en un conjunto de datos. Propiedades importantes de la moda: Se puede aplicar a datos cualitativos. Puede existir una moda (unimodal), dos modas (bimodal), múltiples modas (multimodal) o ninguna moda. Sin embargo, casi no se utiliza en estadística y por lo mismo no hablaremos más de ella. Como tal no existe una función en R que calcule la moda pero en varias páginas y foros como StackOverflow se pueden encontrar scripts para su calculo. 2.2 Medidas de dispersión A diferencia de los estadísticos de tendencia central, los estadísticos de dispersión se basan en cómo se distribuyen los datos y qué tan esparcidos están. Las medidas que veremos son: rango y desviación estándar. 2.2.1 Rango Se utiliza la función range(). Ejemplo: Queremos calcular el rango de las observaciones de la base de datos de plantas que utilizamos en la sección de medidas de tendencia central. range(PlantGrowth$weight) ## [1] 3.59 6.31 Esto nos da como resultado el valor máximo y el valor mínimo pero no nos dice cuál es el valor de nuestro rango. Podríamos hacer una operación simple para su calculo o utilizar las funciones max() y min(). max(PlantGrowth$weight) - min(PlantGrowth$weight) ## [1] 2.72 Como podemos ver el rango es de 2.72 ya que la diferencia entre 6.31 y 3.59 corresponde a este vakir, Para sacar el valor máximo y mínimo de cada uno de los tratamientos podemos utilizar el paquete de dplyr y la función group_by(). resumen &lt;- PlantGrowth %&gt;% group_by(group) %&gt;% summarise(Rango = max(weight) - min(weight), n = n()) resumen ## # A tibble: 3 x 3 ## group Rango n ## &lt;fct&gt; &lt;dbl&gt; &lt;int&gt; ## 1 ctrl 1.94 10 ## 2 trt1 2.44 10 ## 3 trt2 1.39 10 Igualmente, podemos agregar las medidas de tendencia central. resumen &lt;- PlantGrowth %&gt;% group_by(group) %&gt;% summarise(Rango = max(weight) - min(weight), Media = mean(weight), Mediana = median(weight), n = n()) resumen ## # A tibble: 3 x 5 ## group Rango Media Mediana n ## &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; ## 1 ctrl 1.94 5.03 5.15 10 ## 2 trt1 2.44 4.66 4.55 10 ## 3 trt2 1.39 5.53 5.44 10 2.2.2 Desviación estándar Mide qué tanto se desvían los valores de los datos de la media. Es representada por el símbolo \\(s\\) para datos muestrales y se calcula con la ecuación (2.3) y \\(\\sigma\\) para datos poblacionales y se calcula a partir de la ecuación (2.4). \\[\\begin{equation} s = \\sqrt{\\frac{\\sum_{i = 1}^{n}( {x_i-\\overline{x} ) ^2}}{n-1}} \\tag{2.3} \\end{equation}\\] \\[\\begin{equation} \\sigma = \\sqrt{\\frac{\\sum_{i = 1}^{n}( {x_i-\\overline{x} ) ^2}}{N}} \\tag{2.4} \\end{equation}\\] Donde \\(\\sum_{i = 1}^{n}({x_i-\\overline{x})^2}\\) es igual al valor cada observación menos la media, elevado al cuadrado, \\({n-1}\\) corresponde a los grados de libertad que tenemos para el cálculo de la desviación estándar y \\(N\\) es el número total de la población. Propiedades importantes de la desviación estándar: Nunca tiene un valor negativo. Mayor valor de \\(s\\) indica mayor variación en los datos. Puede aumentar considerablemente con valores atípicos, es decir, no es robusta. Las unidades de la desviación estándar son iguales a las unidades de los datos originales. 2.2.2.1 Desviación estándar en R Se utiliza la función sd(). Ejemplo: Queremos calcular la desviación estándar de las observaciones de la base de datos de plantas que utilizamos en la sección del rango. sd(PlantGrowth$weight) ## [1] 0.7011918 Obtenemos una desviación estándar de 0.7011918 para todo el conjunto de datos. En caso de querer calcular la desviación estándar de cada tratamiento utilizamos el paquete de dplyr y group_by(). resumen &lt;- PlantGrowth %&gt;% group_by(group) %&gt;% summarise(SD = sd(weight)) resumen ## # A tibble: 3 x 2 ## group SD ## &lt;fct&gt; &lt;dbl&gt; ## 1 ctrl 0.583 ## 2 trt1 0.794 ## 3 trt2 0.443 Y de nueva cuenta, podemos agregar el resto de estadísticos descriptivos que hemos visto hasta ahora. resumen &lt;- PlantGrowth %&gt;% group_by(group) %&gt;% summarise(SD = sd(weight), Rango = max(weight) - min(weight), Media = mean(weight), Mediana = median(weight), n = n()) resumen ## # A tibble: 3 x 6 ## group SD Rango Media Mediana n ## &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; ## 1 ctrl 0.583 1.94 5.03 5.15 10 ## 2 trt1 0.794 2.44 4.66 4.55 10 ## 3 trt2 0.443 1.39 5.53 5.44 10 En esta tabla podemos ver todos lo estadísticos que hemos calculado hasta ahora. Algo importante a tener en cuenta es que sd() calcula la desviación estandar muestral \\(s\\), no la poblacional \\(\\sigma\\). Para calcular \\(\\sigma\\) necesitamos multiplicar el resultado obtenido por \\(\\sqrt\\frac{N-1}{N}\\). sd(PlantGrowth$weight)*(sqrt((length(PlantGrowth$weight)-1)/length(PlantGrowth$weight))) ## [1] 0.6894063 En caso de que se utilice frecuentemente la \\(\\sigma\\) se puede definir la función de la siguiente manera. sd.p = function(x){sd(x)*sqrt((length(x)-1)/length(x))} Para calcular la varianza \\(s^2\\), simplemente elevamos al cuadrado el valor obtenido de la desviación estándar, se calcula con la siguiente ecuación (2.5). \\[\\begin{equation} Varianza = {s}^2 \\tag{2.5} \\end{equation}\\] sd(PlantGrowth$weight)^2 ## [1] 0.49167 Propiedades importantes de la varianza: Las unidades están elevadas al cuadrado. No es un estadístico robusto. Nunca tiene valores negativos. Otro estadístico que nos puede interesare es el coeficiente de variación que se obtiene a partir de la ecuación (2.6). \\[\\begin{equation} CV = \\frac{S}{\\overline{x}}*100 \\tag{2.6} \\end{equation}\\] Para su calculo simplemente utilizamos las funciones previamente aprendidas. sd(PlantGrowth$weight)/mean(PlantGrowth$weight)*100 ## [1] 13.82204 Nusetro resultado es 13.82204%. Este es el coeficiente de variación para todos los datos. De nueva cuenta, todo esto puede ser integrado en una sola tabla con la función summarise()del paquete dplyr. resumen &lt;- resumen &lt;- PlantGrowth %&gt;% group_by(group) %&gt;% summarise(CV = sd(weight)/mean(weight)*100, Varianza = sd(weight)^2, SD = sd(weight), Rango = max(weight) - min(weight), Media = mean(weight), Mediana = median(weight), n = n()) resumen ## # A tibble: 3 x 8 ## group CV Varianza SD Rango Media Mediana n ## &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; ## 1 ctrl 11.6 0.340 0.583 1.94 5.03 5.15 10 ## 2 trt1 17.0 0.630 0.794 2.44 4.66 4.55 10 ## 3 trt2 8.01 0.196 0.443 1.39 5.53 5.44 10 2.3 Medidas de posición relativa Indican la posición relativa de los datos con respecto de los demás datos. Algunas de las más usadas son: valores Z, frecuencia absoluta, frecuencia relativa y cuartiles. 2.3.1 Valores Z Se obtienen al estandarizar los valores de nuestros datos. Es el número de desviaciones estándar a las que se encuentra un valor dado. Se puede calcular el valor z muestral y poblacional con las ecuaciones (2.7) y (2.8). \\[\\begin{equation} Z = \\frac{X - \\overline{x}}{s} \\tag{2.7} \\end{equation}\\] \\[\\begin{equation} Z = \\frac{X - \\mu}{\\sigma} \\tag{2.8} \\end{equation}\\] Propiedades importantes de los valores Z: No tienen unidades de medida. Si un valor es menor que la media, su valor Z será negativo. 2.3.1.1 Valores Z en R Para el calculo del valor Z de algún número simplemente le extraemos la media \\(\\overline{x}\\) y lo dividimos entre la desviación estándar \\(s\\). Podemos utilizar la función de mutate() del paquete de dplyr para agregar una nueva sección a nuestra base de datos de PlantGrowth. Al utilizar la función group_by() antes de la función mutate() nos aseguramos de que los valores Z se calculen con la media y la desviación estándar de cada tratamiento y no con la media y desviación estándar resultante de todo el conjunto de datos. PlantGrowth &lt;- PlantGrowth %&gt;% group_by(group) %&gt;% mutate(Z = (weight - mean(weight)/sd(weight))) PlantGrowth ## # A tibble: 30 x 3 ## # Groups: group [3] ## weight group Z ## &lt;dbl&gt; &lt;fct&gt; &lt;dbl&gt; ## 1 4.17 ctrl -4.46 ## 2 5.58 ctrl -3.05 ## 3 5.18 ctrl -3.45 ## 4 6.11 ctrl -2.52 ## 5 4.5 ctrl -4.13 ## 6 4.61 ctrl -4.02 ## 7 5.17 ctrl -3.46 ## 8 4.53 ctrl -4.10 ## 9 5.33 ctrl -3.30 ## 10 5.14 ctrl -3.49 ## # ... with 20 more rows 2.3.2 Frecuencia absoluta Es el valor total de los datos observados. Por ejemplo, para los tratamientos de la base de datos de PlantGrowth, podemos ver que cada tratamiento tuvo 10 plantas en observación. La frecuencia de cada tratamiento es por ende de 10. Otra situación es el número de veces que cada peso aparece. Por ejemplo, el valor de 4.17 aparece dos veces en los datos, por lo tanto su frecuencia absoluta es de 2. 2.3.2.1 Frecuencia absoluta en R Utilizamos la función table() para su calculo. table(PlantGrowth$group) ## ## ctrl trt1 trt2 ## 10 10 10 Aquí se puede ver que cada tratamiento se realizo con 10 plantas. Ahora si quisieramos ver en los pesos cuántas veces se repite cada valor, lo haríamos por la variable de weight. table(PlantGrowth$weight) ## ## 3.59 3.83 4.17 4.32 4.41 4.5 4.53 4.61 4.69 4.81 4.89 4.92 5.12 5.14 5.17 5.18 ## 1 1 2 1 1 1 1 1 1 1 1 1 1 1 1 1 ## 5.26 5.29 5.33 5.37 5.5 5.54 5.58 5.8 5.87 6.03 6.11 6.15 6.31 ## 1 1 1 1 1 1 1 1 1 1 1 1 1 Y como podemos confirmar, el valor 4.17 es el único que se repite 2 veces. Aparecen los valores de cada uno de nuestros datos y la frecuencia con la que se repiten. En este caso obtuvimos la frecuencia de cada uno de los tratamientos. Como podemos ver, \\(N = 30\\) y cada tratamiento esta compuesto por 10 observaciones. Otra opción es el uso de la función n() junto con group_by(). De hecho, hemos incluido este valor en nuestra tabla de resumen desde un inicio. resumen &lt;- PlantGrowth %&gt;% group_by(group) %&gt;% summarise(CV = sd(weight)/mean(weight)*100, Varianza = sd(weight)^2, SD = sd(weight), Rango = max(weight) - min(weight), Media = mean(weight), Mediana = median(weight), Frec_Absoluta = n()) resumen ## # A tibble: 3 x 8 ## group CV Varianza SD Rango Media Mediana Frec_Absoluta ## &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; ## 1 ctrl 11.6 0.340 0.583 1.94 5.03 5.15 10 ## 2 trt1 17.0 0.630 0.794 2.44 4.66 4.55 10 ## 3 trt2 8.01 0.196 0.443 1.39 5.53 5.44 10 2.3.3 Frecuencia relativa Es el valor relativo de cada dato respecto del total como se observa en la ecuación (2.9). Si se le multiplica por 100% se obtiene su valor en porcentaje. \\[\\begin{equation} f_i = \\frac{n_i}{N} \\tag{2.9} \\end{equation}\\] Donde \\(f_i\\) es la frecuencia de la i-ésima observación y \\(n_i\\) es el número de observaciones en la i-ésima observación. 2.3.3.1 Frecuencia relativa en R En este caso utilizamos la función prop.table(). frec &lt;- table(PlantGrowth$group) prop.table(frec) ## ## ctrl trt1 trt2 ## 0.3333333 0.3333333 0.3333333 En este caso obtenemos la frecuencia relativa de cada una de las observaciones de los tratamientos. Como cada una era 10 y \\(N = 30\\) cada una tiene una frecuencia relativa de 0.3333 o 33.33%. Ahora las frecuencias relativas para cada uno de nuestros pesos sería de la siguiente manera. frec2 &lt;- table(PlantGrowth$weight) prop.table(frec2) ## ## 3.59 3.83 4.17 4.32 4.41 4.5 4.53 ## 0.03333333 0.03333333 0.06666667 0.03333333 0.03333333 0.03333333 0.03333333 ## 4.61 4.69 4.81 4.89 4.92 5.12 5.14 ## 0.03333333 0.03333333 0.03333333 0.03333333 0.03333333 0.03333333 0.03333333 ## 5.17 5.18 5.26 5.29 5.33 5.37 5.5 ## 0.03333333 0.03333333 0.03333333 0.03333333 0.03333333 0.03333333 0.03333333 ## 5.54 5.58 5.8 5.87 6.03 6.11 6.15 ## 0.03333333 0.03333333 0.03333333 0.03333333 0.03333333 0.03333333 0.03333333 ## 6.31 ## 0.03333333 Y como podemos ver, la frecuencia del valor 4.17 es el doble que las del resto, lo que quiere decir que este valor se repite 2 veces. De nueva cuenta, otra opción es el uso de la función mutate() para agregar una sección de frecuencia relativa a nuestra tabla con los estadísticos descriptivos pasados. resumen &lt;- resumen %&gt;% mutate(Frec_Relativa = Frec_Absoluta / sum(Frec_Absoluta)) resumen ## # A tibble: 3 x 9 ## group CV Varianza SD Rango Media Mediana Frec_Absoluta Frec_Relativa ## &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; ## 1 ctrl 11.6 0.340 0.583 1.94 5.03 5.15 10 0.333 ## 2 trt1 17.0 0.630 0.794 2.44 4.66 4.55 10 0.333 ## 3 trt2 8.01 0.196 0.443 1.39 5.53 5.44 10 0.333 2.3.4 Cuartiles Son medidas que dividen el conjunto de datos en cuatro grupos, con 25% de los valores en cada uno de ellos. Se representan por los símbolos \\(Q_1\\), \\(Q_2\\) y \\(Q_3\\), para el primer, segundo y tercer cuartil. \\(Q_2\\) es equivalente a la mediana. Existen otros estadísticos que pueden ser calculados a partir de los cuartiles como el Rango Intercuartil (\\(RIQ\\)) como se indica en la siguiente fórmula. \\begin{equation} RIQ = Q_3 - Q_1 (\\#eq:RIQ) \\end{equation} 2.3.4.1 Cuartiles en R Para el calculo de los cuartiles en R podemos utilizar la función summary() que viene en los paquetes básicos de R. summary(PlantGrowth) ## weight group Z ## Min. :3.590 ctrl:10 Min. :-7.5661 ## 1st Qu.:4.550 trt1:10 1st Qu.:-6.5986 ## Median :5.155 trt2:10 Median :-3.4749 ## Mean :5.073 Mean :-3.9232 ## 3rd Qu.:5.530 3rd Qu.:-1.7877 ## Max. :6.310 Max. : 0.1573 Como podemos ver, esta función nos da los valores mínimos, máximos, el \\(Q_1\\), \\(Q_3\\), la mediana y la media. Los cinco números utilizados para generar gráfica de caja se encuentran en estos datos. Para sacar estos valores por cada uno de los datos podemos utilizar la función filter() del paquete dplyr. ctrl &lt;- PlantGrowth %&gt;% dplyr::filter(group == &quot;ctrl&quot;) trt1 &lt;- PlantGrowth %&gt;% dplyr::filter(group == &quot;trt1&quot;) trt2 &lt;- PlantGrowth %&gt;% dplyr::filter(group == &quot;trt2&quot;) summary(ctrl) ## weight group Z ## Min. :4.170 ctrl:10 Min. :-4.460 ## 1st Qu.:4.550 trt1: 0 1st Qu.:-4.080 ## Median :5.155 trt2: 0 Median :-3.475 ## Mean :5.032 Mean :-3.598 ## 3rd Qu.:5.293 3rd Qu.:-3.337 ## Max. :6.110 Max. :-2.520 summary(trt1) ## weight group Z ## Min. :3.590 ctrl: 0 Min. :-2.2827 ## 1st Qu.:4.207 trt1:10 1st Qu.:-1.6652 ## Median :4.550 trt2: 0 Median :-1.3227 ## Mean :4.661 Mean :-1.2117 ## 3rd Qu.:4.870 3rd Qu.:-1.0027 ## Max. :6.030 Max. : 0.1573 summary(trt2) ## weight group Z ## Min. :4.920 ctrl: 0 Min. :-7.566 ## 1st Qu.:5.268 trt1: 0 1st Qu.:-7.219 ## Median :5.435 trt2:10 Median :-7.051 ## Mean :5.526 Mean :-6.960 ## 3rd Qu.:5.735 3rd Qu.:-6.751 ## Max. :6.310 Max. :-6.176 2.3.4.2 Valores atípicos Son aquellos valores que se encuentran por encima de \\(Q_3 + 1.5 * RIQ\\) o debajo de \\(Q_1 - 1.5 * RIQ\\). Con estos valores se pueden realizar gráficas de cajas modificadas. Para realizar estas gráficas utilizaremos la paquetería ggplot2 que está incluido en el paquete tidyverse. library(ggplot2) Utilizamos la función ggplot() + geom_boxplot. ggplot(PlantGrowth) + geom_boxplot(aes(group, weight, fill = group)) + xlab(&quot;Grupo&quot;) + ylab(&quot;Peso&quot;) + scale_fill_discrete(name = &quot;Tratamientos&quot;, labels = c(&quot;Control&quot;, &quot;Tratamiento 1&quot;, &quot;Tratamiento 2&quot;)) + theme_classic() Figura 2.1: Gráfica de cajas modificada para la identificación de valores atípicos. Como podemos ver en la gráfica, el tratamiento 1 presenta dos valores atípicos que están por encima del valor \\(Q_3 + 1.5 * IQR\\). 2.4 El uso de la librería dplyr A lo largo de todo esta lección hemos hecho uso de la librería dplyr. Este paquete está incluido dentro de la librería tidyverse. library(tidyverse) La mayoría de estas funciones las hemos estado utilizando a lo largo de esta sección aunque puede que haya algunas que no hayamos aplicado. Las funciones más importantes de este paquete son: group_by: agrupa datos. summarize o summarise: resumen de datos agrupados. filter: encuentra filas con ciertas condiciones. select: junto a starts_with, ends_with o contains mutate: genera nuevas variables. %&gt;%: pipeline. arrange: ordena. Con la función select podemos elegir unas cuantas variables para trabajar. Esto es útil para bases de datos con muchas variables como matrices de abundancias. resumen %&gt;% dplyr::select(group, Media, SD) ## # A tibble: 3 x 3 ## group Media SD ## &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 ctrl 5.03 0.583 ## 2 trt1 4.66 0.794 ## 3 trt2 5.53 0.443 En este caso, solamente nos interesa trabajar con la media y la desviación estándar de nuestros tratamientos. Otra forma de seleccionar las variables es con el argumento starts_with(), por ejemplo, para la selección de los datos del grupo control. PlantGrowth %&gt;% dplyr::select(starts_with(&quot;ctrl&quot;), weight) ## Adding missing grouping variables: `group` ## # A tibble: 30 x 2 ## # Groups: group [3] ## group weight ## &lt;fct&gt; &lt;dbl&gt; ## 1 ctrl 4.17 ## 2 ctrl 5.58 ## 3 ctrl 5.18 ## 4 ctrl 6.11 ## 5 ctrl 4.5 ## 6 ctrl 4.61 ## 7 ctrl 5.17 ## 8 ctrl 4.53 ## 9 ctrl 5.33 ## 10 ctrl 5.14 ## # ... with 20 more rows Como podemos ver, esto despliega los datos de peso seco solamente del grupo de control. También podemos ordenar los datos utilizando la función arrange(). Es importante escribir el argumento .by_group = TRUE para que la función arrange() respete el agrupamiento que hicimos por tratamientos antes del pipeline %&gt;%. PlantGrowth &lt;- PlantGrowth %&gt;% group_by(group) %&gt;% arrange(desc(weight), .by_group = TRUE) "],["probabilidad.html", "Lección 3 Probabilidad 3.1 Probabilidad frecuentista 3.2 Reglas de probabilidad 3.3 Curvas de densidad 3.4 Distribución binomial 3.5 Distribución normal 3.6 Distribución muestral", " Lección 3 Probabilidad La probabilidad es una cantidad numérica que expresa qué tan factible es que ocurra un evento. Normalmente es expresada como P(A), donde A expresa un evento aleatorio. Siempre se encuentra entre un rango de 0 y 1, o expresado en porcentaje, entre 0% y 100%. Existen distintas interpretaciones de la probabilidad como la frecuentista o la bayesiana. En este caso, aprenderemos el enfoque frecuentista como una forma de asignar una probabilidad mensurable a un evento, es decir, la ocurrencia de el evento a la larga. 3.1 Probabilidad frecuentista En este enfoque, la probabilidad de un evento se determina a través por el número de veces que el evento A ocurre en una serie de repeticiones indefinidamente largas. \\[\\begin{equation} P(A) = \\frac{num_{A}}{num_{T}} \\tag{3.1} \\end{equation}\\] Donde \\(num_{A}\\) es el número de veces que se repite el evento A y \\(num_{T}\\) es el número total de repeticiones. 3.2 Reglas de probabilidad Estas son algunas reglas que nos pueden servir para determinar la probabilidad de algunos eventos. La probabilidad de un evento A siempre se encuentra entre 0 y 1. La suma de probabilidades de los eventos tiene que ser igual a 1. La probabilidad de que el evento A no ocurra es \\(1 - P(A)\\) y se denota con el símbolo \\(A_1^C\\), indicando que es el complemento de \\(A_1\\). Si dos eventos \\(A_1\\) y \\(A_2\\) son eventos que no ocurren en conjunto, entonces \\(P(A_1 \\cup A_2) = P(A_1) + P(A_2)\\). Para dos eventos que ocurren en conjunto, \\(P(A_1 \\cup A_2) = P(A_1) + P(A_2) - P(A_1 \\cap A_2)\\). Cabello Café Cabello Negro Cabello Rojo Total Ojos Cafés 400 300 20 720 Ojos Azules 800 200 50 1050 Total 1200 500 70 1770 En este caso, la probabilidad de que alguien tenga el cabello café (CC) o rojo (CR), \\(P(CC \\cup CR) = P(CC) + P(CR) = 500/1770 + 70 / 1770\\). La probabilidad de que alguien tenga el cabello café \\(P(CC) = 1200/1770\\). La probabilidad de tener ojos azules \\(P(OA) = 1050/1770\\). La probabilidad de tener cabello negro y ojos azules \\(P(CN \\cap OA)\\) pueden ocurrir en conjunto ya que hay 200 personas con ojos azules y cabello negro. Entonces \\(P(CN \\cup OA) = P(CN) + P(OA) - P(CN \\cap OA) = 500/1770 + 1050/1770 - 200/1770 = 1350/1770\\). La fórmula de probabilidad condicional es la siguiente: \\[\\begin{equation} P(A_1 | A_2) = \\frac{P(A_1 \\cap A_2)}{P(A_2)} \\tag{3.1} \\end{equation}\\] Si dos eventos, \\(A_1\\) y \\(A_2\\) son independientes, entonces \\(P(A_1 \\cap A_2) = P(A_1)*P(A_2)\\). Para cualquier evento \\(A_1\\) y \\(A_2\\), \\(P(A_1 \\cap A_2) = P(A_1) * P(A_2 | A_1)\\). Por ejemplo, de la tabla anterior, ¿Cuál es la probabilidad de que una persona tenga cabello rojo y ojos cafés? En este caso, \\(P(CR \\cap OA) = P(CR) * P(OA | CR) = 70/1770*20/70 = 20/1770\\). Para dos eventos cualesquiera, \\(A_1\\) y \\(A_2\\), \\(P(A_1) = P(A_2) * P(A_1 | A_2) + P(A_2^C) * P(A_1 | A_2^C )\\) donde \\(A_2^C\\) es el complemento de \\(A_2\\), es decir \\(1 - A_2\\). 3.3 Curvas de densidad Se utilizan para variables continuas. Los histogramas de frecuencias relativas representan las proporciones de las observaciones en cada categoría, en lugar del total de observaciones. Para variables continuas normalmente se utilizan clases muy estrechas para representar al histograma como una curva de densidad. La coordenada y de una curva de densidad representa la escala de densidad y a menudo las frecuencias relativas se representan como áreas debajo de la curva. El área total bajo la curva debe ser igual a 1. Figura 3.1: Curva de densidad. Una probabilidad para una variable continua equivale al área debajo de la curva de densidad entre dos puntos. 3.4 Distribución binomial La distribución binomial de una variable aleatoria es una distribución de probabilidad discreta, que cuenta el número de éxitos tras realizar \\(n\\) veces un experimento, cada intento debe ser independiente entre sí. La probabilidad de éxito es fija entre cada intento y se denota con la letra \\(p\\). Lo primero que se nos viene a la mente con este tipo de distribuciones son lanzamientos de moneda, sin embargo, el comportamiento de muchos genes también sigue una distribución binomial. Normalmente, para que una variable aleatoria sea una variable aleatoria binomial se requieren los siguientes requisitos: Resultados binarios: Solamente existen dos posibilidades para cada intento (éxito o fracaso, cara o cruz, dominante o recesivo, muerto o vivo, niño o niña, etc.). Intentos independientes: Cada prueba o intento deben ser independientes del anterior. El valor de \\(n\\) es fijo: Se sabe con antelación el número de pruebas \\(n\\). Mismo valor \\(p\\): En todos los casos, la probabilidad de éxito o fracaso no debe cambiar, es decir \\(p\\) debe permanecer constante. Resultados mutuamente excluyentes: Es decir, no se puede tener éxito y fracaso al mismo tiempo. Resultados colectivamente exhaustivos: Al menos uno de los dos resultados debe de ocurrir. Ahora bien, la función de masa de probabilidad (PMF) o fórmula de la distribución binomial se indica en la ecuación (3.2). \\[\\begin{equation} P(X = k) = {\\binom{n}{k}}{p^{k}}{q^{n-k}} \\tag{3.2} \\end{equation}\\] Donde: \\(P(X = k)\\) = probabilidad de obtener \\(k\\) éxitos. \\(\\binom{n}{k}\\) = coeficiente binomial, que se calcula con la fórmula (3.3). \\(k\\) = número de éxitos. \\(p\\) = probabilidad de éxito. \\(q\\) = probabilidad de fracaso. \\(n\\) = número de pruebas. \\[\\begin{equation} _{n}C_{k} = \\binom{n}{k} = \\frac{n!}{k!(n-k)!} \\tag{3.3} \\end{equation}\\] 3.4.1 Media y desviación estándar de la distribución binomial Se pueden calcular tanto la media como la desviación estándar de una distribución binomial. La media es el número promedio de éxitos, y la desviación estándar es qué tanto se desvían de la media los valores. La media de una distribución binomial se calcula con la ecuación (3.4). \\[\\begin{equation} \\mu = {n}{p} \\tag{3.4} \\end{equation}\\] En cuanto a la desviación estándar se calcula con la siguiente ecuación (3.5). \\[\\begin{equation} \\sigma = \\sqrt{{n}{p}{(1-p)}} = \\sqrt{{n}{p}{q}} \\tag{3.5} \\end{equation}\\] En caso de querer obtener la varianza, simplemente tomamos la ecuación (3.5) sin aplicar la raíz cuadrada. \\[\\begin{equation} \\sigma^2 = {n}{p}{(1-p)} = {n}{p}{q} \\tag{3.6} \\end{equation}\\] 3.4.2 Distribución binomial en R Para utilizar la distribución binomial en R es bastante sencillo. Veamos el siguiente ejemplo. Ejemplo: Supongamos que analizamos a 5 individuos de una población en la que el 37% de las personas presentan un alelo mutante. Las probabilidades de las distintas configuraciones están dadas por la distribución binomial, en donde \\(n\\) = 5 y \\(p\\) = 0.37. ¿Cuál es la probabilidad de que exactamente 2 personas sean mutantes? Antes de continuar, hay que saber que existen 4 distintos comandos de la función binomial: -dbinom() nos da un valor exacto de la distribución binomial en el punto indicado. -pbinom() nos da la probabilidad acumulada de un evento. -qbinom() toma el valor de probabilidad que le ponemos como primer argumento y nos da como regreso un número cuya probabilidad acumulada empate con el valor de la probabilidad. -rbinom() genera cierta cantidad de número aleatorios de acuerdo con la probabilidad y el número de pruebas realizadas. En este caso, queremos conocer la probabilidad exacta de que 2 personas sean mutantes. Para esto necesitamos la función dbinom(x, size, prob), donde el argumento x equivale al número de éxitos, size al número de ensayos y prob a la probabilidad de éxito. dbinom(2, 5, 0.37) ## [1] 0.3423143 Ejemplo: En Estados Unidos, 85% de la población tiene sangre Rh positivo. Supongamos que tomamos 6 personas y contamos cuántos tiene Rh positivo. En este caso \\(Y\\) representará cuántas personas tienen Rh positivo dentro del grupo de 6. ¿Cuál es la probabilidad de \\(Y\\) = 4? ¿Y la probabilidad de que al menos 4 personas sean Rh positivo? ¿Y la probabilidad de que haya al menos 1 persona con Rh negativo? En este caso debemos utilizar dos comandos, dbinom() y pbinom(), con uno calcularemos \\(Y = 4\\) y con el otro \\(Y \\ge 4\\) (4 o más). Como queremos obtener \\(P(Y \\ge 4)\\), entonces \\(P(Y \\ge 4) = P(Y = 4) + P(Y=5) + P(Y=6)\\), utilizamos el argumento lower.tail y lo ponemos en FALSE para indicar que vamos a trabajar con un valor mínimo de 4, para arriba. Para el caso de al menos una persona con Rh negativo \\(P(Y &lt; 6)\\), tenemos dos opciones. Usar dbinom() cinco veces para calcular el valor individual de \\(P(Y = 0) + P(Y = 1) + P(Y = 2) \\cdots + P(Y = 5)\\), o calcular \\(P(Y = 6)\\) y restarle a 1 este valor. Como vemos ambas operaciones dan los mismos resultados, pero es más sencilla la segunda. #Probabilidad de que 4 personas tengan Rh positivo. dbinom(4, 6, 0.85) ## [1] 0.1761771 #Probabilidad de que al menos 4 personas (pueden ser 4, 5 o 6) tengan Rh positivo. pbinom(3, 6, 0.85, lower.tail = FALSE) ## [1] 0.9526614 #Probabilidad de que al menos 1 persona tenga Rh positivo. dbinom(0, 6, 0.85) + dbinom(1, 6, 0.85) + dbinom(2, 6, 0.85) + dbinom(3, 6, 0.85) + dbinom(4, 6, 0.85) + dbinom(5, 6, 0.85) ## [1] 0.6228505 1 - dbinom(6, 6, 0.85) ## [1] 0.6228505 3.5 Distribución normal Una distribución normal corresponde a una curva en forma de campana, con ciertas características específicas. Se utiliza para representar la distribución de los valores de una variable \\(X\\), de dos maneras distintas: (1) como una aproximación a un histograma basado en los valores muestreados de la variable \\(X\\) o; (2) como una representación idealizada de la distribución poblacional de \\(X\\). Las curvas con distribución normal toman su forma por dos elementos muy importantes: la media \\(\\mu\\) y su desviación estándar \\(\\sigma\\). Cuando se tiene una curva con distribución normal, se expresa de la siguiente manera \\(X \\sim N(\\mu, \\sigma)\\). La fórmula de la distribución normal se encuentra en la ecuación (3.7). No se trata de cualquier curva simétrica, si no de una curva simétrica específica. \\[\\begin{equation} f(x) = \\frac{1}{{\\sigma}{\\sqrt{2 \\pi}}}e^{-\\frac{1}{2}(\\frac{x-\\mu}{\\sigma})^2} \\tag{3.7} \\end{equation}\\] La función \\(f(x)\\) se conoce como función de densidad de probabilidad (PDF) y expresa la altura de la curva como una función de la posición en el eje horizontal. El centro de una curva normal es \\(x = \\mu\\), los puntos de inflexión están en \\(x = \\mu + \\sigma\\) y \\(x = \\mu - \\sigma\\). En principio la curva se extiende hasta el infinito, pero tres desviaciones estándar de la media hacia el valor negativo o positivo da como resultado valores demasiado pequeños. El ancho y alto de una curva normal están determinados por la desviación estándar \\(\\sigma\\). Figura 3.2: Forma de la distribución normal. Normalmente lo que nos interesa de una curva con distribución normal es el área debajo de la curva. Para esto utilizamos la escala estandarizada, en el cual el valor del eje horizontal se denomina valor Z. La escala de Z mide las desviaciones estándar a partir de la media, por ejemplo, \\(z = 1\\) corresponde a una desviación estándar de la media. Para transformar nuestros datos a la escala Z simplemente aplicamos la ecuación (3.8) y a la variable Z se le conoce como una variable de distribución normal estándar, ya que se encuentra estandarizada y no importa en que valor se encuentren los datos originales (kg, °C, cm, mmHg, etc.), la variable Z es adimensional. \\[\\begin{equation} Z = \\frac{X - \\mu}{\\sigma} \\tag{3.8} \\end{equation}\\] Una vez que nuestras variables se encuentran estandarizadas en el valor Z, podemos utilizar tablas de Z para realizar el cálculo debajo del área que corresponde al valor Z obtenido, aunque claro, también podemos hacer estos cálculos en R. Hay que tener en cuenta que para una curva de distribución normal estándar: 68% del área se encuentra entre \\(\\pm\\) 1 distribución estándar. 95% del área se encuentra entre \\(\\pm\\) 2 distribuciones estándar. 99.7% del área se encuentra entre \\(\\pm\\) 3 distribuciones estándar. 3.5.1 Distribución normal en R De manera similar a la distribución binomial, en R encontramos comandos similares para la distribución normal, donde tenemos: -dnorm() nos da un valor de densidad normal en determinado punto (valor puntual de la función de densidad). -pnorm() nos da un valor de densidad normal acumulado hasta cierto punto (área debajo de la curva). -qnorm() toma el valor de densidad normal que le ponemos como primer argumento y nos da como regreso un número cuya densidad normal acumulada empate con el valor de densidad normal ingresado. -rnorm() genera cierta cantidad de número aleatorios de acuerdo al valor de densidad normal. Intentemos resolver algunos ejercicios. Ejemplo: En una población de peces de la especie Pomolobus aestivalis, la longitud de los individuos sigue una distribución normal. La media de la longitud es de 54.0 mm, y la desviación estándar es de 4.5 mm2. ¿Qué porcentaje de los peces mide menos de 60 mm? ¿Qué porcentaje de los peces mide más de 51 mm? ¿Qué porcentaje de los peces miden entre 51 mm y 60 mm? Para responder la primer pregunta, debemos transformar nuestros datos a valores Z, ya que se encuentran en mm. Para esto aplicamos la ecuación (3.8). En R es una operación relativamente sencilla de hacer. (60 - 54)/(4.5) ## [1] 1.333333 Como podemos ver, el valor Z de 60 mm es igual a 1.33. Lo siguiente sería encontrar el área bajo la curva que corresponda a este valor Z. Para esto utilizamos la función pnorm(). pnorm(1.33, 0, 1, lower.tail = TRUE) ## [1] 0.9082409 Figura 3.3: Curva de distribución normal con el área sombreada correspondiente a un valor Z = 1.33 De nuevo, el argumento lower.tail = TRUE significa que encontrará la probabilidad acumulada de valores Z menores a 1.33 hasta 1.33. Como podemos ver la probabilidad de que un pez mida menos de 60 mm es de 90.82%. Para la segunda pregunta, primero debemos encontrar el valor de Z correspondiente a 51 mm y después basta con cambiar el argumento lower.tail a FALSE. (51 - 54)/(4.5) ## [1] -0.6666667 pnorm(-0.67, 0, 1, lower.tail = FALSE) ## [1] 0.7485711 Como podemos ver el resultado indica que 75.86% de los peces miden más de 51 mm. Bastante sencillo, ¿no? Figura 3.4: Curva de distribución normal con el área sombreada correspondiente a un valor Z = -0.67, pero partiendo desde el lado positivo de la curva. Ahora para la última pregunta, simplemente calculamos la probabilidad acumulada hasta nuestro valor Z más grande, que en este caso corresponde a 1.33 y le restamos la probabilidad acumulada del valor Z más pequeño, que corresponde -0.67. pnorm(1.33, 0, 1) - pnorm(-0.67, 0, 1) ## [1] 0.656812 Como resultado obtenemos que el 65.68% de los peces se encuentran en longitudes de entre 51 mm y 60 mm. Si nos damos cuenta, la distribución normal puede, de cierta manera, interpretarse como una distribución de probabilidad continua. Figura 3.5: Curva de distribución normal con el área sombreada correspondiente a un intervalo entre Z = -0.67 y Z = 1.33. En algunas ocasiones, queremos encontrar el valor Z correspondiente a un área bajo la curva determinada, para este tipo de ocasiones, utilizamos la función qnorm(). Del ejemplo anterior, supongamos que queremos encontrar el percentil 70 de la distribución de la longitud de los peces. Supongamos que este valor está representado por la variable \\(y\\). En otras palabras, queremos encontrar el valor tal que el 70% de las longitudes de los peces son menores que \\(y\\) y el 30% son mayores. qnorm(0.7, 0, 1) ## [1] 0.5244005 Como podemos ver, el valor Z correspondiente es 0.5244. Ahora, utilizando la ecuación (3.8) podemos realizar un despeje muy sencillo y obtener la fórmula \\(y = Z* \\sigma + \\mu = 0.5244 * 4.5 + 54 = 56.3\\). Esto quiere decir que 56.3 mm es el percentil 70 de la distribución de nuestros datos. Figura 3.6: Curva de distribución normal con el área sombreada correspondiente a un valor Z = -0.67, pero partiendo desde el lado positivo de la curva. 3.5.2 Pruebas de normalidad en R Ya que muchos procedimientos estadísticos se basan en datos provenientes de una población con distribución normal, es importante saber si nuestros datos siguen está distribución. Uno de los métodos más utilizados son los gráficos cuantil-cuantil, gráficos Q-Q o Q-Q plot. Veamos un ejemplo con datos de plantas que vienen incluidas en ggplot2. data(&quot;PlantGrowth&quot;) PlantGrowth ## weight group ## 1 4.17 ctrl ## 2 5.58 ctrl ## 3 5.18 ctrl ## 4 6.11 ctrl ## 5 4.50 ctrl ## 6 4.61 ctrl ## 7 5.17 ctrl ## 8 4.53 ctrl ## 9 5.33 ctrl ## 10 5.14 ctrl ## 11 4.81 trt1 ## 12 4.17 trt1 ## 13 4.41 trt1 ## 14 3.59 trt1 ## 15 5.87 trt1 ## 16 3.83 trt1 ## 17 6.03 trt1 ## 18 4.89 trt1 ## 19 4.32 trt1 ## 20 4.69 trt1 ## 21 6.31 trt2 ## 22 5.12 trt2 ## 23 5.54 trt2 ## 24 5.50 trt2 ## 25 5.37 trt2 ## 26 5.29 trt2 ## 27 4.92 trt2 ## 28 6.15 trt2 ## 29 5.80 trt2 ## 30 5.26 trt2 Como podemos ver, tenemos tres grupos, el control, tratamiento 1 y tratamiento 2. Vamos a enfocarnos solamente en los datos del grupo del tratamiento 1. Datos &lt;- PlantGrowth %&gt;% dplyr::select(starts_with(&quot;trt1&quot;), weight) Datos ## weight ## 1 4.17 ## 2 5.58 ## 3 5.18 ## 4 6.11 ## 5 4.50 ## 6 4.61 ## 7 5.17 ## 8 4.53 ## 9 5.33 ## 10 5.14 ## 11 4.81 ## 12 4.17 ## 13 4.41 ## 14 3.59 ## 15 5.87 ## 16 3.83 ## 17 6.03 ## 18 4.89 ## 19 4.32 ## 20 4.69 ## 21 6.31 ## 22 5.12 ## 23 5.54 ## 24 5.50 ## 25 5.37 ## 26 5.29 ## 27 4.92 ## 28 6.15 ## 29 5.80 ## 30 5.26 Recordemos que podemos utilizar la librería dplyr para extraer ciertos datos de nuestras matrices de datos. Lo que haremos ahora es utilizar el gráfico Q-Q para comprar los datos de nuestra muestra con unos datos teóricos que siguen una distribución normal. En caso de que nuestros datos se comporten de manera normal, deberíamos de tener casi una línea recta. Para esto necesitaremos la librería ggpubr. library(ggpubr) ggqqplot(Datos$weight) + xlab(&quot;Teórico&quot;) + ylab(&quot;Muestra&quot;) Figura 3.7: Gráfico Q-Q comparando una muestra de datos contra una muestra teórica con distribución normal. Otra opción sería realizar el gráfico de densidad, pero muchas veces es difícil identificar la forma de campana en cierto conjunto de datos, así que los gráficos Q-Q son de gran ayuda. Otra opción para realizar una prueba de normalidad conocida como prueba de Shapiro-Wilks (aunque no se recomienda que para \\(n &gt; 50\\)), pero es muy sensible a ligeras desviaciones de la normalidad, sobre todo con un tamaño de muestra grande. Es muy sencillo de realizar en R, simplemente utilizamos el comando shapiro_test() del paquete rstatix. En este caso, vamos a analizar si nuestros dos tratamientos y el control siguen una distribución normal. Para esto, vamos a hacer uso de la librería dplyr. library(rstatix) ## ## Attaching package: &#39;rstatix&#39; ## The following object is masked from &#39;package:stats&#39;: ## ## filter PlantGrowth %&gt;% group_by(group) %&gt;% shapiro_test(weight) #Agrupamos los datos por tipo de tratamiento y hacemos la prueba a la variable &quot;weight&quot;. ## # A tibble: 3 x 4 ## group variable statistic p ## &lt;fct&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 ctrl weight 0.957 0.747 ## 2 trt1 weight 0.930 0.452 ## 3 trt2 weight 0.941 0.564 ¿Cómo interpretamos los resultados y qué significa el valor-p? Bueno, ya que el control y los dos tratamientos tienen un valor-p &gt; 0.05 decimos que se distribuyen de manera normal. Valores-p menores a 0.05 son indicativos fuertes de no-normalidad. Después de obtener un valor menor a 0.05, podríamos corroborar esto con un gráfico Q-Q o con un histograma para ver la forma de la distribución de nuestros datos. 3.6 Distribución muestral La variabilidad entre muestras aleatorias que provienen de una misma población se conoce como variabilidad de muestreo. Una distribución de probabilidad que caracteriza algún aspecto de la variabilidad de muestreo se conoce como distribución muestral. Usualmente los valores de una muestra se parecen a los de la población de la cuál provienen. Una distribución muestral nos indica qué tan cerca la resemblanza entre entre la muestra y la población es probable que sea. Normalmente tomamos solamente una muestra aleatoria de una población. Pero para visualizar la distribución muestral, necesitamos realizar un meta-estudio, que consiste en repetir de manera indefinida, réplicas del mismo estudio. Por ejemplo, si un estudio consiste en extraer una muestra aleatoria de tamaño \\(n\\) de una población, un meta-estudio consiste en repetir varias veces la extracción de una muestra de tamaño \\(n\\) de una población. Por lo tanto, las probabilidades relativas de una muestra aleatoria se pueden interpretar como frecuencias relativas en un meta-estudio. Conocer la distribución muestral nos permite hacer afirmaciones de probabilidad de otras posibles muestras. Una pregunta natural a realizar es, ¿Qué tan parecida es la media de la muestra \\(\\overline{x}\\) de la media de la población \\(\\mu\\)? Aunque con una sola muestra no podemos responder esta pregunta, si pensamos en un modelo de muestreo aleatorio y tomamos la media muestral como una variable \\(\\overline{X}\\), podemos hacer ciertas inferencias. Reformulamos nuestra pregunta a ¿Qué tan cerca de \\(\\mu\\) es probable que este \\(\\overline{X}\\)? Nuestra respuesta la encontramos en la distribución muestral de \\(\\overline{X}\\). Tenemos que tener en cuenta que, en promedio, la media de la distribución muestral \\(\\overline{X}\\) equivale a la media de la población \\(\\mu\\). Esto se ve mejor en la fórmula (3.9). \\[\\begin{equation} \\mu_{\\overline{X}} = \\mu \\tag{3.9} \\end{equation}\\] La fórmula de la desviación estándar de la muestra es un poco menos intuitiva, aunque si se analiza de manera detallada tiene sentido. \\[\\begin{equation} \\sigma_{\\overline{X}} = \\frac{\\sigma}{\\sqrt{n}} \\tag{3.10} \\end{equation}\\] Mientras el tamaño de muestra incrementa, la desviación estándar de \\(\\overline{X}\\) disminuye. Es decir para muestras más grandes existe menos variación. La forma está determinada por el tamaño de muestra y la naturaleza de la población. Si la población \\(X\\) se distribuye de manera normal, entonces la distribución muestral de \\(\\overline{X}\\) será también normal, sin importar el tamaño de nuestra \\(n\\). Además el teorema del límite central indica que si obtenemos una \\(n\\) suficientemente grande, la distribución muestral de \\(\\overline{X}\\) será aproximadamente normal incluso para muestras cuya población \\(X\\) no se distribuye de manera normal. Ejemplo: Supongamos que tenemos una población del Carbonerito Mexicano (Poecile sclateri) en la cuál el peso medio es de \\(\\mu = 11 g\\) y la desviación estándar \\(\\sigma = 1.2 g\\). Supongamos que tomamos una muestra aleatoria de seis aves. Dejemos que \\(\\overline{x}\\) represente la media del peso de las seis aves. Ya que sabemos que el peso de esta ave sigue una distribución normal en la población, también nuestras muestras seguirán una distribución normal. Para este caso, la media y la desviación estándar de nuestra muestra serán las siguientes: \\[\\begin{equation} \\mu_{\\overline{X}} = \\mu = 11 g\\\\ \\sigma_{\\overline{X}} = \\frac{\\sigma}{\\sqrt{n}} = \\frac{1.2}{\\sqrt{6}} = 0.49 g \\end{equation}\\] En este caso \\(\\mu_{\\overline{X}} = 11 g\\) y \\(\\sigma_{\\overline{X}} = 0.49 g\\). De tal manera que, en promedio la media de la muestra será 11 g, sin embargo, el 68% de las veces \\(\\overline{X}\\) se encontrará entre \\(11g \\pm 0.49g\\) y el 95% de las veces se encontrará entre \\(11g \\pm 0.98g\\). Figura 3.8: Distribución muestral proveniente de una población de Sclateri poecile. Esta distribución muestral expresa distintas posibilidades para los valores de \\(\\overline{X}\\). Supongamos que quisiéramos saber la probabilidad de que la media de una muestra de seis aves sea mayor a 11.5 g. Ya que nuestros datos son normales, podemos usar la transformación a valores Z para obtener nuestro resultado. \\[\\begin{equation} Z = \\frac{\\overline{x}-\\mu_\\overline{X}}{\\sigma_{\\overline{X}}} = \\frac{11.5 - 11}{0.49} = 1.0204 \\end{equation}\\] Ya que nuestro valor Z = 1.0204, usamos la función pnorm() para encontrar nuestra área bajo la curva. pnorm(1.0204, mean = 0, sd = 1, lower.tail = FALSE) ## [1] 0.1537694 De hecho, ni siquiera es necesario realizar la transformación a valores Z en R ya que podemos modificar los parámetros de la función pnorm(). pnorm(11.5, mean = 11, sd = 0.49, lower.tail = FALSE) ## [1] 0.1537675 Debido a que hemos redondeado la desviación estándar el valor es ligeramente distinto, pero en esencia el resultado es el mismo. Entonces podemos concluir que \\[\\begin{equation} P(\\overline{X} &gt; 11.5) = P(Z &gt; 1.0204) = 0.1538 \\approx 0.15 \\end{equation}\\] Figura 3.9: Probabilidad de que la media de una muestra de seis aves de la especie Poecile sclateri sea mayor a 11.5. Si eligiéramos muchas muestras aleatorias provenientes de esta población cerca del 15% de las muestras tendrían una media mayor a 11.5 g. El tamaño de la muestra tiene un efecto directo sobre la forma de nuestra curva. Básicamente, muestras más grandes dan un \\(\\sigma_\\overline{X}\\) menor, y por ende dan un menor error de muestreo. En seguida se muestran distintas gráficas con distintas \\(\\mu_\\overline{X}\\). Para este caso hipotético, \\(\\mu = 100, \\sigma = 40\\). Figura 3.10: Cambios en la forma de la curva dependientes de la desviación estándar muestral. Figura 3.11: Cambios en la forma de la curva dependientes de la desviación estándar muestral. Qué tan cerca esta \\(\\overline{X}\\) de \\(\\mu\\) depende del tamaño de la muestra \\(n\\). La media de una muestra grande no necesariamente está más cerca a la media poblacional que la media de una muestra pequeña, pero existe mayor probabilidad de que lo este. 3.6.1 Poblaciones, muestras y distribuciones muestrales Una vez llegado a este punto puede que exista confusión entre los valores de una población, de una muestra y de una distribución muestral. Para esto aclaremos los siguientes puntos, en torno a una variable \\(X\\). En una población, los estadísticos descriptivos como la media y la desviación estándar se representan por los siguientes símbolos: \\(\\mu\\): media poblacional. \\(\\sigma\\): desviación estándar poblacional. En una muestra, los mismos estadísticos se representan por los siguientes símbolos: \\(\\overline{x}\\): media muestral. \\(s\\): desviación estándar poblacional. En una distribución muestral lo que nosotros hacemos es repetir un muestreo indefinidas veces (meta-estudio) y de cada muestreo extraer la media muestral \\(\\overline{x}\\). Lo que representa la distribución muestral es una distribución de medias, en lugar de observaciones individuales. \\(\\mu_\\overline{X}\\): media de una distribución muestral. \\(\\sigma_\\overline{X}\\): desviación estándar de una distribución muestral. "],["estadística-inferencial.html", "Lección 4 Estadística inferencial 4.1 Error estándar de la media 4.2 Intervalo de confianza para \\(\\mu\\) 4.3 Comparación de dos medias 4.4 Pruebas de hipótesis 4.5 Comparación de muestras pareadas", " Lección 4 Estadística inferencial En esta lección aprenderemos a hacer inferencias, basadas en un modelo de muestreo aleatorio. Usaremos información proveniente de nuestras muestras aleatorias para inferir hechos acerca de la población de la que fueron tomadas. Utilizamos datos para determinar alguna característica de la población original o para medir la precisión de nuestras estimaciones. Sabemos que al tomar muestras de una población, nuestros datos son susceptibles a un error de muestreo. Cabe aclarar que este error no solamente tiene que ver con la precisión de medición, si no que este error surge por el hecho de que no estamos tomando en cuenta los datos de toda la población. 4.1 Error estándar de la media El error estándar mide la magnitud del error de muestreo, es decir, la discrepancia entre \\(\\overline{x}\\) y \\(\\mu\\) utilizando la distribución muestral de \\(\\overline{X}\\). La fórmula del error estándar es la siguiente: \\[\\begin{equation} SE_\\overline{X} = \\frac{s}{\\sqrt{n}} \\tag{4.1} \\end{equation}\\] Como podemos ver, esta fórmula es bastante similar a la fórmula de la desviación estándar de la muestra \\(\\sigma_{\\overline{X}} = \\frac{\\sigma}{\\sqrt{n}}\\). Normalmente, la diferencia entre \\(\\mu\\) y \\(\\overline{x}\\) es unos cuantos errores estándar. De hecho, encontramos a \\(\\overline{x}\\) a un error estándar de \\(\\mu\\) bastante seguido. Hay que destacar que el error estándar dependen de la desviación estándar muestral \\(s\\) y del tamaño de muestra \\(n\\). Es importante destacar la diferencia entre el error estándar y la desviación estándar. La desviación estándar describe la dispersión de los datos de una muestra, mientras que el error estándar describe la falta de fidelidad en la media de una muestra como estimación de la media poblacional, debido al error de muestreo. Una forma de disminuir el error estándar es incrementando el tamaño de muestra \\(n\\). En las gráficas podemos ver el error estándar o la desviación estándar representadas como intervalos en nuestros puntos. Por ejemplo, las siguientes gráficas representan la desviación estándar en la figura 4.1 y el error estándar en la figura 4.2. Como podemos ver, la dispersión de los datos es mayor al error estándar. Figura 4.1: Gráfico con la desviación estándar representada en forma de intervalo. Figura 4.2: Gráfico con el error estándar representado en forma de intervalo. 4.2 Intervalo de confianza para \\(\\mu\\) Un intervalo de confianza nos sirve para determinar qué tan cerca está de \\(\\mu\\) nuestra media muestral \\(\\overline{x}\\). Ya que no podemos medir de manera directa \\(\\mu\\), utilizamos \\(\\overline{x}\\) adicional al error estándar \\(SE_\\overline{X}\\) como se indica en la fórmula (4.2). \\[\\begin{equation} \\overline{x} \\pm {2}*{SE_\\overline{X}} \\tag{4.2} \\end{equation}\\] ¿Por qué utilizar dos veces \\(SE_\\overline{X}\\)? Porque al utilizarlo sabemos que el 95% de las veces \\(\\mu\\) se encontrará en este intervalo. Si utilizáramos solamente un \\(SE_\\overline{X}\\) solamente estaríamos seguros de que \\(\\mu\\) se encuentra en este intervalo el 68% de las veces. Esta idea está basada en la distribución muestral de \\(X\\) que vimos en la lección anterior. Al estandarizar nuestra variable \\(X\\) y transformarla en valores \\(Z\\), lo que buscamos es un área de 0.95. Estos valores Z corresponden a -1.96 como límite inferior y 1.96 como limite superior. \\[\\begin{equation} P({-1.96}&lt;{Z}&lt;1.96) = 0.95 \\\\ P({-1.96}&lt;{\\frac{\\overline{X}-\\mu}{\\sigma/\\sqrt{n}}}&lt;1.96) = 0.95 \\\\ P({-1.96\\times{\\sigma/\\sqrt{n}}}&lt;{{\\overline{X}-\\mu}}&lt;1.96\\times{\\sigma/\\sqrt{n}}) = 0.95 \\\\ P({-\\overline{X}-1.96\\times{\\sigma/\\sqrt{n}}}&lt;{-\\mu}&lt;-\\overline{X}1.96\\times{\\sigma/\\sqrt{n}}) = 0.95 \\\\ P({\\overline{X}-1.96\\times{\\sigma/\\sqrt{n}}}&lt;{\\mu}&lt;\\overline{X}1.96\\times{\\sigma/\\sqrt{n}}) = 0.95 \\\\ \\end{equation}\\] A partir de este despeje obtenemos la fórmula para un intervalo de confianza que contendrá a \\(\\mu\\) el 95% de las veces. \\[\\begin{equation} \\overline{X} \\pm 1.96\\frac{\\sigma}{\\sqrt{n}} \\tag{4.3} \\end{equation}\\] Si en este caso, cambiamos \\(\\sigma\\) por \\(s\\), podemos calcular el intervalo de confianza para nuestra muestra. William Sealy Gosset, quién publicó sus hallazgos bajo el seudónimo de Student, descubrió un método para preservar la interpretación del 95% y desde entonces este método lleva su seudónimo. Básicamente, si la muestra proviene de una población normal, y si remplazamos \\(\\sigma\\) del intervalo por \\(s\\), la interpretación del 95% se puede preservar utilizando la nueva cantidad denotada como \\(t_{0.025}\\) y que está relacionada con una distribución conocida como distribución \\(t\\) de Student. 4.2.1 Distribución \\(t\\) de Student La distribución \\(t\\) de Student es una distribución teórica continua, usada para distintos propósitos en estadística, por ejemplo, para la construcción de intervalos de confianza. La distribución depende de una cantidad conocida como grados de libertad o df. La forma de una distribución \\(t\\) de Student es de campana, similar a la distribución normal, pero con una desviación estándar mayor. Conforme los grados de libertad aumentan, la curva de una distribución \\(t\\) de Student se asemeja más a una curva normal (podría decirse que una curva normal es una curva \\(t\\) de Student con \\(df = \\infty\\)). En la figura 4.3 podemos ver como la forma de una distribución \\(t\\) de Student cambia conforme los \\(df\\) aumentan. La línea punteada es una curva con distribución normal. Figura 4.3: Curvas con distribución t de Student y cómo cambian dependiendo de los df. La cantidad \\(t_{0.025}\\) se conoce como valor crítico al 5% de dos colas de una distribución \\(t\\) de Student. El área entre \\(-t_{0.025}\\) y \\(t_{0.025}\\) contiene el 95% del área de la curva. Es decir el área debajo de \\(-t_{0.025}\\) y encima de \\(t_{0.025}\\) suman en total 5% como se ve en la figura 4.4. Figura 4.4: Valores críticos de una distribución t de Student Existen tablas de distribución \\(t\\) de Student que se utilizan para encontrar los valores críticos acorde con nuestros grados de libertad. Al igual que la distribución binomial y normal, R viene con funciones para la distribución \\(t\\) de Student. Estos son los siguientes: -dt() nos da un valor de densidad en determinado punto de la distribución de \\(t\\) (valor puntual de la función de densidad). -pt() nos da un valor de densidad acumulado hasta cierto punto en la distribución de \\(t\\) (área debajo de la curva). -qt() toma el valor de densidad que le ponemos como primer argumento y nos da como regreso un número cuya densidad acumulada empate con el valor de densidad ingresado. -rt() genera cierta cantidad de número aleatorios de acuerdo al valor de densidad. Sin embargo en este caso, en lugar de contar con el argumento n para el tamaño de muestra, contamos con otro argumento df para los grados de libertad. Para poder utilizar el método \\(t\\) de Student, se necesitan cumplir ciertas condiciones: Las muestras deben de ser aleatorias. Las observaciones de la muestra deben ser independientes unas de otras. Si \\(n\\) es pequeña, la distribución de la población debe ser aproximadamente normal. Si \\(n\\) es grande, la distribución de la población no necesita ser normal. 4.2.2 Construcción de un intervalo de confianza para \\(\\mu\\) El primer paso para construir nuestro intervalo de confianza es elegir nuestro nivel de confianza (generalmente es 95% pero esto no quiere decir que siempre tenga que ser así). Si quisiéramos un nivel de confianza de 90% utilizamos un valor crítico de \\(t_{0.05}\\). Después tenemos que establecer los límites de nuestro intervalo utilizando la siguiente fórmula (para intervalos con 95% de confianza): \\[\\begin{equation} \\overline{x} \\pm t_{0.025}\\frac{s}{\\sqrt{n}} \\tag{4.4} \\end{equation}\\] Como vimos, el valor crítico de \\(t\\) es determinado por los grados de libertad. Estos se calculan utilizando las siguiente fórmula: \\[\\begin{equation} df = n - 1 \\tag{4.5} \\end{equation}\\] ¿Por qué se calculan así los grados de libertad? Bueno, las desviaciones (\\(x_i - \\overline{x}\\)) deben de sumar 0 en total, por lo que solamente una (\\(n - 1\\)) de estas desviaciones puede variar libremente. Ejemplo: Una muestra del área de las alas obtenida de una población de la Mariposa Monarca (Danaus plexippus) de 14 ejemplares tiene una media \\(\\overline{x} = 32.8143cm^2\\) y una desviación estándar \\(s = 2.4757 cm^2\\). Sabemos que los datos provienen de una población con distribución normal. ¿Cuál sería el intervalo de confianza al 95% para esta muestra? El primer paso es calcular los grados de libertad. \\[\\begin{equation} df = n - 1 = 14 - 1 = 13 \\end{equation}\\] Como queremos un intervalo de confianza al 95%, utilizamos el valor crítico de \\(t_{0.025}\\). Para encontrar este valor podemos buscarlo en una tabla de \\(t\\) o utilizar las funciones de R. qt(0.025, 13, lower.tail = FALSE) ## [1] 2.160369 Por lo tanto el intervalo de confianza al 95% para nuestra muestra es \\[\\begin{equation} 32.8143 \\pm {2.16}{\\frac{2.4757}{\\sqrt{14}}} \\\\ 32.8143 \\pm {2.16}*{0.6617} \\\\ 32.8143 \\pm 1.4293 \\end{equation}\\] De nuevo, esto se puede calcular en R de manera sencilla. 32.8143 + 2.160*(2.4757/sqrt(14)) ## [1] 34.24348 32.8143 - 2.160*(2.4757/sqrt(14)) ## [1] 31.38512 Una forma compacta de escribir el intervalo de confianza es con el límite inferior y superior separados por una coma y entre paréntesis (\\(31.4, 34.2\\)). Con esta información sabemos entonces que \\(31.4cm^2 &gt; \\mu &gt; 34.2cm^2\\). Una cosa importante a destacar es que es el propio intervalo de confianza el que es aleatorio. Si consideramos intervalos de confianza al 95%, entonces: \\[\\begin{equation} P(La\\space siguiente\\space muestra\\space nos\\space dará\\space un\\space intervalo\\space de\\space confianza\\space que\\space contenga\\space a\\space \\mu) = 0.95 \\end{equation}\\] Véanoslo de la siguiente manera. Si hiciéramos un meta-estudio y construyéramos un intervalo de confianza a 95% para cada una de las muestras, entonces el 95% de los intervalos de confianza contendrán a \\(\\mu\\). El nivel de confianza (90%, 95%, 99%) es una propiedad del método más que de un intervalo particular. 4.2.3 Intervalos de confianza unilaterales Este tipo de intervalos de confianza se obtienen cuando solamente nos interesa el límite inferior o superior. Supongamos que queremos construir un intervalo de confianza con un límite inferior al 95%. Mientras que un intervalo de confianza bilateral está basado en usar el valor \\(\\pm t_0.025\\), un intervalo de confianza al 95% de un solo lado (en este caso inferior) solamente se preocupa por el área bajo la curva del lado correspondiente. Ya que \\(P(-t_{0.05} &gt; t &gt; \\infty) = 0.95\\), para un intervalo de confianza unilateral al 95% utilizamos un valor de \\(-t_{0.05}\\) y establecemos el límite con la siguientes operaciones: \\[\\begin{equation} \\overline{x} - t_{0.05}{SE_{\\overline{X}}} \\\\ \\overline{x} + t_{0.05}{SE_{\\overline{X}}} \\end{equation}\\] Estas serían las operaciones a realizar para un intervalo de confianza con límite inferior y superior, respectivamente. 4.3 Comparación de dos medias Cuando queremos comparar dos muestras que provienen de supuestamente poblaciones distintas, podemos realizar las comparaciones utilizando (1) las medias de las muestras; (2) sus desviaciones estándar o; (3) la forma de su distribución. Para comprar la media de dos muestras, es natural considerar la diferencia entre ellas. \\[\\begin{equation} \\overline{X}_1 - \\overline{X}_2 \\end{equation}\\] Esta cantidad es un estimado de la diferencia entre las poblaciones (\\(\\mu_1 - \\mu_2\\)). Para caracterizar el error de muestreo de esta estimación, debemos conocer el error estándar entre la diferencia de (\\(\\overline{X}_1 - \\overline{X}_2\\)). La fórmula para el error estándar de (\\(\\overline{X}_1 - \\overline{X}_2\\)) es la siguiente: \\[\\begin{equation} SE_{(\\overline{X}_1 - \\overline{X}_2)} = \\sqrt{\\frac{s^2_1}{n_1} + \\frac{s^2_2}{n_2}} \\tag{4.6} \\end{equation}\\] Otra forma de representar la fórmula es de la siguiente manera \\[\\begin{equation} SE_{(\\overline{X}_1 - \\overline{X}_2)} = \\sqrt{SE^2_1 + SE^2_2} \\tag{4.7} \\end{equation}\\] En donde \\(SE_1 = SE_{\\overline{X}_1} = \\frac{s_1}{\\sqrt{n_1}}\\) y \\(SE_2 = SE_{\\overline{X}_2} = \\frac{s_2}{\\sqrt{n_2}}\\). 4.3.1 Intervalo de confianza para \\(\\mu_1 - \\mu_2\\) Podemos comprar las medias de dos muestras construyendo un intervalo de confianza para la diferencia de las medias. La fórmula para un intervalo de confianza de la diferencia de las medias es bastante similar a la usada para construir uno para una muestra. \\[\\begin{equation} (\\overline{X}_1 - \\overline{X}_2) \\pm t_{0.025} SE_{(\\overline{X}_1 - \\overline{X}_2)} \\tag{4.8} \\end{equation}\\] El valor crítico de \\(t_{0.025}\\) es determinado a partir de la distribución \\(t\\) de Student utilizando los grados de libertad que en este caso se calculan de la siguiente manera: \\[\\begin{equation} df = \\frac{(SE^2_1 + SE^2_2)^2}{SE^4_1/(n_1 - 1) + SE^4_2/(n_2 -1)} \\tag{4.9} \\end{equation}\\] Otros métodos se basan en obtener los grados de libertad utilizando el número más pequeño entre (\\(n_1 - 1\\)) y (\\(n_2 - 1\\)) o \\(n_1 + n_2 - 2\\). Ejemplo: Varios biólogos creen que el tórax de las mariposas monarca macho es mayor que el de las hembras. Una muestra de 7 machos y 8 hembras dan los resultados de la tabla 4.1 y la figura 4.5. Tabla 4.1: Peso del torax (mg) Machos Hembras 67 73 73 54 85 61 84 63 78 66 63 57 80 75 58 En la siguiente figura se puede observar la distribución de los datos para los machos y las hembras. A simple vista parece ser que el tórax de los machos pesa ligeramente más que el de las hebras. Ahora tendremos que calcular ciertos valores para estos datos, como las medias y desviaciones estándar de cada grupo. Recuerden que para esto podemos usar la librería dplyr. Anteriormente creé una variable llamada Sample2 que contiene los datos que vamos a necesitar. Sample2 ## Grupo Peso ## 1 Machos 67 ## 2 Machos 73 ## 3 Machos 85 ## 4 Machos 84 ## 5 Machos 78 ## 6 Machos 63 ## 7 Machos 80 ## 9 Hembras 73 ## 10 Hembras 54 ## 11 Hembras 61 ## 12 Hembras 63 ## 13 Hembras 66 ## 14 Hembras 57 ## 15 Hembras 75 ## 16 Hembras 58 En este caso yo he redondeado los valores a solamente 1 dígito. Ahora para obtener el intervalo de confianza no hace falta que hagamos todas las fórmulas para obtener los grados de libertad, el error estándar, etc. Basta con usar la función t.test() para obtener nuestro intervalo de confianza. t.test(Peso ~ Grupo,Sample2)$conf.int ## [1] -21.353087 -3.325484 ## attr(,&quot;conf.level&quot;) ## [1] 0.95 Como podemos ver nuestro intervalo se encuentra entre (\\(3.3, 21.4\\)) para un nivel de significancia del 95%. ¿Cómo se interpreta esto? Bueno, de la siguiente manera: de acuerdo a nuestro intervalo de confianza, podemos estar 95% seguros de que la media poblacional del tórax de los machos de Mariposa Monarca (\\(\\mu_1\\)) es más grande que la de las hembras (\\(\\mu_2\\)) por una cantidad tan pequeña como 3.3 mg o tan grande como 21.4 mg. Si quisiéramos ajustar el nivel de confianza de nuestro intervalo, también podemos hacerlo. t.test(Peso ~ Grupo,Sample2, conf.level = 0.9)$conf.int ## [1] -19.716479 -4.962093 ## attr(,&quot;conf.level&quot;) ## [1] 0.9 Un intervalo de confianza al 90% se encuentra entre (\\(5.0, 19.7\\)). Figura 4.5: Peso del torax de Mariposas Monarca macho y hembra (mg) 4.4 Pruebas de hipótesis ¿Qué tan diferentes tienen que ser dos muestras para concluir que las poblaciones de las que vienen son distintas? Una aproximación que podemos hacer es comparar las medias de las dos muestras y ver qué tanto difieren comparándolo con la diferencia que esperaríamos si fuese azar. 4.4.1 \\(t\\)-test La idea general es formular una hipótesis para ver si \\(\\mu_1\\) y \\(\\mu_2\\) difieren, y ver la información que tenemos de nuestras muestras para ver si se apoya esta hipótesis. La hipótesis de que \\(\\mu_1\\) y \\(\\mu_2\\) no son iguales es, de manera general, nuestra hipótesis alternativa y se abrevia como \\(H_A\\). \\[\\begin{equation} H_A: \\mu_1 \\neq \\mu_2 \\tag{4.10} \\end{equation}\\] La antítesis de esta hipótesis se conoce como hipótesis nula y se abrevia como \\(H_0\\). \\[\\begin{equation} H_0: \\mu_1 = \\mu_2 \\tag{4.11} \\end{equation}\\] Una estadístico de prueba de hipótesis o estadístico de prueba nos ayuda a corroborar la fuerza de la evidencia presentada en los datos en favor de la \\(H_A\\). Ya que la \\(H_0\\) dice que las medias de las poblaciones son iguales, esperaríamos que esa diferencia fuese 0. \\[\\begin{equation} H_0: \\mu_1 = \\mu_2 \\longleftrightarrow H_0: \\mu_1 - \\mu_2 = 0 \\end{equation}\\] La \\(H_A\\) dice que la diferencia entre las medias no es igual a 0. \\[\\begin{equation} H_0: \\mu_1 \\neq \\mu_2 \\longleftrightarrow H_0: \\mu_1 - \\mu_2 \\neq 0 \\end{equation}\\] La prueba de \\(t\\) es un método que nos ayuda a elegir entre ambas hipótesis. Para hacer un análisis como este, primero necesitamos computar el estadístico de \\(t\\). \\[\\begin{equation} t_s = \\frac{(\\overline{x}_1 - \\overline{x}_2) - 0}{SE_{(\\overline{X}_1 - \\overline{X}_2)}} \\tag{4.12} \\end{equation}\\] Este estadístico mide que tan larga es la diferencia de las medias de la diferencia que esperaríamos si la \\(H_0\\) fuese verdad \\((\\overline{x}_1 - \\overline{x}_2) - 0\\), expresada en relación con el error estándar de la diferencia \\(SE_{(\\overline{X}_1 - \\overline{X}_2)}\\) (la cantidad de variación que esperaríamos ver en la diferencias de muestras aleatorias). Veamos un ejemplo. Ejemplo: Investigadores están interesados en los efectos que produce el tolueno. Para esto, midieron las concentraciones de varios químicos en el cerebro de ratas que habían sido expuestas a tolueno y ratas que no habían sido expuestas (control). En la siguiente tabla se muestran las concentraciones de norepinefrina (NE) de la región de la médula del cerebro. La \\(H_0\\) de los investigadores es que el tolueno no tiene efecto en la concentración de NE en la médula de las ratas. Por ende, la \\(H_A\\) es que el tolueno tiene un efecto en la médlua de las ratas. set.seed(123) Tolueno &lt;- round(rnorm(10, 540.8, 66.1), 1) Control &lt;- round(rnorm(10, 444.2, 69.6), 1) ex1 &lt;- data.frame(Control, Tolueno) Tabla 4.2: Concentración de NE en cerebro de ratas (ng/gm) Control Tolueno 529.4 503.8 469.2 525.6 472.1 643.8 451.9 545.5 405.5 549.3 568.6 654.2 478.9 571.3 307.3 457.2 493.0 495.4 411.3 511.3 En este caso tenemos que conocer la desviación estándar y la media de nuestros datos. Para esto podemos usar la librería de dplyr. ex2 &lt;- gather(ex1, Grupo, Conc) ex3 &lt;- ex2 %&gt;% group_by(Grupo) %&gt;% summarise(n = n(), Media = mean(Conc), SD = sd(Conc), SE = SD/sqrt(n)) ex3 ## # A tibble: 2 x 5 ## Grupo n Media SD SE ## &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Control 10 459. 72.3 22.9 ## 2 Tolueno 10 546. 63.0 19.9 Listo, tenemos nuestra media \\(\\overline{x}\\), desviación estándar \\(s\\) y error estándar \\(SE\\) para el grupo control y el grupo tratado con tolueno. Ahora procedemos a aplicar la fórmula para encontrar nuestro estadístico \\(t\\). Primero debemos calcular el error estándar de (\\(\\overline{X}_1 - \\overline{X}_2\\)). \\[\\begin{equation} SE_{(\\overline{X}_1 - \\overline{X}_2)} = \\sqrt{\\frac{63.04^2}{10} + \\frac{72.26^2}{10}} \\end{equation}\\] Posteriormente aplicaríamos la fórmula para encontrar el estadístico \\(t\\). \\[\\begin{equation} t_s = \\frac{(545.74 - 458.72) - 0}{SE_{(\\overline{X}_1 - \\overline{X}_2)}} \\end{equation}\\] Sin embargo, gracias a la función t.test no es necesario que hagamos este procedimiento paso a paso. Por ejemplo, si queremos obtener el estadístico \\(t\\) simplemente escribimos lo siguiente. t.test(Conc ~ Grupo, ex2)$statistic ## t ## -2.869561 Si agregamos $statistic al final de nuestra función obtenemos el estadístico de \\(t\\) que en este caso es 2.87. Pero, ¿cómo sabemos que tenemos suficiente evidencia para aceptar o rechazar la \\(H_A\\)? Bueno, si la \\(H_0\\) es verdadera, la distribución muestral de \\(t_s\\) se aproximará a una distribución \\(t\\) de Student con los grados de libertad dados por la fórmula (4.9). La esencia de de la prueba de \\(t\\) es ver en qué parte de la distribución \\(t\\) de Student cae el estadístico \\(t_s\\). Si cae cercano al centro, entonces la evidencia de los datos es compatible con la \\(H_0\\). Sin embargo, si \\(t_s\\) cae en alguna de las colas, entonces, la evidencia de los datos es compatible con la \\(H_A\\) y una observación como la obtenida no puede explicarse meramente por azar. Para juzgar si nuestro valor \\(t_s\\) cae en la cola de la distribución \\(t\\) de Student, usamos un valor conocido como valor-p. El valor-p puede definirse como el área bajo la curva de una distribución \\(t\\) de Student en cualquiera de las colas que se encuentra más allá de \\(-t_s\\) o \\(t_s\\) como se muestra en la figura 4.6. Figura 4.6: Valor-p para un valor de \\(t_s\\). Otra forma de definir el valor-p es como la probabilidad de obtener un valor tan extremo como el observado si la \\(H_0\\) fuera verdadera. Para calcular el valor-p necesitamos encontrar el área bajo la distribución \\(t\\) de Student que se encuentre más allá de \\(\\pm SE_{\\overline{X}_1 - \\overline{X}_2}\\). Afortunadamente, la función t.test() también nos da el valor-p. Podemos corroborar esto a mano. t.test(Conc ~ Grupo, ex2)$parameter ## df ## 17.6748 Como podemos ver, para nuestros datos tenemos 17.67 grados de libertad. Entonces simplemente en la distribución \\(t\\) de Student buscamos el área correspondiente para un valor \\(t_s = -2.87\\) con \\(df = 17.67\\). pt(2.87, 17.67, lower.tail = F) ## [1] 0.005160011 pt(-2.87, 17.67, lower.tail = T) ## [1] 0.005160011 Como en este caso se trata de un valor-p de dos colas, tenemos que sumar el área que obtenemos de la cola inferior y superior, que dan como resultado cerca de 0.0103. Por ende, nuestro \\(valor-p = 0.0103\\). Afortunadamente no tenemos que hacer esto cada vez que realizamos una prueba de \\(t\\) en R. Simplemente escribimos $p.value al final de nuestro comando y este nos debería dar nuestro valor-p. t.test(Conc ~ Grupo, ex2)$p.value ## [1] 0.0103277 Como podemos ver el valor es aproximadamente igual (puede diferir debido a los decimales). Un valor-p cercano a 1 indica que el valor \\(t_s\\) se encuentra cercano al centro de la distribución \\(t\\), que resultaría en una falta de evidencia para la \\(H_A\\). Un valor-p cercano a 0 indica que el valor \\(t_s\\) se encuentra en una de las colas de la distribución \\(t\\), lo que daría evidencia para la \\(H_A\\). ¿Cómo decidimos qué valores-p muestran evidencia para la \\(H_A\\)? Para esto necesitamos elegir un nivel de significancia o \\(\\alpha\\). Este valor lo elige, generalmente, quién realiza la decisión acerca del estudio. Valores \\(\\alpha\\) comunes son \\(\\alpha = 0.1, \\space 0.05, \\space 0.01\\). Si nuestro valor-p es menor o igual a nuestro alfa (\\(valor-p \\leq \\alpha\\)) nuestros datos muestran evidencia de una diferencia estadísticamente significativa a favor de \\(H_A\\). Decimos que la \\(H_0\\) se rechaza. Si por el contrario el valor-p es mayor que nuestro alfa (\\(valor-p \\geq \\alpha\\)), decimos que hay insuficiente evidencia para decir que \\(H_A\\) es verdad, por lo tanto no se rechaza la \\(H_0\\). ¿Cómo podemos ver toda está información? Bueno, si no agregamos ningún valor al final de nuestro comando t.test(), obtenemos toda la información condensada. t.test(Conc ~ Grupo, ex2) ## ## Welch Two Sample t-test ## ## data: Conc by Grupo ## t = -2.8696, df = 17.675, p-value = 0.01033 ## alternative hypothesis: true difference in means between group Control and group Tolueno is not equal to 0 ## 95 percent confidence interval: ## -150.81499 -23.22501 ## sample estimates: ## mean in group Control mean in group Tolueno ## 458.72 545.74 ¿Dónde ponemos o indicamos el \\(\\alpha\\)? La función t.test() no especifica un nivel de significancia y lo deja a manos de quién está realizando el experimento para decidir qué \\(\\alpha\\) utilizar. Figura 4.7: Gráfico con el valor-p (área sombreada) correspondiente a los datos del ejercicio anterior. Algo importante a tener en consideración es que, incluso cuando nuestro valor-p no muestra diferencias significativas no quiere decir esto que la \\(H_A\\) no sea verdadera. Esto nos lleva a los tipos de errores que podemos cometer cuando hacemos pruebas de hipótesis. Tipo de error I: Cuando decimos que los datos proveen evidencia significativa a favor de la \\(H_A\\), cuando en realidad \\(H_0\\) es verdad. Este tipo de error se contrarresta cuando elegimos nuestro \\(\\alpha\\). Tipo de error II: cuando la \\(H_A\\) es verdadera, pero no obtenemos suficiente información que demuestre esto. Las muestras con tamaños de muestra pequeños son particularmente vulnerables a este tipo de error. La probabilidad de cometer este tipo de error se conoce como \\(\\beta\\). La posibilidad de no cometer un error tipo II cuando la \\(H_A\\) es verdadera se conoce como poder. \\[\\begin{equation} Poder = 1 - \\beta = P(evidencia \\space significativa \\space para \\space H_{A}) \\space si \\space H_{A} \\space es \\space verdad \\tag{4.13} \\end{equation}\\] 4.4.2 \\(t\\) test de una cola En caso de que tengamos una hipótesis direccional en la que nos interese solamente una de las dos colas de la distribución de \\(t\\), realizamos un \\(t\\) test de una cola. Las \\(H_A\\) podrían ser \\(H_{A}: \\mu_1 &gt; \\mu_2\\) o \\(H_{A}: \\mu_1 &lt; \\mu_2\\). Para este tipo de pruebas lo primero es checar la direccionalidad: ¿Nos interesa la parte inferior o superior de la distribución de \\(t\\)? Después de eso, ver hacia que lado se desvían los datos. Si nuestros datos se desvían en dirección de la \\(H_A\\) entonces podemos seguir con el procedimiento. En este caso, el valor-p de una prueba de \\(t\\) de una cola será el área que se encuentre más allá de \\(t_s\\). Para hacer una prueba de una cola en R simplemente necesitamos cambiar un argumento en t.test(alternative = \"greater\") o t.test(alternative = \"less\"). t.test(Conc ~ Grupo, ex2, alternative = &quot;less&quot;) ## ## Welch Two Sample t-test ## ## data: Conc by Grupo ## t = -2.8696, df = 17.675, p-value = 0.005164 ## alternative hypothesis: true difference in means between group Control and group Tolueno is less than 0 ## 95 percent confidence interval: ## -Inf -34.38172 ## sample estimates: ## mean in group Control mean in group Tolueno ## 458.72 545.74 t.test(Conc ~ Grupo, ex2, alternative = &quot;greater&quot;) ## ## Welch Two Sample t-test ## ## data: Conc by Grupo ## t = -2.8696, df = 17.675, p-value = 0.9948 ## alternative hypothesis: true difference in means between group Control and group Tolueno is greater than 0 ## 95 percent confidence interval: ## -139.6583 Inf ## sample estimates: ## mean in group Control mean in group Tolueno ## 458.72 545.74 Ahora bien, algo importante a recalcar es que existan diferencia significativas no quiere decir que el efecto sea realmente importante. Una prueba de hipótesis solamente responde a la pregunta ¿Hay suficiente diferencia entre ambas muestras como para inferir que vinieron de dos poblaciones distintas? sin embargo no nos dice qué tan grande o importante es la diferencia. 4.4.3 Tamaño de efecto Para medir la importancia de una diferencia uno debe de considerar la magnitud de la diferencia. El tamaño de efecto de un estudio es la diferencia entre \\(\\mu_{1}\\) y \\(\\mu_{2}\\), en relación con la desviación estándar de una de las poblaciones. \\[\\begin{equation} Tamaño \\space de \\space efecto = \\frac{|{\\overline{x}_1-\\overline{x}_2}|}{\\sigma} \\tag{4.14} \\end{equation}\\] Los resultados están dados en desviaciones estándar y el valor se conoce como d de Cohen. Para calcular la d de Cohen en R necesitamos un paquete estadístico llamado effsize y la función cohen.d(). library(effsize) cohen.d(Conc ~ Grupo, data = ex2, pooled_sd = F) ## ## Cohen&#39;s d ## ## d estimate: -1.283307 (large) ## 95 percent confidence interval: ## lower upper ## -2.3150540 -0.2515596 La ventaja de esta función es que nos dice si nuestro tamaño de efecto es grande o pequeño. Figura 4.8: Diferencias entre las medias de ambas muestras utilizadas en el ejercicio anterior. 4.4.4 Prueba de Wilcoxon-Mann-Whitney También es utilizada para comprar dos muestras. A diferencia de la prueba de \\(t\\), incluso cuando las distribuciones de las poblaciones no son normales. Este tipo de pruebas no se enfocan en un parámetro particular como la media o la mediana, por esta razón se conocen como pruebas no paramétricas. En el caso de esta prueba, una \\(H_0\\) generalmente empleada es que las distribuciones de \\(X_1\\) y \\(X_2\\) son iguales, y la \\(H_A\\) es que la distribución de \\(X_1\\) es distinta a la distribución de la población \\(X_2\\). Esta prueba es recomendable aplicarla cuando tenemos datos que no se pueden transformar a una distribución normal aunque realicemos transformaciones (como logaritmos u otra transformación). El estadístico de esta prueba se denota como \\(U_s\\) y mide el grado de separación entra ambas muestras. Lo primero es acomodar los datos en orden, de menor a mayor para ambas muestras. Posteriormente determinamos \\(K_1\\) y \\(K_2\\). Para \\(K_1\\), contamos el número de observaciones en la muestra 2 que son más pequeñas que la muestra 1. Si hay observaciones que sean iguales para ambas muestras, se cuenta como 1/2 punto. Luego contamos para \\(K_2\\) el número de observaciones en la muestra 1 que son menores que la muestra 2 y de igual forma, 1/2 si hay alguna observación con valor igual. Para corroborar si nuestros cálculos son correctos la suma de \\(K_1\\) y \\(K_2\\) debe ser igual al producto de \\(n_1 * n_2\\). El estadístico \\(U_s\\) va a ser el más grande de los dos \\(K_1\\) o \\(K_2\\). Para encontrar el valor-p se utilizan tablas de U. Aunque en R es más sencillo, utilizando la función wilcox.text(). Ejemplo: En un estudio farmacéutico, investigadores midieron la concentración de dopamina en el cerebro de seis ratas expuestas a tolueno y seis ratas que funcionaron como control. Las concentraciones se muestran en la tabla 4.3. Tolueno1 &lt;- c(3420, 2314, 1911, 2464, 2781, 2803) Control1 &lt;- c(1820, 1843, 1397, 1803, 2539, 1990) Dopamina &lt;- data.frame(Tolueno1, Control1) Tabla 4.3: Concentración de dopamina (ng/mg) y sus estadísticos descriptivos. Tolueno Control 3420 1820 2314 1843 1911 1397 2464 1803 2781 2539 2803 1990 Grupo n Media SD SE Control1 6 1898.667 371.073 151.490 Tolueno1 6 2615.500 513.798 209.757 Dopamina &lt;- gather(Dopamina, Tratamiento, Concentracion) #Organizamos los datos de manera que se puedan trabajar. wilcox.test(Concentracion ~ Tratamiento, data = Dopamina) ## ## Wilcoxon rank sum exact test ## ## data: Concentracion by Tratamiento ## W = 4, p-value = 0.02597 ## alternative hypothesis: true location shift is not equal to 0 El valor \\(W\\) es equivalente al estadístico \\(U_s\\). En este caso vemos que en nuestro ejemplo tenemos diferencias significativas entre ambos tratamientos ya que el \\(valor-p &lt; 0.05\\). Eso claro, siempre que elijamos un \\(\\alpha = 0.05\\). 4.5 Comparación de muestras pareadas En esta ocasión vamos a considerar dos muestras que no son independientes entre sí, si no que se encuentran pareadas. En un diseño pareado, las observaciones de (\\(X_1, X_2\\)) ocurren en pares. Esto quiere decir que las observaciones tienen más en común entre sí que con otros pares. 4.5.1 Prueba de \\(t\\) pareada Cuando tenemos datos pareados, cambiamos un poco el enfoque. En lugar de considerar a \\(X_1\\) y \\(X_2\\) por separado, consideramos su diferencia \\(D\\). \\[\\begin{equation} D = X_1 - X_2 \\tag{4.15} \\end{equation}\\] Cuando tenemos datos pareados, son las diferencias en sí lo que queremos registrar. La media de \\(D\\) es \\(\\overline{D}\\) y de igual manera se obtiene como una diferencia entre las medias de ambas muestras. \\[\\begin{equation} \\overline{D} = \\overline{X}_1 - \\overline{X}_2 \\tag{4.16} \\end{equation}\\] Lo mismo con la diferencia entre las medias de la población. \\[\\begin{equation} \\overline{D_\\mu} = \\mu_1 - \\mu_2 \\tag{4.17} \\end{equation}\\] Debido a esto podemos decir que la media de la diferencia es igual a la diferencia de las medias. Debido a esto podemos concentrarnos exclusivamente en \\(D\\). Ya que \\(\\overline{D}\\) es la media de una sola muestra, para obtener el error estándar simplemente aplicamos la fórmula (4.1). \\[\\begin{equation} SE_\\overline{D} = \\frac{s_D}{\\sqrt{n_D}} \\tag{4.18} \\end{equation}\\] Donde \\(s_{D}\\) y \\(n_D\\) son la desviación estándar y el tamaño de muestra de \\(D\\), respectivamente. Veamos esto con un ejemplo. Ejemplo: El adenosin monofosfato cíclico (cAMP) es una sustancia que regula la respuesta celular a las hormonas. En un estudio de maduración de ovarios en la rana Xenopus laevis, los oocitos de cada una de las 4 hembras fueron divdidos en dos grupos: un grupo fue expuesto a progesterona y el otro no. Después de 2 minutos, cada grupo fue analizado para encontrar el contenido de cAMP. Los datos se muestran en la siguiente tabla 4.4. Control &lt;- c(6.01, 2.28, 1.51, 2.12) Progesterona &lt;- c(5.23, 1.21, 1.40, 1.38) Datos1 &lt;- data.frame(Control, Progesterona) Tabla 4.4: Concentración de cAMP (pmol/oocito) Control Progesterona 6.01 5.23 2.28 1.21 1.51 1.40 2.12 1.38 Ahora que tenemos nuestros datos, lo primero es encontrar las diferencias, \\(d\\) entre los datos. Esto lo hacemos muy sencillo agregando una nueva columna $d a nuestra tabla. Datos1$d &lt;- Control - Progesterona #Encontramos d sacando la diferencia entre nuestros datos. Bien, ahora para poder encontrar la media y la desviación estándar de nuestras muestras de la diferencia \\(d\\) necesitamos transformar la tabla en un formato que nos permita hacer estos análisis. Afortunadamente la función gather() nos ayuda con eso, que es parte del paquete tidyr. Datos2 &lt;- gather(Datos1, Grupo, Concentracion) La función gather() nos ayuda a reorganizar los datos para tenerlos de manera en que podamos trabajar con ellos. Ahora con la propia librería de dplyr deberíamos poder encontrar la media, tanto para los tratamientos como para \\(d\\). Datos3 &lt;- Datos2 %&gt;% group_by(Grupo) %&gt;% summarise(n = n(), Media = mean(Concentracion), SD = round(sd(Concentracion), 3)) Tabla 4.5: Estadísticos descriptivos de cAMP Grupo n Media SD Control 4 2.980 2.047 d 4 0.675 0.404 Progesterona 4 2.305 1.952 En este caso tenemos ya la media de la diferencia \\(\\overline{d}\\) y la desviación estándar de la diferencia \\(s_D\\) así como el tamaño de muestra de la diferencia, en este caso \\(n_D\\). Nos quedaría calcular el error estándar de la diferencia \\(SE_{\\overline{D}}\\), que en este caso sería. \\[\\begin{equation} SE_\\overline{D} = \\frac{0.404}{\\sqrt{4}} = 0.202 \\end{equation}\\] Datos3 &lt;- Datos2 %&gt;% group_by(Grupo) %&gt;% summarise(n = n(), Media = mean(Concentracion), SD = round(sd(Concentracion), 3), SE = SD/sqrt(n)) Tabla 4.6: Estadísticos descriptivos incluyendo el error estándar Grupo n Media SD SE Control 4 2.980 2.047 1.0235 d 4 0.675 0.404 0.2020 Progesterona 4 2.305 1.952 0.9760 A partir de aquí podemos construir un intervalo de confianza para \\(D\\). La forma de hacerlo es igual a la que vimos anteriormente. \\[\\begin{equation} \\overline{d} \\pm t_{n_{D}-1, 0.025}SE_\\overline{D} \\tag{4.19} \\end{equation}\\] En donde \\(t_{n_{D}-1, 0.025}\\) es una constante determinada a partir de la distribución \\(t\\) de Student, utilizando como grados de libertad la fórmula \\(df = n_D - 1\\). Intervalos de confianza con otros coeficientes se construyen de la misma manera. Construir el intervalo de confianza correspondiente es relativamente sencillo. Vamos a utilizar la función t.test() pero con el argumento paired = TRUE. Datos4 &lt;- data.frame(Control, Progesterona) Datos4 &lt;- gather(Datos4, Grupo, Concentracion) t.test(Concentracion ~ Grupo, Datos4, paired = TRUE) ## ## Paired t-test ## ## data: Concentracion by Grupo ## t = 3.3387, df = 3, p-value = 0.04443 ## alternative hypothesis: true difference in means is not equal to 0 ## 95 percent confidence interval: ## 0.0315868 1.3184132 ## sample estimates: ## mean of the differences ## 0.675 De hecho, de esta manera obtenemos el intervalo de confianza, el valor crítico de \\(t\\), los grados de libertad, el valor-p, la media de las diferencias \\(\\overline{D}\\). Hay que tener en cuenta que en este caso nuestra hipótesis nula \\(H_0\\) tiene una connotación distinta. En este caso: \\[\\begin{equation} H_0 : \\mu_{d} = 0 \\\\ H_A : \\mu_{d} \\neq 0 \\end{equation}\\] Para nuestro ejemplo, nuestras hipótesis pueden ser las siguientes: \\(H_0\\): La media de la concentración de cAMP de los oocitos es la misma tanto con exposición o sin exposición a progesterona. \\(H_A\\): La media de la concentración de cAMP de los oocitos es distinta tras exponerse a progesterona a los que no fueron expuestos. En este caso nuestro \\(valor-p &lt; 0.05\\) por lo tanto tenemos suficiente evidencia para rechazar la \\(H_0\\). Si quisiéramos hacer el cálculo de manera manual, encontrar el valor crítico \\(t_s\\) se realiza con la siguiente fórmula: \\[\\begin{equation} t_s = \\frac{\\overline{d}-0}{SE_\\overline{D}} \\tag{4.20} \\end{equation}\\] 4.5.2 Prueba de signos El sign test o prueba de signos es un método no paramétrico para comparar dos muestras pareadas. También se basa en las diferencias \\(D = X_1 - X_2\\). La única información que se utiliza en esta prueba es el signo (positivo o negativo) de cada diferencia. El primer paso es contar el número de signos positivos \\(N_+\\) y negativos \\(N_-\\). El estadístico de la prueba de signos se conoce como \\(B_s\\) y dependiendo de nuestra hipótesis, \\(B_s = N_+\\) o \\(B_s = N_-\\). Posteriormente calculamos el valor-p. Como el estadístico \\(B_s\\) sigue una distribución binomial, podemos decir que \\(p\\) representa la probabilidad de que la diferencia sea positiva o negativa. La distribución nula de \\(B_s\\) es una distribución binomial con \\(n\\) = número de muestras y \\(p = 0.5\\). Entonces el valor-p para la prueba consiste en la probabilidad de obtener \\(B_s\\) o más diferencias, positivas o negativas. Esta prueba es aplicable en situaciones en las que la \\(H_0 : P(D \\space es \\space positiva) = 0.5\\) en los que se puede aplicar una distribución binomial. En esta prueba la forma de la distribución no es un factor importante. Para hacer una prueba de signos en R simplemente utilizamos la distribución binomial que vimos anteriormente. Ejemplo: Un investigador estudia la interacción entre dos subespecies de aves, el Junco de Carolina y el Junco Norteño. Puso a ambos en un aviario y observó el comportamiento durante 45 minutos. Este proceso fue repetido en varios días con distintos pares de individuos. Los resultados se muestran en la tabla 4.7. Norteno &lt;- c(0, 0, 0, 2, 0, 2, 1, 0) Carolina &lt;- c(9, 6, 22, 16, 17, 33, 24, 40) Aves &lt;- data.frame(Norteno, Carolina) Tabla 4.7: Número de veces que fue dominante un ave Norteno Carolina Signo 0 9 0 6 0 22 2 16 0 17 2 33 1 24 0 40 Para este ejemplo, \\(N_+ = 0\\) y \\(N_- = 8\\), por lo tanto \\(B_s = 8\\). Nuestro valor-p es la probabilidad de obtener 8 valores con signo -, en un experimento binomial con \\(n = 8\\). Para encontrar el valor-p podemos usar alguna tabla o la función dbinom(). 2*dbinom(8, 8, 0.5) #Multiplicamos por dos, ya que hacemos una prueba de signos de dos colas. ## [1] 0.0078125 Como podemos ver, el \\(valor-p = 0.008\\), por lo tanto el \\(valor-p &lt; 0.05\\) y aceptamos la \\(H_A\\) y vemos que hay diferencias significativas entre las dos subespecies en cuanto a la dominancia. También podemos hacer uso del paquete BSDA, `DescTools o signmedia.test con sus funciones correspondientes, como veremos en todos los casos obtenemos un resultado similar. Diferencia &lt;- Norteno - Carolina #Obtenemos la diferencia de nuestros datos Diferencia ## [1] -9 -6 -22 -14 -17 -31 -23 -40 library(BSDA) ## ## Attaching package: &#39;BSDA&#39; ## The following object is masked from &#39;package:datasets&#39;: ## ## Orange SIGN.test(Diferencia, md = 8) ## ## One-sample Sign-Test ## ## data: Diferencia ## s = 0, p-value = 0.007813 ## alternative hypothesis: true median is not equal to 8 ## 95 percent confidence interval: ## -33.925 -8.025 ## sample estimates: ## median of x ## -19.5 ## ## Achieved and Interpolated Confidence Intervals: ## ## Conf.Level L.E.pt U.E.pt ## Lower Achieved CI 0.9297 -31.000 -9.000 ## Interpolated CI 0.9500 -33.925 -8.025 ## Upper Achieved CI 0.9922 -40.000 -6.000 library(DescTools) SignTest(Diferencia, mu = 8) ## ## One-sample Sign-Test ## ## data: Diferencia ## S = 0, number of differences = 8, p-value = 0.007812 ## alternative hypothesis: true median is not equal to 8 ## 99.2 percent confidence interval: ## -40 -6 ## sample estimates: ## median of the differences ## -19.5 library(signmedian.test) signmedian.test(Diferencia, mu = 8) ## ## Exact sign test ## ## data: Diferencia ## #(x!=8) = 8, mu = 8, p-value = 0.007812 ## alternative hypothesis: the median of x is not equal to mu ## 92.96875 percent confidence interval: ## -31 -9 ## sample estimates: ## point estimator ## -19.5 Como podemos ver obtenemos el mismo valor que en nuestro código con la distribución binomial. 4.5.3 Prueba de rangos con signos de Wilcoxon Esta es una prueba para datos pareados que no siguen una distribución normal. También esta basado en las diferencias de \\(D = X_1 - X_2\\) y combina las ideas de mirar a los signos de las diferencias con mirar las magnitudes de las diferencias. El primer paso para una prueba de rangos con signos de Wilcoxon es calcular las diferencia \\(d\\) entre los datos pareados. Después de esto, buscamos el valor absoluto de las diferencias \\(|d|\\). Después de organizan estas diferencias de las menores a las mayores. Después de esto regresamos los signos que tenían previamente a los valores absolutos y sumamos los valores con símbolos positivos para formar \\(W_+\\) y los valores absolutos de los valores con símbolos negativos para formar \\(W_-\\). El estadístico \\(W_s\\) es el valor que sea más grande entre \\(W_+\\) y \\(W_-\\). Para encontrar el valor-p se pueden utilizar tablas. En este caso, basta con cambiar un argumento al comando wilcox.test(paired = TRUE). Esta prueba se puede llevar a cabo incluso cuando hay información incompleta. La hipótesis es similar al test del Wilcoxon para datos no pareados, \\(H_0 : \\mu_{D} = 0\\). Ejemplo: En una investigación acerca de posible daño cerebral debido al alcoholísmo, un procedimo de rayos-X conocido como tomografía computarizada (TC) fue utilizado para medir la densidad del cerebro en 11 bebedores de alcohol crónicos. Para cada alcohólico, se selecciono una persona no alcohólica para empatar en edad, sexo, educación, etc. Tabla 4.8: Diferencia de densidad entre personas alcohólicas y no alcohólicas Alcohólico No Alcohólico Diferencia 40.1 41.3 -1.2 38.5 40.2 -1.7 36.9 37.4 -0.5 41.4 46.1 -4.7 40.6 43.9 -3.3 42.3 41.9 0.4 37.2 39.9 -2.7 38.6 40.4 -1.8 38.5 38.6 -0.1 38.4 38.1 0.3 38.1 39.5 -1.4 wilcox.test(Densidad ~ Grupo, Densidad2, paired = TRUE) ## ## Wilcoxon signed rank exact test ## ## data: Densidad by Grupo ## V = 5, p-value = 0.009766 ## alternative hypothesis: true location shift is not equal to 0 Como podemos ver, nuestro valor-p quiere decir que hay diferencias significativas ya que \\(valor-p &lt; 0.05\\). Si quisiéramos usar una hipótesis unidireccional, podríamos escribir como argumento alternative = \"less\" para ver si el alcoholismo reduce la densidad del cerebro. wilcox.test(Densidad ~ Grupo, Densidad2, paired = TRUE, alternative = &quot;less&quot;) ## ## Wilcoxon signed rank exact test ## ## data: Densidad by Grupo ## V = 5, p-value = 0.004883 ## alternative hypothesis: true location shift is less than 0 "],["inferencia-de-datos-categóricos.html", "Lección 5 Inferencia de datos categóricos 5.1 Distribuciones de una muestra 5.2 Prueba de Chi-cuadrada 5.3 Prueba de Chi-cuadrada para tablas de contingencia 5.4 Interpretación de las tablas de contingencia 5.5 Prueba exacta de Fisher 5.6 Tablas de contingencia \\(r \\times k\\) 5.7 Intervalos de confianza para la diferencia entre probabilidades 5.8 Datos pareados en tablas 2x2", " Lección 5 Inferencia de datos categóricos 5.1 Distribuciones de una muestra En esta sección consideraremos variables categóricas, es decir, variables que solamente tienen dos posibles valores, y su distribución muestral de la proporción de la muestra. Normalmente, al tomar muestras de una población dicotómica grande, un estimado de la proporción, \\(p\\), es la muestra de la proporción \\(\\hat{p} = \\frac{x}{n}\\), donde \\(x\\) es el número de observaciones en la muestra que cumplen con el atributo de interés, y \\(n\\) es el tamaño de muestra. La proporción de Wilson ajustada a la muestra, \\(\\tilde{p}\\), es otro estimado que utiliza la fórmula (5.1). Esto equivale a sesgar la estimación hacia el valor 1/2. \\[\\begin{equation} \\tilde{p} = \\frac{x + 2}{n + 4} \\tag{5.1} \\end{equation}\\] La distribución binomial determina la distribución muestral de \\(\\tilde{P}\\). Veamos cómo funciona esto con un ejemplo. Ejemplo: En una región de Estados Unidos, el 17% de las máquinas dispensadoras de bebidas están contaminadas por la bacteria Chryseobacterium meningosepticum. Imaginemos que examinamos una muestra de dos máquinas dispensadoras. La probabilidad de tener dos máquinas contaminadas es \\(0.17 * 0.17 = 0.0289\\), la probabilidad de que ninguna este contaminada es de \\(1-0.17 * 1-0.17\\) y la probabilidad de que una este contaminada y la otra no es \\(0.17 * (1-0.17) + 0.17 * (1-0.17)\\). Una muestra que contenga cero máquinas contaminadas, \\(\\tilde{p} = \\frac{0+2}{2+4} = 0.33\\) ocurre con una probabilidad de 0.6889, una muestra con una máquina contaminada, \\(\\tilde{p} = \\frac{1+2}{2+4} = 0.5\\) ocurre con una probabilidad de 0.2822 y una muestra con dos máquinas contaminadas, \\(\\tilde{p} = \\frac{2+2}{2+4} = 0.67\\) ocurre con una probabilidad de 0.0289. Por ende, hay un 68.89% de probabilidad de que \\(\\tilde{P}\\) sea igual a 0.33, un 28% de que \\(\\tilde{P}\\) sea igual a 0.5 y 3% de que \\(\\tilde{P}\\) sea igual a 0.67. Imaginemos ahora que nuestra muestra pasa de \\(n = 2\\) a \\(n = 20\\), para encontrar la probabilidad de cuántas máquinas podríamos esperar que estén contaminadas en nuestra muestra, acudimos a la distribución binomial. Por ejemplo, para calcular que 5 de las 20 máquinas están contaminadas usamos \\(p = 0.17\\) y \\(n = 20\\). dbinom(5, 20, 0.17) ## [1] 0.1345426 Como vemos, la probabilidad de obtener 5 máquinas contaminadas es 0.1345, ahora, usando \\(\\tilde{P}\\), una muestra con 5 máquinas contaminadas sería \\(\\tilde{p} = \\frac{5+2}{20+4} = 0.2917\\), por lo que \\(P(\\tilde{P} = 0.2917) = 0.1345\\). Con la distribución binomial, podemos determinar toda la distribución muestral de \\(\\tilde{P}\\). Por ejemplo, podemos preguntar ¿Cuál es la probabilidad de que no más de 5 máquinas estén contaminadas? pbinom(5, 20, 0.17) ## [1] 0.8902245 Como vemos, la probabilidad de que no más de 5 máquinas estén contaminadas es de 0.8902245. Y como \\(\\tilde{p} = \\frac{5+2}{20+4} = 0.2917\\) entonces \\(P(\\tilde{P} \\leq 0.2917) = 0.8902\\). La distribución muestral de \\(\\tilde{P}\\) se puede utilizar para predecir cuánto error de muestreo esperar en nuestro estimado. De igual manera, entre mayor sea nuestro \\(n\\) mayor posibilidades hay de que \\(\\tilde{P}\\) se encuentre cercano a \\(p\\). Para la construcción de intervalos de confianza de la proporción de una población, necesitamos encontrar el error estándar de \\(\\tilde{P}\\). Para eso utilizamos la siguiente fórmula (para intervalos al 95%). \\[\\begin{equation} SE_\\tilde{p} = \\sqrt{\\frac{\\tilde{p}(1-\\tilde{p})}{n + 4}} \\tag{5.2} \\end{equation}\\] Posteriormente, realizamos el intervalo de confianza correspondiente con la siguiente fórmula (esto para un intervalo de confianza al 95%). \\[\\begin{equation} \\tilde{p} \\pm 1.96 \\times SE_{\\tilde{p}} \\tag{5.3} \\end{equation}\\] Al igual que con otros métodos, podemos construir un intervalo de confianza unilateral si así lo deseamos. Para eso podemos utilizar la función qnorm() en la cuál indicamos el valor de densidad (o probabilidad) que deseamos y así obtener un valor Z. qnorm(0.95, 0, 1) ## [1] 1.644854 Como podemos ver, para casos de intervalos de confianza unilaterales al 95% utilizaríamos un multiplicador de 1.645. \\[\\begin{equation} \\tilde{p} \\pm 1.645 \\times SE_{\\tilde{p}} \\tag{5.4} \\end{equation}\\] Las fórmulas para intervalos de confianza generales son las siguientes: \\[\\begin{equation} \\tilde{p} \\pm z_{\\alpha/2} \\times SE_{\\tilde{p}} \\tag{5.4} \\end{equation}\\] \\[\\begin{equation} \\tilde{p} = \\frac{x + 0.5(z^2_{\\alpha/2})}{n + z^2_{\\alpha/2}} \\tag{5.5} \\end{equation}\\] \\[\\begin{equation} SE_\\tilde{p} = \\sqrt{\\frac{\\tilde{p}(1-\\tilde{p})}{n + z^2_{\\alpha/2}}} \\tag{5.6} \\end{equation}\\] 5.2 Prueba de Chi-cuadrada Una prueba de bondad de ajuste se utiliza para ver la compatibilidad de los datos con una distribución o \\(H_0\\) específica. Una de las más utilizadas es la prueba de \\(\\chi^2\\) (léase chi-cuadrada). Para esto se utilizan frecuencias absolutas de nuestros datos. Para cada nivel de categoría, \\(i\\), dejamos que \\(o_i\\) represente la frecuencia observada y \\(e_i\\) la frecuencia esperada, es decir, la frecuencia que esperaríamos obtener bajo la \\(H_0\\). Las frecuencias esperadas \\(e_i\\) se obtienen multiplicando las probabilidades dadas por la \\(H_0\\) por \\(n\\). Posteriormente utilizamos la fórmula (5.7) para calcular nuestro estadístico de prueba. \\[\\begin{equation} \\chi^2 = \\sum_{i = 1}^{k}\\frac{(o_i - e_i)^2}{e_i} \\tag{5.7} \\end{equation}\\] Donde \\(k\\) es el número total de categorías e \\(i\\) es la i-ésima observación. Ejemplo: Después de seis meses de un incendio, los investigadores muestrearon una parcela de 3000 acres al rededor del área quemada. La dividieron en cuatro regiones: (1) el área cercana al centro del incendio, (2) el área interior cercana al incendio, (3) el área exterior cercana al incendio y (4) área fuera de la zona quemada. La \\(H_0\\) es que los venados no muestran preferencia por alguna de las áreas. La \\(H_A\\) es que muestran cierta preferencia por alguna de las áreas. Area &lt;- c(520, 210, 240, 2030) Zona &lt;- c(&quot;Área interior&quot;, &quot;Área exterior&quot;, &quot;Borde exterior&quot;, &quot;No quemado&quot;) Proporcion &lt;- round(Area/sum(Area), 3) Datos &lt;- data.frame(Zona, Area, Proporcion) Tabla 5.1: Áreas quemadas y no quemadas Zona Área Proporción Área interior 520 0.173 Área exterior 210 0.070 Borde exterior 240 0.080 No quemado 2030 0.677 Si la \\(H_0\\) es verdadera, entonces \\[\\begin{aligned} H_0: P(Área \\space interior) = \\frac{520}{3000} = 0.173 \\\\ P(Área \\space exterior) = \\frac{210}{3000} = 0.070 \\\\ P(Borde \\space exterior) = \\frac{240}{3000} = 0.080 \\\\ P(No \\space quemado) = \\frac{2030}{3000} = 0.677 \\end{aligned}\\] Entonces la \\(H_A\\) sería \\[\\begin{aligned} H_A: P(Área \\space interior) \\ne 0.173 \\\\ P(Área \\space exterior) \\ne 0.070 \\\\ P(Borde \\space exterior) \\ne 0.080 \\\\ P(No \\space quemado) \\ne 0.677 \\end{aligned}\\] Los investigadores encontraron 75 venados en el área, distribuidos en distintas zonas como se muestra a continuación. Venados &lt;- c(2, 12, 18, 43) Datos &lt;- data.frame(Datos, Venados) Figura 5.1: Zonas de avistamientos de venados, \\(n = 75\\). Tabla 5.2: Áreas quemadas y no quemadas y el número de venados observados Zona Área Proporción No. de venados Área interior 520 0.173 2 Área exterior 210 0.070 12 Borde exterior 240 0.080 18 No quemado 2030 0.677 43 Para este caso, las probabilidades estimadas (denotadas como \\(\\hat{p}\\)) son \\[\\begin{align} \\hat{P}(Área \\space interior) = \\frac{2}{75} = 0.027 \\\\ \\hat{P}(Área \\space exterior) = \\frac{12}{75} = 0.160 \\\\ \\hat{P}(Borde \\space exterior) = \\frac{18}{75} = 0.240 \\\\ \\hat{P}(No \\space quemado) = \\frac{43}{75} = 0.573 \\end{align}\\] Estas las podemos calcular fácilmente. Prop_venados &lt;- round(Venados/sum(Venados), 3) Datos &lt;- data.frame(Datos, Prop_venados) Tabla 5.3: Áreas quemadas y no quemadas y el número de venados observados con su proporción Zona Área Proporción esperada No. de venados Proporción observada Área interior 520 0.173 2 0.027 Área exterior 210 0.070 12 0.160 Borde exterior 240 0.080 18 0.240 No quemado 2030 0.677 43 0.573 Como podemos ver, las proporciones observadas \\(\\hat{P}\\) difieren de las que esperaríamos en el modelo de la \\(H_0\\). Figura 5.2: Valores observados contra esperados en las proporciones de venados en zonas quemadas. Ahora, para aplicar la fórmula de \\(\\chi^2\\) necesitamos los valores absolutos de nuestros datos, para esto simplemente multiplicamos la probabilidad calculada por la \\(H_0\\), \\(p * n\\). #Usamos la función sum() para sumar todos los valores de la variable &quot;Venados&quot;. Venados_esperados &lt;- Proporcion * sum(Datos$Venados) Venados_esperados ## [1] 12.975 5.250 6.000 50.775 Como podemos ver, los valores son aproximadamente 13, 5.25, 6 y 50.78. Ahora simplemente aplicamos la fórmula para nuestros datos. \\[\\begin{align} \\chi_s^2 = \\frac{(2-13)^2}{13} + \\frac{(12-5.25)^2}{5.25} + \\frac{(18-6)^2}{6} + \\frac{(43-50.78)^2}{50.78} = 43.2 \\end{align}\\] Ahora tenemos que considerar la distribución nula de \\(\\chi_s^2\\), que es la distribución muestral que se sigue en caso de que nuestra \\(H_0\\) sea verdadera. Esta distribución muestral sigue la distribución de chi-cuadrada \\(\\chi^2\\) cuando el tamaño de muestra es suficientemente grande. La forma de la distribución de \\(\\chi^2\\) depende de los grados de libertad. Figura 5.3: Distribución de \\(\\chi^2\\) con \\(df = 5\\) en azul, \\(df = 8\\) en rojo y \\(df = 10\\) en verde. Al igual que las demás distribuciones de probabilidad en R, tenemos una serie de comandos para encontrar los valores de una distribución de chi-cuadrada fácilmente. -dchisq() nos da un valor de densidad en determinado punto de la distribución de \\(\\chi^2\\). -pchisq() nos da un valor de densidad acumulado hasta cierto punto en la distribución de \\(\\chi^2\\) (área debajo de la curva). -qchisq() toma el valor de densidad que le ponemos como primer argumento y nos da como regreso un número cuya densidad acumulada empate con el valor de densidad ingresado. -rchisq() genera cierta cantidad de número aleatorios de acuerdo al valor de densidad. Sin embargo en este caso, en lugar de contar con el argumento n para el tamaño de muestra, contamos con otro argumento df para los grados de libertad. Por ejemplo, supongamos que quisiéramos encontrar el valor crítico \\(\\chi_{5,0.05}^2\\) cuando \\(df = 5\\), para esto utilizamos la función qchisq(). qchisq(0.95, 5) ## [1] 11.0705 Como podemos ver, el valor crítico es 11.07. Esto corresponde a un área de 0.05 por encima de la cola de la distribución de \\(\\chi^2\\). El cálculo de los grados de libertad \\(df\\) para una distribución de \\(\\chi^2\\) depende del número de categorías, \\(k\\). \\[\\begin{align} df = k - 1 \\tag{5.8} \\end{align}\\] Siguiendo el ejemplo de los venados, en este caso tenemos \\(k = 4\\) por lo que \\(df = 4 - 1 = 3\\). Ya que el valor que habíamos obtenido de nuestra fórmula de \\(\\chi_s^2 = 43.2\\), buscaremos el área debajo de la curva correspondiente a este valor. pchisq(43.2, 3, lower.tail = F) ## [1] 2.231754e-09 Como podemos ver el resultado es extremadamente pequeño y nuestro \\(valor-p &lt; 0.0001\\) por lo que tenemos evidencia para rechazar la \\(H_0\\) y aceptar la hipótesis de que los venados prefieren ciertas zonas sobre otras. La prueba de \\(\\chi^2\\) se puede utilizar para cualquier número de categorías \\(k\\). Ahora todo esto se facilita gracias a la función chisq.test() que viene incluida en R. Para realizar la prueba de Chi-cuadrada con esta función, lo único que necesitamos es una variable con el valor absoluto de nuestras observaciones y otra con los valores relativos esperados como los de la tabla 5.4. chisq.test(Datos$Venados, Datos$Propocion) ## ## Chi-squared test for given probabilities ## ## data: Datos$Venados ## X-squared = 48.787, df = 3, p-value = 1.448e-10 Estos valores los podemos obtener de la matriz de datos que habíamos creado previamente, simplemente los seleccionamos con el símbolo $ seguido del nombre de la variable ($Venados y $Proporcion, en mí caso). Tabla 5.4: Datos utilizados por la función chisq.test() en R Proporción esperada No. de venados 0.173 2 0.070 12 0.080 18 0.677 43 5.2.1 Hipótesis compuestas y direccionales Hay que darnos cuenta que en el ejemplo de los venados, nuestras hipótesis son hipótesis compuestas, es decir, hacen múltiples afirmaciones, a diferencia de las hipótesis que hacíamos en las pruebas de \\(t\\) (\\(H_0: \\mu_1 = \\mu_2\\)). Las primeras tres afirmaciones de nuestra hipótesis nula \\(H_0\\) son independientes, \\(P(Área \\space interior) = 0.173\\), \\(P(Área \\space exterior) = 0.070\\), \\(P(Borde \\space exterior) = 0.080\\) pero nuestra última afirmación, \\(P(No \\space quemado) = 0.677\\), es dependiente de las demás afirmaciones realizadas. Cuando esto ocurre, la hipótesis alternativa es necesariamente no direccional. Otra cosa interesante es que cuando se rechaza la \\(H_0\\), la prueba no nos dice la dirección de la conclusión. En cambio, cuando una variable es dicotómica, la \\(H_0\\) no es compuesta y por lo tanto, alternativas direccionales o conclusiones direccionales no suponen un problema. Para una variable dicotómica, una prueba de hipótesis direccional de Chi-cuadrada se realizaría de la misma manera que en pruebas pasadas. Primero elegimos la direccionalidad y corroboramos que los datos se desvían de la \\(H_0\\) en la dirección especificada por la \\(H_A\\). Si es así realizamos el siguiente paso. Encontramos el valor-p, el cuál será la mitad de lo que sería si nuestra \\(H_A\\) fuese no direccional. 5.3 Prueba de Chi-cuadrada para tablas de contingencia Una tabla de contingencia muestra los datos de tal manera que existe una asociación entre las variables de las filas y columnas. Ejemplo: Un grupo de 75 pacientes fueron asignados de manera aleatoria para recibir una cirugía para tratar dolores de migraña. La cirugía podía ser real (\\(n = 49\\)) o no (\\(n = 26\\)). Los cirujanos esperan una reducción en los dolores de migraña (éxito). En la tabla ?? se pueden observar los datos. #Creamos nuestra matriz de datos. cirugia &lt;- matrix(c(41, 15, 8, 11), nrow = 2, ncol = 2, byrow = T) dimnames(cirugia) &lt;- list(c(&quot;Éxito&quot;, &quot;Fracaso&quot;), c(&quot;Real&quot;, &quot;Falsa&quot;)) Como podemos ver, tenemos nuestra tabla de contingencia lista. Real Falsa Éxito 41 15 Fracaso 8 11 Posteriormente podríamos agregar los casos totales para cada lado de nuestra tabla de contingencia. Real Falsa Total Éxito 41 15 56 Fracaso 8 11 19 Total 49 26 75 Cuando hacemos una prueba de Chi-cuadrado para este tipo de tablas, nuestra hipótesis nula es que la probabilidad de éxito dado que la cirugía fue real es la misma que la probabilidad de éxito dado que la cirugía fue falsa, \\(H_0: P(Éxito|Real) = P(Éxito|Falsa)\\). Como vimos en la lección de probabilidad, este tipo de probabilidad se conoce como condicional y predice qué tan seguido un evento ocurre bajo condiciones específicas. \\[\\begin{equation} P(A|B) \\end{equation}\\] Esta fórmula nos dice cuál es la probabilidad de que A ocurra, dado que B ha ocurrido. Cuando una probabilidad condicional se estima de los datos, normalmente se denota con el símbolo ^. \\[\\begin{equation} \\hat{P}(A|B) \\end{equation}\\] Por ejemplo, para nuestros datos: \\[\\begin{align} \\hat{P}(Éxito|Real) = \\frac{41}{49} = 0.837 \\\\ \\hat{P}(Éxito|Falsa) = \\frac{15}{26} = 0.577 \\end{align}\\] Como podemos ver, estas probabilidades las obtenemos utilizando las columnas Real y Falsa de nuestra tabla de contingencia y sus respectivos totales. Pero, ¿es esta diferencia significativa? Para esto realizaremos la prueba de Chi-cuadrada. IMPORTANTE: No es lo mismo \\(P(Éxito|Real)\\) que \\(P(Real|Éxito)\\). Puedes comprobarlo tú mismo. \\[\\begin{align} \\hat{P}(Éxito|Real) = \\frac{41}{49} = 0.837 \\\\ \\hat{P}(Real|Éxito) = \\frac{41}{56} = 0.7321 \\\\ \\hat{P}(Éxito|Real) \\ne \\hat{P}(Real|Éxito) \\end{align}\\] Ahora, el primer paso para aplicar la prueba de Chi-cuadrada a nuestra tabla de contingencia es calcular las frecuencias marginales, es decir, las frecuencias de los totales de nuestra tabla de contingencia. Si nuestra \\(H_0\\) es verdadera, entonces las columnas Real y Falsa son equivalentes, así que podemos agruparlas (\\(41 + 15\\)) y dividirlas sobre el total de observaciones (\\(\\frac{56}{75} = 0.747\\)). Bien, ahora aplicamos esta probabilidad a nuestros valores observados para el grupo Real y Falso (\\(\\frac{56}{75} \\times 49 = 36.59\\) éxitos esperados y \\(\\frac{56}{75} \\times 26 = 36.59\\) éxitos esperados). De igual manera, aplicamos lo mismo para los fracasos esperados (\\(\\frac{19}{75} = 0.253\\)). Para obtener nuestros fracasos esperados obtenemos, multiplicamos este valor por los de los grupos Real y Falso (\\(\\frac{19}{75} \\times 49\\) fracasos esperados y \\(\\frac{19}{75} \\times 26\\) fracasos esperados). Real Falsa Total Éxito 41 (36.59) 15 (19.41) 56 Fracaso 8 (12.41) 11 (6.59) 19 Total 49 26 75 Ya que estos cálculos se realizan exclusivamente con los valores marginales, los valores esperados \\(e\\) se pueden obtener de la siguiente manera. \\[\\begin{align} e = \\frac{(Total \\space columna) \\times (Total \\space fila)}{Gran \\space total} \\tag{5.9} \\end{align}\\] Los grados de libertad para nuestra tabla de contingencia de 2x2 es de 1. ¿Por qué? Bueno, porque de las 4 celdas disponibles, solamente una de ellas es libre de variar. Ya que, por ejemplo, al determinar el valor esperado de la celda superior izquierda (36.59), el valor de la celda superior derecha tiene que ser 19.41, ya que la suma de ambas debe de ser 56. Lo mismo para la celda inferior izquierda (12.41) que al sumarse a la celda superior izquierda dan como resultado 49. Y la misma lógica aplica a la celda inferior derecha. Ya que la celda superior derecha es 19.41, la celda inferior tiene que ser 6.59 para sumar 26 en total. En este sentido, solo una celda es libre de variar y por ende \\(df = 1\\) para una tabla de contingencia de 2x2. Para este tipo de tablas, la hipótesis puede ser direccional o no direccional. En el caso de nuestro ejemplo, nos interesa saber si el efecto de la cirugía reduce el dolor de migraña (hipótesis direccional), así que al calcular el valor-p simplemente hay que dividirlo entre 2 y obtendremos el valor direccional. Ahora sí, podemos aplicar la fórmula de \\(\\chi^2\\). \\[\\begin{align} \\chi_s^2 = \\frac{(41-36.59)^2}{36.59} + \\frac{(15-19.41)^2}{19.41} + \\frac{(8-12.41)^2}{12.41} + \\frac{(11-6.59)^2}{6.59} = 6.06 \\end{align}\\] Todo este proceso se puede hacer de manera rapidísima con la función chisq.test(). Lo único que tenemos que hacer es poner nuestra matriz de datos y el argumento correct = F para evitar aplicar correcciones. chisq.test(cirugia, correct = F) ## ## Pearson&#39;s Chi-squared test ## ## data: cirugia ## X-squared = 6.0619, df = 1, p-value = 0.01381 Como vemos el valor-p es igual a 0.01381, pero como nuestra hipótesis es direccional debemos de dividirlo en 2. chisq.test(cirugia, correct = F)$p.value / 2 ## [1] 0.00690659 Por lo que obtenemos un \\(valor-p = 0.0069\\). En este caso, si nuestro \\(\\alpha = 0.01\\) rechazaríamos la \\(H_0\\) y diríamos que hay evidencias suficientes que apoyan que la cirugía real es mejor que la falsa para reducir los dolores de migraña. 5.4 Interpretación de las tablas de contingencia Una tabla de contingencia 2x2 se puede interpretar de dos maneras: Dos muestras independientes con una variable dicotómica. Una muestra con dos variable dicotómicas. Afortunadamente, la aritmética de la prueba de Chi-cuadrada es la misma para ambos contextos, pero la interpretación de la hipótesis y las conclusiones son distintas. En algunas tablas de contingencia las filas y columnas juegan un papel muy distinto. Por ejemplo, puede que las columnas sean un tratamiento y las filas una respuesta. En otros casos, las columnas y las filas pueden jugar papeles intercambiables. Cuando un conjunto de datos es visto como una sola muestra con dos variable dicotómicas, la relación expresada por \\(H_0\\) se conoce como dependencia estadística de las variables de columna y fila. Variables que no son independientes son llamadas dependientes o asociadas. Por eso a veces la prueba de Chi-cuadrada puede ser conocida como prueba de asociación o prueba de independencia. Ejemplo: Un antropólogo alemán que estidaba la relación entre el color de ojos y cabello observó una muestra de \\(n = 6800\\) hombres con los resultados siguientes. Cabello oscuro (CO) Cabello claro (CC) Total Ojo oscuro (OO) 726 131 857 Ojo claro (OC) 3129 2814 5943 Total 3855 2945 6800 En este caso, podemos calcular las probabilidades condicional de la siguiente manera: \\[\\begin{align} \\hat{P} = (OO|CO) = \\frac{726}{3855} = 0.19 \\\\ \\hat{P} = (OO|CC) = \\frac{131}{2945} = 0.04 \\end{align}\\] Pero la siguiente manera también es valida: \\[\\begin{align} \\hat{P} = (CO|OO) = \\frac{726}{857} = 0.85 \\\\ \\hat{P} = (CO|OC) = \\frac{3129}{5943} = 0.53 \\end{align}\\] Por lo tanto nuestra hipótesis nula se vería de las siguientes dos maneras, ambas válidas: \\[\\begin{align} H_0: P(OO|CO) = P(OO|CC) \\\\ H_0: P(CO|OO) = P(CO|OC) \\end{align}\\] Esto es lo mismo que decir que \\(H_0: El \\space color \\space de \\space ojos \\space es \\space independiente \\space del \\space color \\space de \\space cabello\\) o \\(H_0: El \\space color \\space del \\space cabello \\space es \\space independiente \\space del \\space color \\space de \\space ojos\\). Que es lo mismo que decir \\(H_0: El \\space color \\space de \\space ojos \\space y \\space el \\space color \\space del \\space cabello \\space son \\space independientes\\). La hipótesis nula de independencia puede decirse de la siguiente manera. Dos grupos, \\(G_1\\) y \\(G_2\\) son comparados con respecto de la probabilidad de la característica \\(C\\). La hipótesis nula es entonces: \\[\\begin{align} H_0: P(C|G_1) = P(C|G_2) \\end{align}\\] Ejemplo: Considere una especie ficticia de plantas, que se puede categorizar en pequeñas (S), y altas (T), así como resistentes (R) y no resistentes (NR) a una enfermedad. Consideremos las siguientes \\(H_0\\): \\(H_0: P(R|S) = P(R|T)\\) \\(H_0: P(NR|S) = P(NR|T)\\) \\(H_0: P(S|R) = P(S|NR)\\) \\(H_0: P(T|R) = P(T|NR)\\) Todas ellas son igual de válidas. Sin embargo la siguiente hipótesis no es valida. \\(H_0: P(R|S) = P(NR|S)\\) La primer hipótesis compara dos grupos (plantas pequeñas con altas) respecto a la resistencia a la enfermedad. mientras que la hipótesis 5 es una afirmación acerca de la distribución de la resistencia a la enfermedad en solo un grupo (plantas bajas). Supongamos ahora que hacemos un muestreo aleatorio de 100 plantas de esta población. plantas &lt;- matrix(c(12, 18, 28, 42), nrow = 2, ncol = 2, byrow = T) dimnames(plantas) &lt;- list(c(&quot;R&quot;, &quot;NR&quot;), c(&quot;S&quot;, &quot;T&quot;)) plantas ## S T ## R 12 18 ## NR 28 42 S T Total R 12 18 30 NR 28 42 70 Total 40 60 100 Vamos a corroborar las hipótesis antes generadas: \\(H_0: P(R|S) = P(R|T) = \\frac{12}{40} = \\frac{18}{60} = 0.30\\) \\(H_0: P(NR|S) = P(NR|T) = \\frac{28}{40} = \\frac{42}{60} = 0.70\\) \\(H_0: P(S|R) = P(S|NR) = \\frac{12}{30} = \\frac{28}{70} = 0.40\\) \\(H_0: P(T|R) = P(T|NR) = \\frac{18}{30} = \\frac{42}{70} = 0.30\\) Sin embargo, con la hipótesis 5. \\(H_0: P(R|S) = \\frac{12}{40} = 0.40\\) y \\(H_0: P(NR|S) = \\frac{28}{40} = 0.70\\). 5.4.1 Cuestiones acerca de las columnas y filas El hecho de que la tabla anterior muestre la independencia vista tanto por medio de las filas o columnas no es casualidad, como se demuestra en la siguiente tabla. Total a b a + b c d c + d Total a + c b + d Entonces, \\[\\begin{align} \\frac{a}{c} = \\frac{b}{d} \\end{align}\\] si y solo si \\[\\begin{align} \\frac{a}{b} = \\frac{c}{d} \\end{align}\\] Otra forma en la que esto se puede expresar es \\[\\begin{align} \\frac{a}{a+c} = \\frac{b}{b+d} \\end{align}\\] si y solo si \\[\\begin{align} \\frac{a}{a+b} = \\frac{c}{c+d} \\end{align}\\] Otra cuestión es que si \\(a\\), \\(b\\), \\(c\\) y \\(d\\) son números positivos, entonces \\[\\begin{align} \\frac{a}{a+c} &gt; \\frac{b}{b+d} \\end{align}\\] si y solo si \\[\\begin{align} \\frac{a}{a+b} &gt; \\frac{c}{c+d} \\end{align}\\] 5.5 Prueba exacta de Fisher Con esta prueba se encuentra la probabilidad de que la tabla de contingencia que tengamos, haya surgido por mero azar, dado que las probabilidades marginales de la tabla son fijas. Se utiliza especialmente con muestras pequeñas. La fórmula para calcular nuestro valor es la siguiente: \\[\\begin{align} p = \\frac{\\binom{a+b}{a}\\binom{c+d}{c}}{\\binom{n}{a+c}} = \\frac{(a+b)!(c+d)!(a+c)!(b+d)!}{a! \\space b! \\space c! \\space d! \\space n!} \\tag{5.10} \\end{align}\\] Donde \\(\\binom{n}{k}\\) es el coeficiente binomial (\\(_{n}C_{k}\\)). Para recordar este coeficiente aquí tenemos la fórmula. \\[\\begin{equation} _{n}C_{k} = \\frac{n!}{k!(n-k)!} \\tag{5.11} \\end{equation}\\] Por ejemplo, para el primer y segundo coeficiente binomial obtendríamos lo siguiente: \\[\\begin{align} _{a+b}C_{a} = \\frac{a+b!}{a!(a+b-a)!} \\\\ _{a+b}C_{a} = \\frac{c+d!}{c!(c+d-c)!} \\end{align}\\] Y para el último coeficiente, sería algo así: \\[\\begin{align} _{n}C_{a+c} = \\frac{n!}{a+c!(n-a+c)!} \\end{align}\\] Cuando hacemos una prueba exacta de Fisher para probar una hipótesis nula contra una hipótesis alternativa direccional, necesitamos encontrar la probabilidad de todas las tablas de datos (teniendo los mismos margenes que la tabla observada) que provean evidencia al menos tan fuerte en contra de \\(H_0\\), en la dirección predicha por la \\(H_A\\), como la tabla observada. Posteriormente calculamos el valor-p como la suma de las probabilidades de obtener tablas tan extremas como las obtenidas, si la \\(H_0\\) es verdad. Afortunadamente no tenemos que buscar los valores de todas las tablas posibles una por una y en su lugar podemos utilizar el comando fisher.test() que viene incluido en R. Ejemplo: La oxigenación extracorpórea por membrana (ECMO) es un procedimiento con el potencial de salvar vidas de bebés recién nacidos que sufren de una falla respiratoria severa. En un experimento se trataron a 29 bebés recién nacidos con ECMO y a 10 bebés recién nacidos con terapia médica convencional (CMT). #Creamos nuestra matriz de datos. Tratamientos &lt;- matrix(c(4, 1, 6, 28), nrow = 2, ncol = 2, byrow = T) dimnames(Tratamientos) &lt;- list(c(&quot;Muerto&quot;, &quot;Vivo&quot;), c(&quot;CMT&quot;, &quot;ECMO&quot;)) Tratamientos ## CMT ECMO ## Muerto 4 1 ## Vivo 6 28 CMT ECMO Total Muerto 4 1 5 Vivo 6 28 34 Total 10 29 39 fisher.test(Tratamientos) ## ## Fisher&#39;s Exact Test for Count Data ## ## data: Tratamientos ## p-value = 0.01102 ## alternative hypothesis: true odds ratio is not equal to 1 ## 95 percent confidence interval: ## 1.366318 944.080411 ## sample estimates: ## odds ratio ## 16.78571 Como podemos ver, el \\(valor-p = 0.01102\\) lo que quiere decir que rechazamos la \\(H_0\\) y aceptamos la \\(H_A: P(Muerto|ECMO) &lt; P(Muerto|CMT)\\). fisher.test(Tratamientos) ## ## Fisher&#39;s Exact Test for Count Data ## ## data: Tratamientos ## p-value = 0.01102 ## alternative hypothesis: true odds ratio is not equal to 1 ## 95 percent confidence interval: ## 1.366318 944.080411 ## sample estimates: ## odds ratio ## 16.78571 Para cambiar la direccionalidad de nuestra hipótesis basta con que agreguemos el argumento alternative = \"two.sided\" para una \\(H_A\\) no direccional y alternative = \"less\" o alternative = \"greater\" para una \\(H_A\\) direccional. 5.6 Tablas de contingencia \\(r \\times k\\) En esta sección consideraremos tablas de contingencia más grandes, con \\(r\\) filas y \\(k\\) columnas. Veamos el siguiente ejemplo. Ejemplo: Ecólogos que estudiaron los hábitats reproductivos del Chorlo Llanero (Charadrius montanus) durante 3 años y anotaron en dónde es que anidan. Encontraron 66 nidos en zonas agricolas (ZA), 67 en pastizales con perritos de las praderas (PP) y otros 20 en praderas (P). La siguiente tabla muestra las elecciones a través de los años. chorlos &lt;- matrix(c(21, 19, 26, 17, 38, 12, 5, 6, 9), nrow = 3, ncol = 3, byrow = T) dimnames(chorlos) &lt;- list(c(&quot;Zona Agricola (ZA)&quot;, &quot;Pastizal con Perritos (PP)&quot;, &quot;Praderas (P)&quot;), c(&quot;2004&quot;, &quot;2005&quot;, &quot;2006&quot;)) chorlos ## 2004 2005 2006 ## Zona Agricola (ZA) 21 19 26 ## Pastizal con Perritos (PP) 17 38 12 ## Praderas (P) 5 6 9 2004 2005 2006 Total Zona Agrícola (ZA) 21 19 26 66 Pastizal con Perritos (PP) 17 38 12 67 Praderas (P) 5 6 9 20 Total 43 63 47 153 En este caso, podemos transformar los valores absolutos en relativos haciendo uso del total que se encuentra en las columnas. Por ejemplo, en el caso de 2004 y la ZA tendríamos algo como \\(\\frac{21}{43} = 0.488\\) y así sucesivamente. 2004 2005 2006 Zona Agrícola (ZA) 48.8 30.2 55.3 Pastizal con Perritos (PP) 39.5 60.3 25.5 Praderas (P) 11.6 9.5 19.1 Total 99.9 100 99.9 Algunos totales no dan el 100 debido al redondeo. Podemos ver en la figura 5.4 las proporciones de cada una de las áreas de anidamiento correspondientes con cada año. Estos porcentajes que tenemos en la tabla son nuestras probabilidades condicionales, es decir \\(P(ZA|2004) = 0.488\\), \\(P(PP|2004) = 0.395\\), \\(P(P|2004) = 0.116\\) y así sucesivamente Figura 5.4: Gráfica con las proporciones de las zonas de anidamiento del Chorlo Llanero. Para esto simplemente aplicamos la fórmula de Chi-cuadrada. \\[\\begin{align} \\sum_{r \\times k} = \\frac{(o_i - e_i)^2}{e_i} \\tag{5.12} \\end{align}\\] Donde \\(r \\times k\\) son todas las celdas de nuestra tabla de contingencia. Los valores \\(e\\) se calculan con la fórmula vista anteriormente (5.9). Para los grados de libertad \\(df\\) utilizaríamos la siguiente fórmula: \\[\\begin{align} df = (r-1)(k-1) \\tag{5.13} \\end{align}\\] Nuestra \\(H_0\\) para los Chorlos sería que la preferencia de anidamiento es igual en todos los años y la \\(H_A\\) sería que son distintas. \\[\\begin{equation*} H_0: \\begin{Bmatrix} P(ZA|2004) = P(ZA|2005) = P(ZA|2006) \\\\ P(PP|2004) = P(PP|2005) = P(PP|2006) \\\\ P(P|2004) = P(P|2005) = P(P|2006) \\end{Bmatrix} \\end{equation*}\\] Igual que con la tabla de contingencia de 2x2, multiplicamos la probabilidad marginal por el total para obtener el valor esperado. Es decir, del total chorlos en la Zona Agrícola (ZA) \\(\\frac{66}{153} \\times 43 = 18.55\\) y así sucesivamente. 2004 2005 2006 Total Zona Agrícola (ZA) 21 (18.55) 19 (21.18) 26 (20.27) 66 Pastizal con Perritos (PP) 17 (18.83) 38 (27.59) 12 (20.58) 67 Praderas (P) 5 (5.62) 6 (8.24) 9 (6.14) 20 Total 43 63 47 153 Ahora simplemente aplicamos la fórmula de Chi-cuadrada para obtener nuestro estadístico \\(\\chi_s^2\\). \\[\\begin{equation} \\chi_s^2 = \\frac{(21-18.55)^2}{18.55} + \\frac{(19-21.18)^2}{21.18} + \\cdots + \\frac{(9-6.14)^2}{6.14} = 14.09 \\end{equation}\\] Nuestros \\(df\\) son los siguientes, dado que \\(r = 3\\) y \\(k = 3\\): \\[\\begin{equation} df = (3-1)(3-1) = 4 \\end{equation}\\] Si buscamos el valor de tablas, vemos que el valor-p se encuentra entre 0.01 y 0.001. Pero para facilitarnos la vida, simplemente ingresamos nuestra variable chorlos que creamos previamente a la función chisq.test() y obtenemos nuestro resultado. Podemos corroborar que el valor de \\(\\chi^2\\) calculado coincide al igual que los \\(df\\). Nuestro \\(valor-p = 0.007\\) por lo tanto, \\(valor-p &lt; 0.01\\), tenemos evidencias significativas con un \\(\\alpha = 0.01\\) de de que los Chorlos Llaneros cambiaron de sitio de anidación. Para estos casos, nuestra \\(H_A\\) siempre será no direccional. chisq.test(chorlos) ## ## Pearson&#39;s Chi-squared test ## ## data: chorlos ## X-squared = 14.089, df = 4, p-value = 0.007015 Al igual que con las tablas de contingencia 2x2, las tabla \\(r \\times k\\) pueden ser vistas bajo dos contextos. \\(k\\) muestras independientes; una variable categórica observada con \\(r\\) categorías Una muestra; dos variables categóricas observadas  una con \\(k\\) categorías y otra con \\(r\\) categorías. Y al igual que las tablas 2x2, el cálculo del valor \\(\\chi^2\\) es el mismo para ambos contexto pero la interpretación de las hipótesis cambia. 5.7 Intervalos de confianza para la diferencia entre probabilidades Un modo de análisis complementario para la prueba de Chi-cuadrada es la construcción de intervalos de confianza para la magnitud de diferencia (\\(p_1 - p_2\\)). Para esto vamos a definir nuevas estimaciones que están basadas en la idea de añadir 1 observación a cada celda de nuestra tabla. Consideremos una tabla de contingencia 2x2. Tabla 5.5: Dos muestra hipotéticas Muestra 1 Muestra 2 \\(x_1\\) \\(x_2\\) \\(n_1 - x_1\\) \\(n_2 - x_2\\) \\(n_1\\) \\(n_2\\) Definimos entonces \\[\\begin{equation} \\tilde{p_1} = \\frac{x_1 + 1}{n_1 + 2} \\\\ \\tilde{p_2} = \\frac{x_2 + 1}{n_2 + 2} \\end{equation}\\] Y usaremos la diferencia entre \\(\\tilde{p_1} - \\tilde{p_2}\\) para construir el intervalo de confianza para (\\(p_1 - p_2\\)). De igual manera, \\(\\tilde{P_1} - \\tilde{P_2}\\) esta sujeta a error de muestreo. Para calcular este error usamos la fórmula de error estándar para \\(\\tilde{P_1} - \\tilde{P_2}\\). \\[\\begin{equation} SE_{\\tilde{P_1} - \\tilde{P_2}} = \\sqrt{\\frac{\\tilde{p_1}(1-\\tilde{p_1})}{n_1 + 2} + \\frac{\\tilde{p_2}(1-\\tilde{p_2})}{n_2 + 2}} \\tag{5.14} \\end{equation}\\] Un intervalo de confianza al 95% sería de la siguiente manera. \\[\\begin{equation} (\\tilde{p_1} - \\tilde{p_2}) \\pm (1.96)SE_{\\tilde{P_1} - \\tilde{P_2}} \\end{equation}\\] Para calcular un intervalo de confianza de las proporciones o probabilidades en R utilizamos la función prop.test(). Para este comando es necesario que la matriz de datos sea 2x2. Por ejemplo. prop.datos &lt;- matrix(c(18, 23, 6, 16), nrow = 2, ncol = 2, byrow = T) prop.datos ## [,1] [,2] ## [1,] 18 23 ## [2,] 6 16 prop.test(prop.datos, correct = F) ## ## 2-sample test for equality of proportions without continuity ## correction ## ## data: prop.datos ## X-squared = 1.679, df = 1, p-value = 0.1951 ## alternative hypothesis: two.sided ## 95 percent confidence interval: ## -0.07392967 0.40652391 ## sample estimates: ## prop 1 prop 2 ## 0.4390244 0.2727273 5.8 Datos pareados en tablas 2x2 Para datos pareados en tablas de 2x2 necesitamos reconstruir nuestra tabla de cierta manera. Veamos esto con un ejemplo. Ejemplo: Se conduce un estudio para determinar el riesgo de que una mujer transmita el VIH a sus hijos no nacidos. De una muestra de 114 mujeres infectadas con VIH que dieron a luz a dos hijos, se encontró que la infección por VIH ocurrió en 19 de los 114 hijos mayores y en 20 de los 114 hijos menores. Hijo mayor Hijo menor VIH + 19 20 VIH - 95 94 Total 114 114 Como se puede ver, estos datos claramente están pareados ya que tanto los hijos menores como mayores provienen de la misma madre. Una forma de representar esta tabla es de la siguiente manera. Hijo menor con VIH + Hijo menor con VIH - Hijo mayor con VIH + 2 17 Hijo mayor con VIH - 18 77 Total 114 114 Como podemos ver, hay dos casos que corresponden a una respuesta sí/sí y 77 casos que corresponden a una respuesta no/no. Estos no nos ayudan a determinar si el VIH ocurre con mayor frecuencia en hijos menores que en mayores, y se les conoce como pares concordantes. Tenemos a su vez 17 casos sí/no y 18 no/sí. Estos se conocen como pares discordantes y nos ayudan a determinar si la infección por VIH ocurre con mayor frecuencia en uno de los dos casos (hijos mayores vs menores). Por lo tanto, nuestra \\(H_0:\\) la probabilidad de infección por VIH es la misma para hijos mayores y menores. \\[\\begin{equation} H_0: entre \\space pares \\space discordantes, \\space P(sí/no) = P(no/sí) = \\frac{1}{2} \\end{equation}\\] Para este tipo de pruebas se utiliza la prueba de Chi-cuadrada de McNemar. Tiene una forma sumamente sencilla de aplicarse. Imaginemos que \\(n_{11}\\) corresponde al número de pares sí/sí, \\(n_{12}\\) corresponde al número de pares sí/no, \\(n_{21}\\) al número de pares no/sí y \\(n_{22}\\) al número de pares no/no. + - + n11 n12 - n21 n22 La fórmula para encontrar nuestro estadístico \\(X_{s}^2\\) sería: \\[\\begin{equation} X_{s}^2 = \\frac{(n_{12} - \\frac{(n_{12} + n_{21})}{2})^2}{\\frac{(n_{12} - n_{21})}{2}} + \\frac{(n_{21} - \\frac{(n_{12} + n_{21})^2}{2})^2}{\\frac{(n_{12} - n_{21})^2}{2}} \\\\ \\tag{5.15} \\end{equation}\\] Esto se simplifica de la siguiente manera: \\[\\begin{equation} X_{s}^2 = \\frac{(n_{12} - n_{21})^2}{n_{12} + n_{21}} \\tag{5.16} \\end{equation}\\] Para realizar esta prueba en R, simplemente utilizamos el comando mcnemar.test() con el argumento con nuestra matriz de datos y el argumento correct = F, para evitar queR realice correcciones. VIH &lt;- matrix(c(2, 17, 18, 77), nrow = 2, ncol = 2, byrow = T) dimnames(VIH) &lt;- list(&quot;Hijo mayor&quot; = c(&quot;VIH +&quot;, &quot;VIH -&quot;), &quot;Hijo menor&quot; = c(&quot;VIH +&quot;, &quot;VIH -&quot;)) VIH ## Hijo menor ## Hijo mayor VIH + VIH - ## VIH + 2 17 ## VIH - 18 77 mcnemar.test(VIH, correct = F) ## ## McNemar&#39;s Chi-squared test ## ## data: VIH ## McNemar&#39;s chi-squared = 0.028571, df = 1, p-value = 0.8658 "],["modelado-de-relaciones.html", "Lección 6 Modelado de relaciones 6.1 Análisis de Varianza (ANOVA) 6.2 Diseño por bloques aleatorizados 6.3 ANOVA de dos vías 6.4 Combinaciones lineales de las medias 6.5 Pruebas post hoc", " Lección 6 Modelado de relaciones En esta lección vamos a considerar la comparación de más de dos muestras y los diversos métodos que podemos emplear para esto. 6.1 Análisis de Varianza (ANOVA) El análisis clásico para realizar observaciones de múltiples muestras es el análisis de varianza o ANOVA. Para llevar a cabo este análisis existen ciertos supuestos que nuestros datos deben de cumplir. Distribución normal de los datos. Homocedasticidad (varianzas iguales). Datos independientes. La distribución normal y la homocedasticidad son hasta cierto punto flexibles. Sin embargo, el ANOVA es sumamente sensitivo a muestras no independientes. Pese a su nombre, el análisis de varianza en realidad busca diferencias en las medias (\\(\\mu\\)) de las muestras. Empezaremos con el ANOVA de una vía, es decir, un ANOVA en el cuál una sola variable define los grupos o tratamientos. El ANOVA de \\(k\\) muestras o grupos inicia con el cálculo de las cantidades que describen la variabilidad de los datos entre los grupos y dentro de los grupos. Antes de empezar con esto, veamos un poco de notación utilizada en un ANOVA. \\(x_{ij}\\): observación \\(j\\) en el \\(i\\)-ésimo grupo. Por ejemplo la primer observación del primer grupo o muestra sería \\(x_{11}\\), la segunda \\(x_{12}\\) y así sucesivamente. \\(k\\): Número total de grupos, muestras o tratamientos. \\(n_i\\): número de observaciones en el \\(i\\)-ésimo grupo. \\(\\overline{x}_i\\): la media del \\(i\\)-ésimo grupo. \\(s_i\\): la desviación estándar del \\(i\\)-ésimo grupo. \\(n\\): el número total de observaciones, dado por la fórmula: \\[\\begin{equation} n = \\sum_{i = 1}^k n_i \\end{equation}\\] \\(\\overline{x}\\): la gran media o promedio de todas las observaciones, dado por la fórmula: \\[\\begin{equation} \\overline{x} = \\frac{\\sum_{i = 1}^k \\sum_{j = 1}^{n_i}n_{ij}}{n} = \\frac{\\sum_{i = 1}^k n_{i} \\overline{x}_i}{\\sum_{i}^k n_i} = \\frac{\\sum_{i = 1}^k n_{i} \\overline{x}_i}{n} \\end{equation}\\] 6.1.1 Variación dentro de los grupos Una medida de variación dentro de los grupos es la varianza agrupada, denotada como \\(s_{p}^2\\). Su cálculo se obtiene de la siguiente manera: \\[\\begin{equation} s_{p}^2 = \\frac{\\sum_{i=1}^k (n_i - 1)s_i^2}{\\sum_{i=1}^k(n_i - 1)} = \\frac{\\sum_{i=1}^k (n_i - 1)s_i^2}{n - k} \\tag{6.1} \\end{equation}\\] Mientras que para obtener la desviación estándar agrupadas simplemente obtenemos la raíz cuadrada. \\[\\begin{equation} s_{p} = \\sqrt{\\frac{\\sum_{i=1}^k (n_i - 1)s_i^2}{\\sum_{i=1}^k(n_i - 1)}} = \\sqrt{\\frac{\\sum_{i=1}^k (n_i - 1)s_i^2}{n - k}} \\tag{6.2} \\end{equation}\\] El valor de la desviación estándar agrupada solamente depende de la variabilidad dentro de los datos y no de sus medias. Por ahora, vamos a asignar términos correspondientes al ANOVA a nuestra fórmula. El numerador de la varianza agrupada se conoce como suma de cuadrados dentro de los grupos, \\(SS(dentro)\\) mientras que el denominador son los grados de libertad dentro de los grupos, \\(df(dentro)\\). Así, entonces: \\[\\begin{equation} SS(dentro) = \\sum_{i = 1}^k (n_i - 1)^2 \\tag{6.3} \\end{equation}\\] \\[\\begin{equation} df(dentro) = n - k \\tag{6.4} \\end{equation}\\] La razón entre estos dos valores se conoce como cuadrados medios dentro de los grupos, \\(MS(dentro)\\). \\[\\begin{equation} MS(dentro) = \\frac{SS(dentro)}{df(dentro)} \\tag{6.5} \\end{equation}\\] Por lo tanto \\(MS(dentro)\\) mide la variabilidad dentro de los grupos. 6.1.2 Variación entre los grupos En los casos en los que solamente tenemos dos grupos, la diferencia es simplemente \\((\\overline{x}_1 - \\overline{x}_2)\\). Sin embargo, en este caso tenemos más dos grupos que queremos comparar. Para este caso utilizamos los cuadrados medios entre los grupos, \\(MS(entre)\\). Su cálculo se obtiene de la siguiente manera: \\[\\begin{equation} MS(entre) = \\frac{\\sum_{i=1}^k n_i(\\overline{x}_i - \\overline{x})^2}{k - 1} \\tag{6.6} \\end{equation}\\] De igual manera, el numerador y denominador tienen sus nombres respectivos. Al numerador se le conoce coo suma de cuadrados entre los grupos, \\(SS(entre)\\) y al denominador como grados de libertad entre los grupos, \\(df(entre)\\). \\[\\begin{equation} SS(entre) = \\sum_{i=1}^k n_i(\\overline{x}_i - \\overline{x})^2 \\tag{6.7} \\end{equation}\\] \\[\\begin{equation} df(dentro) = k - 1 \\tag{6.8} \\end{equation}\\] 6.1.3 Una relación importante en el ANOVA El nombre análisis de varianza viene de la comparación entre \\(SS(entre)\\) y \\(SS(dentro)\\). Consideremos un valor, \\(x_{ij}\\). \\[\\begin{equation} x_{ij} - \\overline{x} = (x_{ij} - \\overline{x}_i) + (x_{ij} - \\overline{x}) \\end{equation}\\] Esta ecuación expresa la desviación que hay en una observación \\((x_{ij})\\) de la gran media \\((\\overline{x})\\) como la suma de dos partes: una desviación dentro del grupo \\((x_{ij} - \\overline{x}_i)\\) y una desviación entre grupos \\((\\overline{x}_i - \\overline{x})\\). Esta relación se mantiene para la suma de cuadrados correspondiente: \\[\\begin{equation} \\sum_{i=1}^k\\sum_{j=1}^{n_i}(x_{ij} - \\overline{x})^2 = \\sum_{i=1}^k \\sum_{j=1}^{n_i}(x_{ij} - \\overline{x}_i)^2 + \\sum_{i=1}^k\\sum_{j=1}^{n_i}(x_{ij} - \\overline{x})^2 \\tag{6.9} \\end{equation}\\] Que, al reescribirse se puede expresar como: \\[\\begin{equation} \\sum_{i=1}^k\\sum_{j=1}^{n_i}(x_{ij} - \\overline{x})^2 = \\sum_{i=1}^k (n_i - 1)s_{i}^2 + \\sum_{i=1}^k n_i (\\overline{x}_i - \\overline{x})^2 \\tag{6.10} \\end{equation}\\] Esta cantidad se conoce como suma de cuadrados totales, \\(SS(total)\\). \\[\\begin{equation} SS(total) = \\sum_{i=1}^k\\sum_{j=1}^{n_i}(x_{ij} - \\overline{x})^2 \\tag{6.11} \\end{equation}\\] Este valor mide la cantidad de variabilidad entre todas las \\(n\\) observaciones en los grupos \\(k\\). \\[\\begin{equation} SS(total) = SS(entre) + SS(dentro) \\tag{6.12} \\end{equation}\\] Esta relación anterior muestra cómo la variación total de nuestros datos puede dividirse o partirse en dos componentes: la variación entre grupos y la variación dentro de los grupos (de ahí el nombre, análisis de varianza). Para los grados de libertad totales \\(df(total)\\) se utiliza la siguiente fórmula: \\[\\begin{equation} df(total) = n - 1 \\tag{6.13} \\end{equation}\\] Y al igual que la suma de cuadrados, se pueden calcular sumando los grados de libertad entre y dentro de los grupos: \\[\\begin{equation} df(total) = df(dentro) + df(entre) \\\\ df(total) = (n - k) + (k - 1) \\tag{6.14} \\end{equation}\\] 6.1.4 Tabla de ANOVA Cuando se realiza un ANOVA, es una práctica muy común realizar una tabla de ANOVA. En estas tablas vamos anotando los resultados de nuestras sumas de cuadrados, nuestros grados de libertad y nuestros cuadrados medios, de la siguiente manera. Tabla 6.1: Tabla de ANOVA Valores df SS MS Entre los grupos (Tratamientos) \\(k - 1\\) \\(\\sum_{i = 1}^{k}n_{i}(\\overline{x}_{i} - \\overline{x})^2\\) \\(\\frac{SS(entre)}{df(entre)}\\) Dentro de los grupos (Residuales) \\(n - k\\) \\(\\sum_{i = 1}^{k}\\sum_{j = 1}^{n_i}(x_{ij} - \\overline{x}_{i})^2\\) \\(\\frac{SS(dentro)}{df(dentro)}\\) Total \\(n - 1\\) \\(\\sum_{i = 1}^{k}\\sum_{j = 1}^{n_i}(x_{ij}-\\overline{x})^2\\) Una tabla de ANOVA nos ayuda a organizar los datos y los valores de nuestro conjunto de datos. En la tabla 6.1 podemos ver las fórmulas que necesitamos utilizar para obtener los valores correspondientes. Por ejemplo, para obtener los grados de libertad entre tratamientos,\\(df(entre)\\), utilizaría la fórmula \\(k - 1\\) y así sucesivamente. La suma de cuadrados y los cuadrados medios entre grupos son los que corresponden a los tratamientos y los que se dan dentro de los grupos son típicamente conocidos como residuales. 6.1.5 Modelo del ANOVA Pensamos acerca de \\(x_{ij}\\) como una observación aleatoria del grupo \\(i\\), donde la media poblacional del grupo \\(i\\) es \\(\\mu_i\\). Utilizamos el ANOVA para investigar la hipótesis nula, \\(H_0: \\mu_1 = \\mu_2 = \\cdots = \\mu_i\\). El siguiente modelo describe al ANOVA: \\[\\begin{equation} x_{ij} = \\mu + \\tau_i + \\epsilon_{ij} \\tag{6.15} \\end{equation}\\] En este modelo, \\(\\mu\\) representa la gran media poblacional, es decir la media de todos los grupos combinados. Si la \\(H_0\\) entonces \\(\\mu\\) es la media en común para todos los grupos. Si es falsa, entonces al menos una de las \\(\\mu_i\\) difiere de la gran media poblacional \\(\\mu\\). El término \\(\\tau_i\\) (tau) representa el efecto que tiene el grupo \\(i\\), es decir, la diferencia entre la media poblacional para el grupo \\(i\\), \\(\\mu_i\\) y la gran media poblacional, \\(\\mu\\). Por ende: \\[\\begin{equation} \\tau_i = \\mu_i - \\mu \\tag{6.16} \\end{equation}\\] Esto es equivalente a decir que \\(H_0: \\tau_1 = \\tau_2 = \\cdots = \\tau_n = 0\\). Si la \\(H_0\\) es falsa, entonces al menos alguno de los grupos o tratamientos difiere de los demás. Si \\(\\tau_i\\) es positivo, entonces, las observaciones del grupo \\(i\\) tienden a ser mayores que el promedio, en cambio, si es negativo, las observaciones tienden a ser menores que el promedio. El término \\(\\epsilon_{ij}\\) (epsilon) representa el error aleatorio asociado con la observación \\(j\\) en el grupo \\(i\\). Por lo que nuestro modelo puede ser interpretado como: \\[\\begin{equation} observación = gran \\space media + efecto \\space de \\space grupo + error \\space aleatorio \\tag{6.17} \\end{equation}\\] Estimamos la media general, \\(\\mu\\), con la gran media de los datos \\(\\overline{x}\\): \\[\\begin{equation} \\hat{\\mu} = \\overline{x} \\end{equation}\\] De igual manera, estimamos la media poblacional del grupo \\(i\\) con la media muestral del grupo \\(i\\): \\[\\begin{equation} \\hat{\\mu}_i = \\overline{x}_i \\end{equation}\\] Entonces, efecto de grupo es: \\[\\begin{equation} \\hat{\\tau}_i = \\overline{x}_i - \\overline{x} \\end{equation}\\] Finalmente, para el error aleatorio de la observación \\(j\\) en el grupo \\(i\\): \\[\\begin{equation} \\hat{\\epsilon}_{ij} = x_{ij} - \\overline{x}_i \\end{equation}\\] Poniendo todos estos valores juntos obtenemos: \\[\\begin{equation} x_{ij} = \\overline{x} + (\\overline{x}_i - \\overline{x}) + (x_{ij} + \\overline{x}_i) = \\hat{\\mu} + \\hat{\\tau_i} + \\hat{\\epsilon}_{ij} \\end{equation}\\] En algunos libros e incluso algunos software, se utiliza la terminología \\(SS(error)\\) en lugar de \\(SS(dentro)\\) ya que este componente, \\(x_{ij} - \\overline{x}_i\\), es la parte del error aleatorio en el modelo del ANOVA. También se le conoce como residuales y de está manera la encontramos en R. Cuando hacemos un análisis de varianza, ANOVA, comparamos el tamaño del efecto de grupo de la muestra, \\(\\hat{\\tau}_i\\), al tamaño del error aleatorio de los datos, \\(\\hat{\\epsilon}_{ij}\\). Esto se puede observar en las fórmulas para \\(SS(entre)\\) y \\(SS(dentro)\\). \\[\\begin{equation} SS(entre) = \\sum_{i=1}^k n_i \\hat{\\tau}_i^2 \\end{equation}\\] \\[\\begin{equation} SS(dentro) = \\sum_{i=1}^k \\sum_{j=1}^{n_i} \\hat{\\epsilon}_{ij}^2 \\end{equation}\\] 6.1.6 Distribución de \\(F\\) Consideremos la hipótesis nula \\(H_0: \\mu_1 = \\mu_2 = \\cdots = \\mu_i\\) y una hipótesis alternativa no direccional, \\(H_A: Al \\space menos \\space una \\space \\mu_i \\space no \\space es \\space igual\\). El rechazo de la \\(H_0\\) no nos dice cuál grupo es el que es diferente. La distribución de \\(F\\) fue nombrada así en honor del estadista y genetista R.A. Fisher y es utilizada en muchísimos análisis estadísticos. Su forma depende de dos parámetros: el numerador de los grados de libertad (\\(df(entre)\\)) y el denominador de los grados de libertad (\\(df(dentro)\\)). La siguiente figura muestra una distribución de \\(F\\) con \\(df(entre) = 4\\) y \\(df(dentro) = 20\\). Figura 6.1: Distribución de \\(F\\) con \\(df(entre) = 4\\) y \\(df(dentro) = 20\\). Como con el resto de distribuciones, tenemos una serie de comandos en R para trabajar con la distribución de \\(F\\). -df() nos da un valor de densidad en determinado punto de la distribución de \\(F\\). -pf() nos da un valor de densidad acumulado hasta cierto punto en la distribución de \\(F\\) (área debajo de la curva). -qf() toma el valor de densidad que le ponemos como primer argumento y nos da como regreso un número cuya densidad acumulada empate con el valor de densidad ingresado. -rf() genera cierta cantidad de número aleatorios de acuerdo al valor de densidad. Para encontrar los valores críticos de nuestra distribución de \\(F\\) simplemente utilizamos la función qf(). qf(0.05, df1 = 4, df = 20, lower.tail = F) ## [1] 2.866081 Y listo, nuestro valor crítico de \\(F\\) para un \\(F(4, 20)_{0.05}\\) es 2.87. Estos comandos se utilizan como el resto que hemos visto con las demás distribuciones. 6.1.7 Prueba de \\(F\\) Para obtener nuestro estadístico de \\(F\\) utilizamos la siguiente fórmula: \\[\\begin{equation} F_s = \\frac{MS(entre)}{MS(dentro)} \\tag{6.18} \\end{equation}\\] Esta claro que el valor de nuestra \\(F_s\\) será mayor si las discrepancias entre las medias de los grupos (\\(\\overline{X}_i\\)) son grandes en relación a variabilidad dentro de los grupos. Valores grandes de \\(F_s\\) tienden a proveer evidencia en contra de la \\(H_0\\). Veamos todo lo que hemos visto con un ejemplo. Ejemplo: Se realizó un experimento sobre la alimentación de corderos con tres dietas distintas. Las ganancias de peso se muestran en la tabla @tab:ex1. Tabla 6.2: Ganancia de peso (lb) de los corderos Dieta 1 Dieta 2 Dieta 3 8 9 15 16 16 10 9 21 17 11 6 18 En lugar de realizar todo el proceso manual (el cuál puede tomar cierto tiempo) vamos a utilizar la función aov() para analizar estos datos y ver si realmente existe alguna diferencia entre los datos. Para esto necesitamos generar nuestra variable con nuestros datos. Tratamiento &lt;- c((rep(&quot;Dieta 1&quot;, 3)), c(rep(&quot;Dieta 2&quot;, 5)), c(rep(&quot;Dieta 3&quot;, 4))) Peso &lt;- c(8, 16, 9, 9, 16, 21, 11, 18, 15, 10, 17, 6) Corderos &lt;- data.frame(Tratamiento, Peso) Corderos ## Tratamiento Peso ## 1 Dieta 1 8 ## 2 Dieta 1 16 ## 3 Dieta 1 9 ## 4 Dieta 2 9 ## 5 Dieta 2 16 ## 6 Dieta 2 21 ## 7 Dieta 2 11 ## 8 Dieta 2 18 ## 9 Dieta 3 15 ## 10 Dieta 3 10 ## 11 Dieta 3 17 ## 12 Dieta 3 6 Ahora que tenemos nuestra variable simplemente ingresamos el siguiente comando. ANOVA &lt;- aov(Peso ~ Tratamiento, Corderos) Si queremos observar nuestros valores-p calculados, y ver si existen diferencias significativas, tenemos que guardar el análisis en una variable, en este caso yo la nombre ANOVA. Posteriormente utilizamos la función summary(). summary(ANOVA) ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## Tratamiento 2 36 18.00 0.771 0.491 ## Residuals 9 210 23.33 Donde obtenemos una tabla de ANOVA con nuestros valores F esperados y F estimados. El apartado Pr(&gt;F) es nuestro valor-p. Si quisiéramos guardar nuestra tabla de ANOVA para algún reporte, podemos hacerlo a través de la librería broom. library(broom) ANOVA.tabla &lt;- tidy(ANOVA) Tabla 6.3: Ganancia de peso (lb) de los corderos Término df SS MS Estadístico de F Valor p Tratamiento 2 36 18.00000 0.7714286 0.490658 Residuals 9 210 23.33333 Por lo tanto nuestro estadístico \\(F\\) calculado nos da un valor de 0.77, el área correspondiente a esta densidad valor \\(F\\) es 0.4907, por lo que nuestro valor-p &gt; 0.05 y aceptamos la \\(H_0\\). 6.2 Diseño por bloques aleatorizados En un diseño por bloques aleatorizados, primero agrupamos las unidades experimentales en bloques de unidades relativamente similares y después alocamos tratamientos de manera aleatoria dentro de cada bloque. Para llevar a cabo este tipo de diseños, el investigador debe de crear o identificar bloques adecuados de unidades experimentales y posteriormente asignar tratamientos de manera aleatoria dentro de cada bloque de tal manera que cada tratamiento aparece en el bloque (al menos para un diseño de bloques completo). ¿Por qué crear bloques? Bueno este procedimiento ayuda a reducir o eliminar variabilidad causada por variables extrañas, por lo tanto, aumenta la precisión de los experimentos. Podemos observar el diseño experimental de manera tabular, de la siguiente manera. T1 T2 T3 Bloque 1 X1,1 X1,2 X1,3 Bloque 2 X2,1 X2,2 X2,3 Bloque 3 X3,1 X3,2 X3,3 . . . . . . . . . . . . Bloque 10 X10,1 X10,2 X10,3 Donde \\(X_{i,j}\\) representa la observación \\(i\\) que recibió el tratamiento \\(j\\). 6.2.1 Creando los bloques La creación de los bloques es una forma de organizar la variación inherente que existe entre las unidades experimentales. De manera ideal, los bloques incrementan la información disponible del experimento. Para esto se deben crear bloques lo más homogéneos posibles entre sí para que la variación inherente entre las unidades experimentales sea, dentro de lo posible, variación entre bloques más que dentro de los bloques. Una vez que los bloques se han generado, lo que ocurre dentro de cada bloque es como un mini-experimento. La aleatorización se hace para cada bloque de manera separada. Para el análisis de los bloques utilizamos un ANOVA por bloques aleatorizados. Consideremos el siguiente ejemplo: Ejemplo: Un agrónomo se encuentra comparando variedades de cebada, por lo que genera distintas parcelas de cada variedad para así medir su eficiencia. Sin embargo, el arreglo espacial de las parcelas es muy importante, ya que pueden surgir diferencias por el tipo de suelo, pH, etc., por lo que el área de cultivo fue dividida en varias regiones (los bloques) y después, cada bloque fue subdividido en distintas parcelas. Dentro de cada bloque las variades de granos son sembradas de manera aleatoria, con una aleatorización echa para cada bloque. Se obtienen los siguientes resultados. Tabla 6.4: Rendimiento (lb) de las variedades de cebada Bloque 1 Bloque 2 Bloque 3 Bloque 4 Media de la variedad Variedad 1 93.5 66.6 50.5 42.4 63.3 Variedad 2 102.9 53.2 47.4 43.8 61.8 Variedad 3 67.0 54.7 50.0 40.1 53.0 Variedad 4 86.3 61.3 50.7 46.4 61.2 Media del bloque 87.4 59.0 49.7 43.2 Como podemos ver en los datos, existe claramente un gradiente de mayor fertilidad, partiendo del bloque 1 al bloque 4. Gracias al diseño por bloques aún podemos realizar la comparación entre las variedades. La hipótesis nula de nuestro ANOVA es que las medias de nuestras poblaciones son iguales, \\(H_0: \\mu_1 = \\mu_2 = \\cdots = \\mu_k\\). Por lo que nuestra hipótesis nula sería que todas las variedades son iguales. El modelo de ANOVA por bloques aleatorizados sufre ciertas alteraciones al modelo de una vía. \\[\\begin{equation} x_{ijk} = \\mu + \\tau_i + \\beta_{j} + \\epsilon_{ijk} \\tag{6.19} \\end{equation}\\] Donde \\(x_{ijk}\\) representa la \\(k\\)-ésima observación, cuando el tratamiento \\(i\\) es aplicado al bloque \\(j\\). \\(\\mu\\) representa la gran media poblacional, y \\(\\tau_i\\) el efecto del grupo \\(i\\). El nuevo término, \\(\\beta_{j}\\) representa el efecto de \\(j\\)-ésimo bloque. Podemos repensar este modelo de la siguiente manera: \\[\\begin{equation} (x_{ijk} - \\tau_i) = \\mu + \\beta_{j} + \\epsilon_{ijk} \\end{equation}\\] Bueno, ahora la parte izquierda de esta ecuación toma describe los datos una vez que los efectos del tratamiento, \\(\\tau_i\\) son removidos, ¿cómo estimamos este lado izquierdo con nuestros datos? Bastante sencillo. \\[\\begin{equation} x_{ijk} - \\hat{\\tau_i} = x_{ijk} - (\\overline{x}_i - \\overline{x}) \\end{equation}\\] Dentro de cada tratamiento, la media del tratamiento es extraída de cada valor. Si viésemos las desviaciones de la media de los tratamientos de nuestros datos, veríamos que existe todavía bastante variabilidad atribuible a los bloques. Para este caso, escribiremos la \\(SS(entre)\\) como \\(SS(tratmientos)\\) para distinguir nuestra suma de cuadrados de los tratamientos de la de los bloques. Para este caso vamos a partir nuestra \\(SS(dentro)\\) en dos partes: \\(SS(bloques)\\), que mide la variabilidad entre la media de los bloques y \\(SS(dentro)\\), que mide el resto de variabilidad no explicada de los datos. Así, tenemos la siguiente fórmula: \\[\\begin{equation} SS(total) = SS(tratamientos) + SS(bloques) + SS(dentro) \\end{equation}\\] ¿Cómo calculamos \\(SS(bloques)\\)? Con la siguiente fórmula: \\[\\begin{equation} SS(bloques) = {\\sum_{j=1}^J m_j (\\overline{x}_{bloque} - \\overline{x})^2} \\tag{6.20} \\end{equation}\\] Donde \\(\\overline{x}_{bloque}\\) es la media de las observaciones del bloque \\(j\\), \\(J\\) es el número total de bloques y \\(m_j\\) es el número de observaciones en el bloque \\(j\\). Los grados de libertad serían: \\[\\begin{equation} df(bloques) = J - 1 \\tag{6.21} \\end{equation}\\] Y de manera análoga al ANOVA de una vía, \\(SS(bloques)\\) es el numerador y \\(df(bloques)\\) es el denominador para el calculo de los cuadrados medios de los bloques, \\(MS(bloques)\\). \\[\\begin{equation} MS(bloques) = \\frac{\\sum_{j=1}^J m_j (\\overline{x}_{bloque} - \\overline{x})^2}{J - 1} \\tag{6.22} \\end{equation}\\] Para el calculo de \\(SS(dentro)\\) realizamos la siguiente operación, ya que el calculo de \\(SS(bloques)\\) reduce la variabilidad no explicada, \\(SS(dentro)\\). \\[\\begin{equation} SS(dentro) = SS(total) - SS(tratamientos) - SS(bloques) \\tag{6.23} \\end{equation}\\] De igual manera, para calcular los grados de libertad utilizamos: \\[\\begin{equation} df(dentro) = df(total) - df(tratamiento) - df(bloques) \\tag{6.24} \\end{equation}\\] Para el caso de nuestros datos, el valor de nuestra gran media poblacional, \\(\\overline{x}\\) es: \\[\\begin{equation} \\overline{x} = \\frac {93.5 + 102.9 + 67.0 + 86.3 + \\cdots + 40.1 + 46.4}{16} = \\frac{956.8}{16} = 59.8 \\end{equation}\\] Ahora calculamos la suma de cuadrados de los tratamientos \\(SS(tratamientos)\\) como en el ANOVA de una sola vía. \\[\\begin{equation} SS(tratamientos) = 4(63.3 - 59.8)^2 + 4(61.8 - 59.8)^2 + 4(53.0 - 59.8)^2 + 4(61.2 - 59.8)^2 = 259 \\end{equation}\\] Ya que el número de tratamientos es 4, \\(df(tratamientos)\\): \\[\\begin{equation} df(tratamientos) = 4-1 = 3 \\end{equation}\\] Entonces: \\[\\begin{equation} Ms(tratamientos) = \\frac{259}{3} = 86.333 \\end{equation}\\] Ahora calculamos \\(SS(bloques)\\): \\[\\begin{equation} SS(bloques) = 4(87.4 - 59.8)^2 + 4(59.0 - 59.8)^2 + 4(49.7 - 59.8)^2 + 4(43.2-59.8)^2 = 4573 \\end{equation}\\] Ya que tenemos cuatro bloques, \\(J = 4\\) y los \\(df(bloques)\\) son: \\[\\begin{equation} df(bloques) = 4-1 = 3 \\end{equation}\\] Ahora calculamos \\(MS(bloques)\\): \\[\\begin{equation} MS(bloques) = \\frac{4573}{3} = 1524.333 \\end{equation}\\] Para el calculo de la \\(SS(total)\\) realizamos la extracción de la gran media poblacional a cada valor: \\[\\begin{equation} SS(total) = (93.5-59.8)^2 + (102.9-59.8)^2 + (67.0-59.8)^2 + \\cdots + (46.4-59.8)^2 = 5411 \\end{equation}\\] Por lo tanto, \\(SS(dentro)\\) se obtiene restando las demás \\(SS\\) a \\(SS(total)\\). \\[\\begin{equation} SS(dentro) = 5411 - 259 - 4573 = 579 \\end{equation}\\] Y nuestros \\(df(dentro)\\) se calculan restando el resto de \\(df\\). \\[\\begin{equation} df(dentro) = 15 - 3 - 3 = 9 \\end{equation}\\] Por lo tanto, \\(MS(dentro)\\) se obtiene así: \\[\\begin{equation} MS(dentro) = \\frac{579}{9} = 64.333 \\end{equation}\\] Ahora sí, podemos calcular nuestro estadístico de \\(F\\) con la fórmula (6.18). \\[\\begin{equation} F_s = \\frac{86.333}{64.333} = 1.342 \\end{equation}\\] Vamos a ver qué resultado obtenemos con R, para realizar un ANOVA por bloques simplemente añadimos + Bloque después del tratamiento en el argumento de la función aov(). ¡Recuerden guardar el ANOVA en una variable! Bloque &lt;- factor(rep(1:4, each = 4)) Variedad &lt;- factor(rep(c(&quot;Variedad 1&quot;, &quot;Variedad 2&quot;, &quot;Variedad 3&quot;, &quot;Variedad 4&quot;), 4)) Peso &lt;- c(93.5, 102.9, 67.0, 86.3, 66.6, 53.2, 54.7, 61.3, 50.5, 47.4, 50, 50.7, 42.4, 43.8, 40.1, 46.4) Cebada &lt;- data.frame(Peso, Bloque, Variedad) anova.bloque &lt;- aov(Peso ~ Variedad + Bloque, Cebada) summary(anova.bloque) ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## Variedad 3 259 86.4 1.344 0.320294 ## Bloque 3 4573 1524.4 23.712 0.000131 *** ## Residuals 9 579 64.3 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Como podemos ver, el valor-p obtenido es de 0.320294, por lo que aceptamos la \\(H_0\\) y decimos que las cuatro variedades tienen el mismo rendimiento. ¡Observemos la cantidad de variabilidad explicada por \\(SS(bloques)\\)! library(broom) ANOVA.tabla.bloque &lt;- tidy(anova.bloque) Tabla 6.5: Tabla de ANOVA por bloques para el rendimiento de cebada Término df SS MS Estadístico de F Valor p Variedad 3 259.265 86.42167 1.344294 0.3202945 Bloque 3 4573.105 1524.36833 23.711635 0.0001314 Residuals 9 578.590 64.28778 6.3 ANOVA de dos vías Algunos análisis de varianza requieren más de un factor o variable. En el ejemplo anterior, nuestro factor era el tipo de variedad de cebada y los niveles eran 4: variedad 1, 2, 3 y 4. Ejemplo: Un fisiologo vegetal investiga al efecto del estrés mecánico en el crecimiento de plantas de soya. Las plantas fueron asignadas a uno de los cuatro tratamientos, con 13 plantulas por tratamiento. Las plantas de dos de los tratamientos fueron estresadas al agitarlas durante 20 minutos, dos veces al día, mientras que el grupo control no fue estresado. Aquí tenemos nuestro primer factor, con dos niveles: agitamiento y control. Además, las plantas crecieron en dos condiciones de luz: baja y moderada. Así que en este caso, nuestro segundo factor es la cantidad de luz, con dos niveles: baja y moderada. Nuestros tratamientos quedan de la siguiente manera: Tratamiento 1: Control, luz baja Tratamiento 2: Estrés, luz baja Tratamiento 3: Control, luz moderada Tratamiento 4: Estrés, luz moderada Después de 16 días las plantas fueron colectadas y se midió el área total de la hoja (cm2) de cada planta. Los resultados fueron los siguientes: Tabla 6.6: Área total de la hoja (cm2) Control, luz baja Estrés, luz baja Control, luz moderada Estrés, luz moderada 264 235 314 283 200 188 320 312 225 195 310 291 268 205 340 259 215 212 299 216 241 214 268 201 232 182 345 267 256 215 271 326 229 272 285 241 288 163 309 291 253 230 337 269 288 255 282 282 230 202 273 257 Figura 6.2: Gráfica de puntos de los datos de crecimiento de plantas, la barra horizontal indica la media del tratamiento. El modelo del ANOVA para este caso sería: \\[\\begin{equation} x_{ijk} = \\mu + \\tau_i + \\beta_j + \\epsilon_{ijk} \\tag{6.25} \\end{equation}\\] Donde \\(x_{ijk}\\) es la \\(k\\)-ésima observación en el nivel \\(i\\) del primer factor y el nivel \\(j\\) del segundo factor. El término \\(\\tau_i\\) representa el efecto del nivel \\(i\\) del primer factor (en este caso, la condición de estrés), y el término \\(\\beta_j\\) representa el efecto del nivel \\(j\\) del segundo factor (condición de luz). Si la influencia conjunta de dos factores es igual a la suma de sus influencias separadas, se dice que los dos factores son aditivos en cuanto a sus efectos. Visualizar esto es más sencillo con una tabla que incluya las medias de cada tratamiento. Luz baja Luz moderada Diferencia Control 245.3 304.1 58.8 Estrés 212.9 268.8 55.9 Diferencia -32.4 -35.3 Por ejemplo, el efecto del estrés reduce el área de la hoja por casi la misma cantidad, por lo que podríamos decir que el estrés tiene un efecto aditivo negativo sobre el área. Veamos esto en una gráfica. Figura 6.3: Efecto del estrés sobre la media del área de las hojas. Como podemos ver en la figura 6.3, el efecto de los tratamientos 2 y 4 (que corresponden a ambos tratamientos con estrés) parece reducir de manera casi idéntica el área de las hojas de nuestras plantas. Cuando los efectos de los factores son aditivos, decimos que no hay interacción entre los factores. Algunas veces, el efecto que un factor tiene sobre una variable depende del nivel del segundo factor. Cuando esto ocurre, decimos que los dos factores interactúan para generar el efecto de la variable de respuesta. Ejemplo: Investigadores condujeron un estudio sobre el enriquecimiento de bebidas frutales a base de leche con cantidades bajas y altas de hierro (Fe) y zinc (Zn). Las bebidas fueron digeridas en un tracto gastrointestintal simulado y la retención celular de hierro fue medida (\\(\\mu\\)g Fe/mg de proteína celular). La figura 6.4 muestra la interacción del promedio obtenido de los cuatro tratamientos. Cuando el nivel de Zn fue bajo, el efecto en la retención de Fe es mucho menor a cuando el nivel de Zn fue alto. Zn bajo Zn alto Diferencia Fe bajo 0.707 0.215 -0.492 Fe alto 0.994 1.412 0.418 Diferencia 0.287 1.197 En este ejemplo, las líneas de interacción no son paralelas, lo que quiere decir que existe una interacción entre el factor 1 (nivel de Fe) y el factor 2 (nivel de Zn). Quiere decir que el nivel de retención de Fe depende de la cantidad de Zn utilizada. Figura 6.4: Interacción entre el nivel de suplemento de Zn y Fe. Para estos casos podemos ajustar nuestro modelo del ANOVA para tomar en cuenta la interacción entre estos dos factores. \\[\\begin{equation} x_{ijk} = \\mu + \\tau_i + \\beta_j + \\gamma_{ij} + \\epsilon_{ijk} \\tag{6.26} \\end{equation}\\] Donde el nuevo término \\(\\gamma_{ij}\\) es el efecto de la interacción entre el nivel \\(i\\) del primer factor y el nivel \\(j\\) del segundo factor. De igual manera, si hay \\(n\\) observaciones, entonces \\(df(total) = n - 1\\). Si hay \\(I\\) niveles para el primer factor, entonces \\(I - 1\\) nos da los grados de libertad y si hay \\(J\\) niveles para el segundo factor, de nuevo, \\(J - 1\\) nos da sus grados de libertad. Existen entonces \\((I - 1) \\times (J-1)\\) grados de libertad para la interacción. Con \\(I\\) niveles en el primer factor y \\(J\\) niveles en el segundo, hay \\(IJ\\) combinaciones para los tratamientos, por lo tanto \\(df(dentro) = n - IJ\\). Una hipótesis nula de interés podría ser que las interacciones suman 0: \\[\\begin{equation} H_0: \\gamma_{11} = \\gamma_{12} = \\cdots = \\gamma_{IJ} = 0 \\end{equation}\\] Para probar esta hipótesis calculamos el estadístico \\(F_s\\): \\[\\begin{equation} F_s = \\frac{MS(interacción)}{MS(dentro)} \\end{equation}\\] Y rechazamos la \\(H_0\\) si nuestro valor-p es pequeño. Por ejemplo, para los datos de nuestras plantas, vamos a crear una matriz con los datos y realizar el ANOVA de dos vías. Luz &lt;- factor(c(rep(&quot;Luz baja&quot;, 26), rep(&quot;Luz alta&quot;, 26))) Condicion &lt;- factor(c(rep(&quot;Control&quot;, 13), rep(&quot;Estrés&quot;, 13), rep(&quot;Control&quot;, 13), rep(&quot;Estrés&quot;, 13))) Area &lt;- c(264, 200, 225, 268, 215, 241, 232, 256, 229, 288, 253, 288, 230, 235, 188, 195, 205, 212, 214, 182, 215, 272, 163, 230, 255, 202, 314, 320, 310, 340, 299, 268, 345, 271, 285, 309, 337, 282, 273, 283, 312, 291, 259, 216, 201, 267, 326, 241, 291, 269, 282, 257) Soya &lt;- data.frame(Luz, Condicion, Area) plantas.anova &lt;- aov(Area ~ Condicion + Luz, Soya) summary(plantas.anova) ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## Condicion 1 14858 14858 16.93 0.000148 *** ## Luz 1 42752 42752 48.71 7.14e-09 *** ## Residuals 49 43003 878 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Este ANOVA que acabamos de realizar no toma en cuenta el efecto de la interacción, ¿cómo podemos incluirlo? Bueno para esto tenemos que agregar un argumento especial en nuestro anova, que sería Condicion:Luz, le cual toma en cuenta la interacción entre la condición de estrés y la cantidad de luz. plantas.anova.int &lt;- aov(Area ~ Condicion + Luz + Condicion:Luz, Soya) summary(plantas.anova.int) ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## Condicion 1 14858 14858 16.595 0.000173 *** ## Luz 1 42752 42752 47.749 1.01e-08 *** ## Condicion:Luz 1 26 26 0.029 0.864570 ## Residuals 48 42976 895 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Como podemos ver (y como ya sabíamos por la gráfica que realizamos), parece ser que la interacción entre el estrés y la condición de luz no es significativa, ya que obtenemos un valor-p muchísimo mayor a 0.05 (0.8646). Al parecer encontramos también diferencias significativas en nuestros dos factores (condición de estrés y cantidad de luz), de nuevo, algo que también ya habíamos observado en la figura 6.3. Algo importante a considerar es que cuando existen interacciones entre los factores, como en la figura 6.4, la interpretación de los efectos de los factores no es la misma. Es difícil determinar el efecto independiente de un factor, por ejemplo, el efecto del Fe, ya que la naturaleza y la magnitud del efecto depende de un nivel particular de Zn. Debido a esto, primero buscamos la presencia de una interacción (a través de una gráfica o un ANOVA). Si encontramos una interacción entre los factores, usualmente detenemos el análisis en este punto. Si no hay una interacción (como en el caso de las plantas), procedemos a evaluar los efectos de los factores individuales. Ya que en el caso de las plantas era muy evidente que no existe una interacción entre ambos factores (corroborado por el estadístico \\(F\\) para la interacción y la gráfica), pudimos concluir el ANOVA de dos vías y analizar el efecto de cada uno de los factores en donde, corroboramos que ambos parecen afectar el crecimiento de las plantas. 6.4 Combinaciones lineales de las medias Algunas preguntas interesantes pueden ser abordadas considerando la combinación lineal de las medias de los grupos. Definida con el término \\(L\\), es una cantidad que se obtiene de la siguiente manera: \\[\\begin{equation} L = m_1x_1 + m_2x_2 + \\cdots + m_Ix_I \\tag{6.27} \\end{equation}\\] Donde \\(m\\) son multiplicadores para \\(x_i\\). Uno de los usos de la combinación lineal es ajustar debido a una variable extraña. Ejemplo: Una medida para evaluar la funcionalidad de los pulmones es la capacidad vital forzada (FVC), que mide la cantidad máxima de aire que una persona puede expirar en un solo aliento. En un estudio de salud pública, investigadores midieron la FVC de una gran muestra de personas. Los resultados de adultos ex-fumadores se muestran en la siguiente tabla. Tabla 6.7: FVC en ex-fumadores hombres Edad (años) n Media Desviación estándar 25-34 83 5.29 0.76 35-44 102 5.05 0.77 45-54 126 4.51 0.74 55-64 97 4.24 0.80 65-74 73 3.58 0.82 En este caso, si calculásemos la gran media del conjunto de datos nos encontraríamos con un problema; no se puede comparar con otras poblaciones ya que podrían tener otra distribución de las edades. Si quisiéramos comparar nuestros datos con la FVC de no-fumadores. La diferencia observada en la FVC se vería distorsionada ya que, el grupo de ex-fumadores es (sin sorpresa alguna) más viejo que el grupo de no-fumadores. Podríamos optar por utilizar una media ajustada a la edad, que es un estimado de la media de la FVC en una población de referencia, con una distribución de edad específica. En este caso utilizaremos los valores de la siguiente tabla. Tabla 6.8: Distribución de edad en una población de referencia Edad Frecuencia relativa 25-34 0.22 35-44 0.22 45-54 0.24 55-64 0.20 65-74 0.12 La media ajustada a la edad de la FVC se obtendría de la siguiente manera: \\[\\begin{equation} L = 0.22\\overline{x}_1 + 0.22\\overline{x}_2 + 0.24\\overline{x}_3 + 0.20\\overline{x}_4 \\overline{x}_5 \\end{equation}\\] Como podemos ver, el valor de \\(m\\) corresponde a la frecuencia relativa de la distribución de edades de nuestra población de referencia. \\[\\begin{equation} L = (0.22)(5.29) + (0.22)(5.05) + (0.24)(4.51) + (0.20)(4.24) + (0.12)(3.58) = 4.63 \\space litros \\end{equation}\\] Este valor corresponde a una media de la FVC en una población idealizada donde hay personas que son biológicamente similares a hombres ex-fumadores, pero cuya distribución de edad es la de la población de referencia. 6.4.1 Contrastes Una combinación lineal cuyos multiplicadores \\(m\\) suman 0 se conoce como contraste. Ejemplo: La siguiente tabla muestra la media de los tratamientos y el tamaño de muestra de un experimento en plantas de soya que vimos en secciones pasadas. Podemos utilizar contrastes para describir los efectos del estrés en las dos cantidades de luz. Tabla 6.9: Datos de crecimiento de plantas de soya Tratamiento Media del área de la hoja n Control, poca luz 245.3 13 Estrés, poca luz 212.9 13 Control, luz moderada 304.1 13 Estrés, luz moderada 268.8 13 Lo primero que debemos observar es que en este caso una diferencia entre pares es lo mismo que un contraste. Por lo tanto, para medir el efecto del estrés en bajas condiciones de luz podemos considerar el contraste: \\[\\begin{equation} L = \\overline{x}_1 - \\overline{x}_2 = 245.3 - 212.9 = 32.4 \\end{equation}\\] Para este contraste, los multplicadores serán \\(m_1 = 1\\), \\(m_2 = -1\\), \\(m_3 = 0\\) y \\(m_4 = 0\\). Como podemos ver, dan como resultado 0. Para medir el efecto del estrés en condiciones del luz moderada consideramos el contraste: \\[\\begin{equation} L = \\overline{x}_3 - \\overline{x}_4 = 304.1 - 268.8 = 35.3 \\end{equation}\\] Para este contraste, los multiplicadores serán \\(m_1 = 0\\), \\(m_2 = 0\\), \\(m_3 = 1\\) y \\(m_4 = -1\\). También dan como resultado 0. Para medir el efecto general del estrés, podemos promediar lo contrastes en partes y obtener: \\[\\begin{equation} L = \\frac{1}{2}(\\overline{x}_1 - \\overline{x}_2) + \\frac{1}{2}(\\overline{x}_3 - \\overline{x}_4) = 33.85 \\end{equation}\\] Para este contraste, los multiplicadores serán \\(m_1 = \\frac{1}{2}\\), \\(m_2 = -\\frac{1}{2}\\), \\(m_3 = \\frac{1}{2}\\) y \\(m_4 = -\\frac{1}{2}\\). Cada combinación lineal es un estimado, basado en \\(\\overline{x}\\), de la combinación lineal correspondiente a las medias de las poblaciones (\\(\\mu\\)). Debemos entonces considerar el error estándar de la combinación lineal, calculado de la siguiente manera: \\[\\begin{equation} SE_L = s_{p} \\sqrt{\\sum_{i=1}^I\\frac{m_i^2}{n_i}} \\tag{6.28} \\end{equation}\\] Donde \\(s_p = \\sqrt{MS(dentro)}\\) de nuestro ANOVA. También podemos escribir nuestra fórmula de la siguiente manera: \\[\\begin{equation} SE_L = s_{p} \\sqrt{\\frac{m_1^2}{n_1} + \\frac{m_2^2}{n_2} + \\cdots + \\frac{m_I^2}{n_I}} \\end{equation}\\] En caso de que \\(n_i\\) sea igual en todas las muestras, entonces: \\[\\begin{equation} SE_L = s_{p} \\sqrt{\\frac{m_1^2 + m_2^2 + \\cdots m_I^2}{n}} = s_{p} \\sqrt{\\frac{1}{n}\\sum_{i=1}^Im_i^2} \\end{equation}\\] Para la combinación lineal del efecto general de los datos de las plantas de soya: \\[\\begin{equation} \\sum_{i=1}^I m_i^2 = (\\frac{1}{2})^2 + (-\\frac{1}{2})^2 + (\\frac{1}{2})^2 + (-\\frac{1}{2})^2 = 1 \\end{equation}\\] Sabemos por la sección anterior que \\(MS(entre) = 895\\) por lo que \\(s_p = \\sqrt{895} = 29.92\\). Por lo tanto, el \\(SE_L\\): \\[\\begin{equation} SE_L = s_p \\sqrt{\\frac{1}{13}} = 29.92 \\times 0.277 = 8.299 \\end{equation}\\] 6.4.2 Intervalos de confianza Las combinaciones lineales también se pueden utilizar en pruebas de hipótesis, con los valores críticos obtenidos a partir de la distribución \\(t\\) de Student en donde \\(df = df(dentro)\\). Los intervalos de confianza se construyen con el error estándar de la combinación lineal. \\[\\begin{equation} L \\pm t_{0.025} SE_L \\tag{6.29} \\end{equation}\\] Ya que sabemos que \\(L = 33.85\\), y que \\(SE_L = 8.299\\), lo único que necesitamos son los \\(df(dentro)\\), que están dados en la tabla de ANOVA que hicimos en el ejercicio de las plantas de soya. Entonces \\(df(dentro) = 48\\). Encontramos el valor crítico con la función #Dividimos el valor de alfa entre dos, ya que nos interesan ambas colas de la distribución qt(0.025, df = 48, lower.tail = F) ## [1] 2.010635 Y tenemos nuestro intervalo: \\[\\begin{equation} 33.85 \\pm (2.011)(8.299) \\\\ 33.85 \\pm 16.7 \\end{equation}\\] Estamos 95% seguros de que el efecto del estrés, promediado sobre las condiciones de luz, es reducir el área de la hoja de las plantas por una cantidad que está entre 17.15 cm2 y 50.55 cm2. 6.4.3 Prueba de \\(t\\) Para una prueba de \\(t\\) se utiliza la siguiente fórmula: \\[\\begin{equation} t_s = \\frac{L}{SE_L} \\tag{6.30} \\end{equation}\\] Y la prueba de \\(t\\) se realiza de manera usual. A veces un investigador quiere estudiar los efectos separados y en conjunto de dos o más factores sobre una variable de respuesta. Por ejemplo, a parte del ANOVA de dos vías, podemos analizar las interacciones entre dos factores gracias a los contrastes lineares. Para el caso de las plantas de soya, nuestros dos factores de interés son la condición de estrés y la cantidad luz. Luz baja Luz moderada Diferencia Control 245.3 304.1 58.8 Estrés 212.9 268.8 55.9 Diferencia -32.4 -35.3 En cada nivel del luz, el efecto medio del estrés se puede medir como un contraste: \\[\\begin{equation} Efecto \\space del \\space estrés \\space en \\space luz \\space baja: \\overline{x}_2 - \\overline{x}_1 = 212.9 - 245.3 = -32.4 \\\\ Efecto \\space del \\space estrés \\space en \\space luz \\space moderada: \\overline{x}_4 - \\overline{x}_3 = 268.8 - 304.1 = -35.3 \\\\ \\end{equation}\\] Ahora consideremos la pregunta: ¿Es la reducción en el área de las hojas debida al estrés la misma en las dos condiciones de luz? Una forma de responder esto es comparar \\((\\overline{x}_2 - \\overline{x_1})\\) contra \\((\\overline{x}_4 - \\overline{x}_3)\\). \\[\\begin{equation} L = (\\overline{x}_2 - \\overline{x_1}) - (\\overline{x}_4 - \\overline{x}_3) = -32.4 - (-35.3) = 2.9 \\end{equation}\\] Este contraste se puede utilizar para realizar un intervalo de confianza o una prueba de hipótesis. Por ejemplo, para la prueba de hipótesis partimos de la idea \\(H_0: (\\mu_2 - \\mu_1) = (\\mu_4 - \\mu_3)\\), o lo que es lo mismo \\(H_0\\): El efecto del estrés es el mismo bajo ambas condiciones de luz. Para el caso de esta contraste linear, \\(L\\): \\[\\begin{equation} SE_L = s_p \\sqrt{\\sum_{i=1}^I \\frac{m_i^2}{n_i}} = s_p \\sqrt{\\frac{4}{13}} = 29.922 \\sqrt{\\frac{4}{13}} = 16.6 \\end{equation}\\] Por lo que nuestro estadístico es: \\[\\begin{equation} t_s = \\frac{2.9}{16.6} = 0.2 \\end{equation}\\] Simplemente encontramos nuestro valor \\(t_{48,0.05}\\). qt(0.05, 48, lower.tail = F) ## [1] 1.677224 Como vemos nuestro \\(valor-p = 3.34\\) muestra que no hay diferencia significativas (lo que coincide con el ANOVA de dos vías que hicimos anteriormente), por lo que el efecto del estrés es el mismo bajo ambas condiciones de luz. 6.5 Pruebas post hoc Usualmente estamos interesados en un análisis detallado de las medias muestrales \\(\\overline{X}_1, \\overline{X}_2, \\dots, \\overline{X}_I\\), considerando todas las comparaciones entre pares posibles. Cuando consideramos múltiples comparaciones, podemos hablar de la oportunidad de un error tipo I para una comparación particular, por ejemplo, \\(H_0: \\mu_1 = \\mu_2\\). Este tipo de error I se conoce como ratio de error tipo I en sentido de la comparación, \\(\\alpha_{cw}\\) o podemos hablar de la oportunidad de un error tipo I entre cualquiera de las comparaciones, llamado ratio de error tipo I en sentido del experimento, \\(\\alpha_{ew}\\). Algo que siempre se cumple es que \\(\\alpha_{ew} \\le k \\times \\alpha_{cw}\\), donde \\(k\\) es el número de comparaciones. Por ejemplo, si se hicieran seis comparaciones con un \\(\\alpha = 0.05\\), el ratio de error tipo I en sentido del experimento sería \\(\\alpha_{ew} = 6 \\times 0.05 = 0.30\\). Debido a esto, realizar múltiples pruebas de \\(t\\) es contraproducente. 6.5.1 Diferencia menos significativa de Fisher (LSD) Se aprovecha del método de comparaciones lineales para producir interválos de confianza entre todos los pares de diferencias de las medias de la población, utilizando \\(\\alpha_{cw} = \\alpha\\) la razón de error tipo I utilizada en el ANOVA. Para comparar dos de nuestras observaciones, definimos \\(D_{ab} = \\overline{X}_a - \\overline{X}_b\\) de tal manera que tenemos un contraste lineal de la siguiente manera: \\[\\begin{equation} d_{ab} = 1\\overline{x}_a + (-1)\\overline{x}_b + 0 \\overline{x}_c + \\cdots 0 \\overline{x}_I \\end{equation}\\] Donde los únicos multplicadores distintos a 0 serán los del contraste que queremos realizar. Posteriormente se calcula el error estándar para esta diferencia, \\(SE_{D_{ab}} = s_p \\sqrt{\\sum_{i=1}^I}{\\frac{m_i^2}{n_i}}\\). Después calculamos un intervalo de confianza al 95% para la diferencia de la media poblacional de nuestro contraste. \\[\\begin{equation} d_{ab} \\pm t_{df,\\alpha/2} \\times SE_{D_{ab}} \\end{equation}\\] Si nuestro intervalo de confianza no contiene 0, quiere decir que hay diferencias significativas en esta comparación. Por ejemplo, para el ANOVA de las plantas de soya que realizamos, podemos identificar los grupos con diferencias significativas. Para esto vamos a necestiar la librería agricolae. library(agricolae) ## Registered S3 methods overwritten by &#39;klaR&#39;: ## method from ## predict.rda vegan ## print.rda vegan ## plot.rda vegan #Asignamos nuestro resultado a una variable. lsd1 &lt;- LSD.test(plantas.anova, &quot;Condicion&quot;) lsd1 ## $statistics ## MSerror Df Mean CV t.value LSD ## 877.6048 49 257.7885 11.49175 2.009575 16.51133 ## ## $parameters ## test p.ajusted name.t ntr alpha ## Fisher-LSD none Condicion 2 0.05 ## ## $means ## Area std r LCL UCL Min Max Q25 Q50 Q75 ## Control 274.6923 39.95574 26 263.0170 286.3676 200 345 244.00 272 306.50 ## Estrés 240.8846 42.82180 26 229.2093 252.5599 163 326 206.75 238 271.25 ## ## $comparison ## NULL ## ## $groups ## Area groups ## Control 274.6923 a ## Estrés 240.8846 b ## ## attr(,&quot;class&quot;) ## [1] &quot;group&quot; plot(lsd1) lsd2 &lt;- LSD.test(plantas.anova, &quot;Luz&quot;) lsd2 ## $statistics ## MSerror Df Mean CV t.value LSD ## 877.6048 49 257.7885 11.49175 2.009575 16.51133 ## ## $parameters ## test p.ajusted name.t ntr alpha ## Fisher-LSD none Luz 2 0.05 ## ## $means ## Area std r LCL UCL Min Max Q25 Q50 Q75 ## Luz alta 286.4615 35.5879 26 274.7863 298.1368 201 345 268.25 284.0 311.5 ## Luz baja 229.1154 32.3720 26 217.4401 240.7907 163 288 206.75 229.5 254.5 ## ## $comparison ## NULL ## ## $groups ## Area groups ## Luz alta 286.4615 a ## Luz baja 229.1154 b ## ## attr(,&quot;class&quot;) ## [1] &quot;group&quot; plot(lsd2) Con el comando $groups podemos ver cuáles son los grupos donde hay diferencia. En el caso de la soya, hay diferencias significativas tanto para la cantidad de luz como para el nivel de estrés. Vamos a utilizar los datos de PlantGrowth para realizar un ANOVA y una prueba LSD. data(PlantGrowth) ANOVA.PG &lt;- aov(weight ~ group, PlantGrowth) summary(ANOVA.PG) ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## group 2 3.766 1.8832 4.846 0.0159 * ## Residuals 27 10.492 0.3886 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Como vemos hay diferencias significativas, así que ahora vamos a realizar la prueba LSD. lsd3 &lt;- LSD.test(ANOVA.PG, &quot;group&quot;) lsd3 ## $statistics ## MSerror Df Mean CV t.value LSD ## 0.3885959 27 5.073 12.28809 2.051831 0.5720126 ## ## $parameters ## test p.ajusted name.t ntr alpha ## Fisher-LSD none group 3 0.05 ## ## $means ## weight std r LCL UCL Min Max Q25 Q50 Q75 ## ctrl 5.032 0.5830914 10 4.627526 5.436474 4.17 6.11 4.5500 5.155 5.2925 ## trt1 4.661 0.7936757 10 4.256526 5.065474 3.59 6.03 4.2075 4.550 4.8700 ## trt2 5.526 0.4425733 10 5.121526 5.930474 4.92 6.31 5.2675 5.435 5.7350 ## ## $comparison ## NULL ## ## $groups ## weight groups ## trt2 5.526 a ## ctrl 5.032 ab ## trt1 4.661 b ## ## attr(,&quot;class&quot;) ## [1] &quot;group&quot; plot(lsd3) En este caso encontramos que el tratamiento 2 y el tratamiento 1 tienen diferencias significativas entre sí, pero no se diferencían del grupo control. 6.5.2 Diferencia significativa honesta de Tukey (HSD) Es un método muy similar al LSD, pero en lugar de utilizar \\(t\\) en la fórmula del intervalo de confianza, valores de una distribución conocida como distribución del rango Estudentizada. Su cálculo en R es bastante sencillo. Veamos, de nuevo, el ejemplo con nuestros datos de las planta de soya. hsd1 &lt;- TukeyHSD(plantas.anova) hsd1 ## Tukey multiple comparisons of means ## 95% family-wise confidence level ## ## Fit: aov(formula = Area ~ Condicion + Luz, data = Soya) ## ## $Condicion ## diff lwr upr p adj ## Estrés-Control -33.80769 -50.31902 -17.29636 0.0001479 ## ## $Luz ## diff lwr upr p adj ## Luz baja-Luz alta -57.34615 -73.85748 -40.83482 0 Para los datos de las plantas, veamos cómo obtenemos nuestros resultados. hsd2 &lt;- TukeyHSD(ANOVA.PG) hsd2 ## Tukey multiple comparisons of means ## 95% family-wise confidence level ## ## Fit: aov(formula = weight ~ group, data = PlantGrowth) ## ## $group ## diff lwr upr p adj ## trt1-ctrl -0.371 -1.0622161 0.3202161 0.3908711 ## trt2-ctrl 0.494 -0.1972161 1.1852161 0.1979960 ## trt2-trt1 0.865 0.1737839 1.5562161 0.0120064 plot(hsd2) De nuevo, vemos que el valor-p para la comparación entre el tratamiento 1 y 2 es significativa (\\(valor-p &lt; 0.05\\)). Pero las diferencias entre el control con ambos tratamientos no lo son. "],["correlación-y-regresión-lineal.html", "Lección 7 Correlación y regresión lineal 7.1 El coeficiente de correlación \\(r\\) 7.2 Regresión lineal 7.3 El model lineal 7.4 Inferencia estadística respecto de \\(\\beta_1\\) 7.5 Prueba de hipótesis para \\(H_0: \\beta_1 = 0\\) 7.6 Intervalos de confianza y de predicción", " Lección 7 Correlación y regresión lineal La correlación lineal y regresión lineal son técnicas basadas en ajustar una línea recta a nuestros datos. Estos análisis usualmente consisten de un par de observaciones (\\(X, Y\\)). Ejemplo: La anfetamina es una droga que suprime el apetito. En un estudio, investigadores alocaron aleatoriamente a 24 ratas a tres distintos grupos de tratamientos para recibir una inyección de anfetamina a distintas dosis y una inyección con solución salina. Se midió la cantidad de comida consumida por cada animal 3 horas después de la inyección. Los resultados (g de comida consumidos por kg de peso) se muestran en la siguiente tabla. Tabla 7.1: Consumo de comida (Y) de las ratas (g/kg). X = Dosis de anfetamina (mg/kg) X = 0 X = 2.5 X = 5 112.6 73.3 38.5 102.1 84.8 81.3 90.2 67.3 57.1 81.5 55.3 62.3 105.6 80.7 51.5 93.0 90.0 48.3 106.6 75.5 42.7 108.3 77.1 57.9 A continuación tenemos una gráfica de dispersión con los valores de la tabla 7.1. Este es un ejemplo de dos variables que se encuentran en pares (Consumo de comida y dosis de anfetamina). 7.1 El coeficiente de correlación \\(r\\) Supongamos que tenemos \\(n\\) número de par de observaciones, con cada par representado por dos variables, \\(X\\) y \\(Y\\). Si realizamos un gráfico de dispersión y vemos una tendencia lineal general, es natural intentar describir la fuerza de la asociación. Para realizar esto utilizamos el coeficiente de correlación. Ejemplo: En un estudio de una población silvestre de la serpiente Vipera bertis, investigadores capturaron y midieron nueve hembras adultas. Se midió su peso así como su longitud. Estos valores se muestran en la tabla 7.2 y en el gráfico de dispersión de la figura 7.1. Tabla 7.2: Longitud (X, en cm) y peso (Y, en g) de serpientes hembra Longitud (cm) Peso (g) 60.0 136.0 69.0 198.0 66.0 194.0 64.0 140.0 54.0 93.0 67.0 172.0 59.0 116.0 65.0 174.0 63.0 145.0 Media 63.0 152.0 SD 4.6 35.3 Figura 7.1: Regresión lineal del peso y longitud de 9 serpientes hembra. Como podemos ver, parece ser que existe una asociación positiva entre la longitud y el peso de las serpientes. Por lo tanto, si una serpiente es más larga que el promedio \\(\\overline{x} = 63\\) también tenderá a ser más pesada que el promedio \\(\\overline{y} = 152\\). La línea que tenemos en el gráfico se conoce como línea de cuadrados mínimos o regresión lineal ajustada de \\(Y\\) en \\(X\\). 7.1.1 Midiendo la fuerza de la asociación Para medir la fuerza de nuestra asociación utilizamos el coeficiente de correlación, \\(r\\). Carece de escala por lo que no es afectado por cambios en las unidades de medición. Para entender su funcionamiento, consideremos los datos de las serpientes transformados en valores \\(Z\\). Figura 7.2: Gráfico de dispersión de la longitud estandarizada contra el peso estandarizado. Como podemos ver en la gráfica, la mayoría de nuestros puntos caen en el cuadrante superior derecho y el inferior izquierdo. Los puntos que caigan en estos cuadrantes tendrán valores \\(Z\\) cuyos productos (\\(z_x z_y\\)) son positivos. De igual manera, cualquier punto que caiga en el cuadrante superior izquierdo e inferior derecho tendrá valores \\(Z\\) cuyos productos serán negativos. En nuestro caso, existe una asociación positiva entre la longitud y el peso de las serpientes, ya que la mayoría de nuestros puntos caen en cuadrantes con productos positivos, por ende, la suma de los productos estandarizados dará un valor positivo. Si no existiera una relación lineal en nuestros datos, los puntos estarían distribuidos de manera aleatoria en los cuadrantes y la suma de los productos sería 0. De hecho, el coeficiente de correlación está basado en esta suma. Se computa como el promedio de los productos de los valores estandarizados y se obtiene de la siguiente manera: \\[\\begin{equation} r = \\frac{1}{n - 1}\\sum_{i=1}^n (\\frac{x_i - \\overline{x}}{s_x})(\\frac{y_i - \\overline{y}}{s_y}) \\tag{7.1} \\end{equation}\\] Afortunadamente en R podemos obtener este coeficiente sin necesidad de hacer todas las transformaciones y operaciones correspondientes, simplemente utilizamos la función cor() en nuestros datos. Longitud &lt;- c(60, 69, 66, 64, 54, 67, 59, 65, 63) Peso &lt;- c(136, 198, 194, 140, 93, 172, 116, 174, 145) Serpientes &lt;- data.frame(Longitud, Peso) Serpientes ## Longitud Peso ## 1 60 136 ## 2 69 198 ## 3 66 194 ## 4 64 140 ## 5 54 93 ## 6 67 172 ## 7 59 116 ## 8 65 174 ## 9 63 145 cor(Serpientes$Longitud, Serpientes$Peso) ## [1] 0.9436756 Es importante que tengamos los datos arreglados en dos columnas, una con la variable \\(x\\) (por ejemplo, longitud) y la otra con la variable \\(y\\) (por ejemplo, peso). Como podemos ver, nuestro coeficiente \\(r = 0.9437\\) lo que indica una fuerte correlación lineal. Normalmente, el valor de \\(r\\) se sitúa entre -1 y 1. Valores cercanos a 1 indican una correlación positiva, como en el caso de las serpientes, mientras que valores cercanos a -1 indican una correlación negativa. Valores cercanos a 0 indican ausencia de correlación lineal (los datos pueden estar correlacionados de otras maneras). En caso de nuestra muestra, nos referimos a este coeficiente como la correlación muestral, ya que las medidas tomadas de estas 9 serpientes componen una muestra de una población más grande. Es decir nosotros estamos estimando la correlación poblacional, normalmente denotada con el símbolo \\(\\rho\\) (léase rho). Debe considerarse que, para que nuestro coeficiente \\(r\\) pueda ser utilizado como una estimación de \\(\\rho\\), tanto los valores de \\(X\\) como \\(Y\\) deben de ser seleccionados de manera aleatoria siguiendo el modelo de muestreo aleatorio bivariado, donde cada par de observaciones (\\(x_i, y_i\\)) se considera una muestra aleatoria tomada a partir de una población (\\(x, y\\)) de pares. En este modelo, los valores observados de \\(X\\) y \\(Y\\) se toman como muestras aleatorias, así que los valores \\(\\overline{x}, \\overline{y}, s_x\\) y \\(s_y\\) son estimados de los valores poblacionales correspondientes, \\(\\mu_x, \\mu_y, \\sigma_x\\) y \\(\\sigma_y\\). Aunque este modelo es razonable para muchos investigadores, quienes no lo consideran así, especialmente cuando cuando los valores de \\(X\\) son especificados por el investigador, como en el caso de las ratas y las anfetaminas. Este modelo se conoce como modelo de submuestreo aleatorio. 7.1.2 Inferencia acerca de la correlación Ya hemos visto como el coeficiente de correlación nos sirve para describir un conjunto de datos. Ahora consideraremos la estadística inferencial basada en \\(r\\) para los datos de este modelo. Consideremos que cualquier patrón o tendencia aparente en nuestros datos es meramente ilusoria y que solamente refleja la variabilidad de nuestros datos. Para estos casos, \\(H_0: X \\space y \\space Y no \\space están \\space correlacionadas\\ space en \\space la \\space población\\) o de manera alterna, \\(H_0: no \\space existe \\space correlación \\space linear \\space entre \\space X \\space y \\space Y\\). ¿Cómo podemos probar esto? Podemos realizar una prueba de \\(t\\). En este caso la fórmula para calcular nuestro estadístico es la siguiente: \\[\\begin{equation} t_s = r\\sqrt{\\frac{n-2}{1-r^2}} \\tag{7.2} \\end{equation}\\] Para este caso los grados de libertad \\(df\\) se obtienen así: \\[\\begin{equation} df = n - 2 \\tag{7.3} \\end{equation}\\] ¿Por qué \\(n-2\\)? Bueno, dos puntos cualesquiera determinan una linea recta, sin embargo, con esa cantidad de datos, \\(n=2\\), no obtenemos información acerca de la variabilidad inherente de los datos. Es hasta que observamos un tercer punto que podemos comenzar a estimar la fuerza de la correlación. Para nuestros datos de las serpientes: \\[\\begin{equation} t_s = 0.9437 \\sqrt{\\frac{9 - 2}{1 - 0.9437^2}} = 7.549 \\end{equation}\\] Ahora tendríamos que checar el valor crítico en nuestro valor de tablas o con la función de \\(t\\). Sin embargo, en R basta con utilizar la función cor.test() con los argumentos de nuestra correlación y obtenemos nuestro resultado, junto con nuestro valor-p. cor.test(Serpientes$Longitud, Serpientes$Peso) ## ## Pearson&#39;s product-moment correlation ## ## data: Serpientes$Longitud and Serpientes$Peso ## t = 7.5459, df = 7, p-value = 0.0001321 ## alternative hypothesis: true correlation is not equal to 0 ## 95 percent confidence interval: ## 0.7489030 0.9883703 ## sample estimates: ## cor ## 0.9436756 Como podemos ver, hay una clara evidencia a favor de la correlación que estamos viendo pues nuestro \\(valor-p &lt; 0.05\\) por lo que rechazaríamos la \\(H_0\\) y diríamos que sí existe correlación lineal entre nuestras dos variables \\(X\\) y \\(Y\\). 7.1.3 Intervalo de confianza para \\(\\rho\\) Como vimos la función cor.test() nos construye el intervalo de confianza al 95% de nuestro valor para \\(r\\), es decir, podemos estar 95% seguros de que \\(\\rho\\) se encuentra entre \\((0.749, 0.988)\\). Sin embargo, veamos cómo se calcula esto de manera tradicional. Si nuestro tamaño de muestra es suficientemente grande, podemos realizar la construcción del intervalo. Ya que la distribución muestral del coeficiente de correlación muestral, \\(r\\), se encuentra sesgada, aplicamos una transformación conocida como la transformación de Fisher: \\[\\begin{equation} z_r = \\frac{1}{2}\\ln \\left[{\\frac{1+r}{1-r}}\\right] \\end{equation}\\] Donde \\(\\ln\\) es un logaritmo natural base \\(e\\). Podemos construir el intervalo de confianza al 95% para este valor transformado \\(\\frac{1}{2}\\ln \\left[{\\frac{1+\\rho}{1-\\rho}}\\right]\\) de la siguiente manera: \\[\\begin{equation} z_r \\pm 1.96 \\frac{1}{\\sqrt{n-3}} \\end{equation}\\] Debido a la transformación que hicimos, tenemos que convertir los limites de nuestro intervalo de confianza para \\(\\frac{1}{2}\\ln \\left[{\\frac{1+r}{1-r}}\\right]\\) a un intervalo de confianza para \\(\\rho\\). Para hacer esto, resolvemos para \\(\\rho\\): \\[\\begin{equation} \\frac{1}{2}\\ln \\left[{\\frac{1+\\rho}{1-\\rho}}\\right] = z_r \\pm 1.96 \\frac{1}{\\sqrt{n-3}} \\end{equation}\\] En donde \\(\\rho\\): \\[\\begin{equation} \\rho = \\frac{e^{2(z_r - 1.96 \\frac{1}{\\sqrt{n-3}})}-1}{e^{2(z_r - 1.96 \\frac{1}{\\sqrt{n-3}})}+1} \\\\ \\rho = \\frac{e^{2(z_r + 1.96 \\frac{1}{\\sqrt{n-3}})}-1}{e^{2(z_r + 1.96 \\frac{1}{\\sqrt{n-3}})}+1} \\end{equation}\\] Intentemos hacer el cálculo para nuestros datos: \\[\\begin{equation} z_r = \\frac{1}{2} \\ln \\left[ {\\frac{1+0.9437}{1-0.9437}} \\right] = \\frac{1}{2} \\ln \\left[ {\\frac{1.9437}{0.0563}} \\right] = 1.77085 \\end{equation}\\] Bien ya sabemos que nuestro valor \\(z_r = 1.77085\\), ahora debemos calcular el intervalo: \\[\\begin{equation} 1.77085 \\pm 1.96 \\frac{1}{\\sqrt{9-3}} \\\\ 1.77085 \\pm 0.8 \\end{equation}\\] Entonces nuestros límites de nuestro intervalo para \\(\\frac{1}{2}\\ln \\left[{\\frac{1+r}{1-r}}\\right]\\) son \\((0.971, 2.571)\\), aproximadamente (tomando en cuenta los múltiples redondeos). Ahora simplemente transformamos el intervalo para \\(\\frac{1}{2}\\ln \\left[{\\frac{1+r}{1-r}}\\right]\\) en un intervalo para \\(\\rho\\): \\[\\begin{equation} \\rho = \\frac{e^{2(0.971)}-1}{e^{2(0.971)}+1} = 0.988 \\\\ \\rho = \\frac{e^{2(2.571)}-1}{e^{2(2.571)}+1} = 0.75 \\end{equation}\\] Más o menos el valor estimado que obtuvimos con la función de R y como pudimos ver es más sencillo obtener el intervalo de confianza para \\(\\rho\\) utilizando el comando cor.test(), que corresponde a \\((0.749, 0.988)\\). 7.2 Regresión lineal En esta sección aprenderemos a encontrar e interpretar la línea que mejor se ajuste a nuestros datos. Ejemplo: Consideremos un conjunto de datos que tienen una relación linear perfecta entre \\(X\\) y \\(Y\\), por ejemplo, la temperatura medida en \\(X\\) = Celsius y \\(Y\\) = Fahrenheit. Sabemos que la fórmula para transformar de grados Celsius a Fahrenheit es \\(y = 32 + \\frac{9}{5}x\\), por lo que esta es la línea que describe esta relación. Celsius &lt;- c(12.3, 12.8, 13.2, 13.8, 14.5, 14.9, 15, 15.1, 15.2, 15.25, 15.6, 15.7, 16.1, 16.3, 16.5, 17, 17.1, 17.7, 17.8, 17.85) Fahrenheit &lt;- Celsius*(9/5) + 32 Temperatura &lt;- data.frame(Celsius, Fahrenheit) Tabla 7.3: Grados Celsius °C contra grados Fahrenheit °F °C °F 12.30 54.14 12.80 55.04 13.20 55.76 13.80 56.84 14.50 58.10 14.90 58.82 15.00 59.00 15.10 59.18 15.20 59.36 15.25 59.45 15.60 60.08 15.70 60.26 16.10 60.98 16.30 61.34 16.50 61.70 17.00 62.60 17.10 62.78 17.70 63.86 17.80 64.04 17.85 64.13 La siguiente tabla muestra la relación entre las dos variables. Figura 7.3: Gráfico de temperatura en °C contra °F Tabla 7.4: Media y desviación estándar de los datos de temperatura °C °F Media 15.48 59.87 SD 1.62 2.92 Como ambas variables están midiendo lo mismo, cuando el valor \\(X\\) aumente en una desviación estándar (\\(s_x = 1.62\\)), es lógico pensar que también el valor \\(y\\) aumentará una desviación estándar (\\(s_y = 2.92\\)). Combinados estos dos valores, se puede describir la pendiente que se ajusta perfectamente a los datos: \\[\\begin{equation} \\frac{s_y}{s_x} = \\frac{2.92}{1.62} = 1.80 \\end{equation}\\] Ya conocíamos este valor antes, que equivale a la pendiente de \\(\\frac{9}{5} = 1.80\\) que vimos en la fórmula para convertir °C a °F. 7.2.1 La línea SD Cuando existe una relación linear perfecta (\\(r = \\pm1\\)) la línea que se ajusta exactamente a los datos será \\(\\pm s_y/s_x\\), con el signo de la pendiente correspondiendo al signo de la correlación y pasa a través del punto, (\\(\\overline{x}, \\overline{y}\\)). Esta línea se le conoce como línea SD. Cuando no tenemos una correlación perfecta, la línea SD tiende a sobrestimar o infraestimar algunos valores. Se puede demostrar, de manera matemática, que la línea que mejor se ajusta para predecir \\(Y\\) es la llamada línea de cuadrados mínimos o regresión lineal ajustada, que tiene una pendiente igual a \\(r(s_y/s_x)\\) y pasa por el punto (\\(\\overline{x},\\overline{y}\\)). Es decir, por cada valor de \\(X\\) que se encuentra a una desviación estándar por encima del promedio, el valor de la media de \\(Y\\) solo estará \\(r\\) desviaciones estándar encima del promedio (asumiendo que \\(r\\) es positivo, si es negativo, entonces por cada valor \\(X\\) una desviación estándar encima del promedio, el valor de la media \\(Y\\) estará \\(r\\) desviaciones estándar debajo del promedio). 7.2.2 Ecuación de la regresión lineal La ecuación para una recta se puede escribir de la siguiente manera: \\[\\begin{equation} Y = b_0 + b_1X \\tag{7.4} \\end{equation}\\] Donde \\(b_0\\) es el intercepto de \\(y\\) y \\(b_1\\) es la pendiente de la línea. \\(b_1\\) también es la razón de cambio de \\(Y\\) con respecto de \\(X\\). La regresión lineal ajustada de \\(Y\\) en \\(X\\) se escribe \\(\\hat{y} = b_0 + b_1x\\). Escribimos \\(\\hat{y}\\) para recordar que la línea nos está dando solamente un estimado o predicción de los valores de \\(Y\\). La pendiente y el intercepto de la linea de regresión de los cuadrados mínimos se calcula a partir de los datos de la siguiente manera: \\[\\begin{equation} Pendiente: b_1 = r(\\frac{s_y}{s_x}) \\tag{7.5} \\end{equation}\\] \\[\\begin{equation} Intercepto: b_0 = \\overline{y} - b_1\\overline{x} \\tag{7.6} \\end{equation}\\] Por ejemplo, para nuestros datos de las serpientes, podemos encontrar la pendiente con nuestro coeficiente de correlación \\(r\\) multiplicado por las desviaciones estándar de los datos de peso y longitud (4.6 y 35.3 respectivamente). Tabla 7.5: Media y desviación estándar de la longitud y peso de serpientes Longitud (cm) Peso (g) Media 63.0 152.0 SD 4.6 35.3 Por ende, \\(s_x = 4.6\\) y \\(s_y = 35.3\\). Además sabíamos que para estos datos, \\(r = 0.9437\\). Tenemos todos los elementos para encontrar nuestra pendiente \\(b_1\\) y nuestro intercepto \\(b_0\\). \\[\\begin{equation} r(s_y/s_x) = 0.9437 \\times (35.337/4.637) = 7.1916 \\end{equation}\\] Lo que equivale a un cambio de 7.1916 g/cm. Es decir que un incremento en la longitud de una serpiente de 1 cm trae consigo un aumento en su peso de cerca de 7.1916 g. Usando estos valores podemos encontrar el intercepto de \\(Y\\). \\[\\begin{equation} b_0 = 152 - 7.7976 \\times 63 = -301.08 \\end{equation}\\] Por ende, nuestra línea de regresión ajustada es \\(\\hat{y} = -301.08 + 7.1916x\\). 7.2.3 Gráfica de promedios Si tenemos varias observaciones de \\(Y\\) en un nivel dado de \\(X\\), como el caso del ejemplo de las anfetaminas y ratas, podemos estimar la media de la población \\(Y\\) para un valor dado de \\(X\\) (\\(\\mu_{Y|X}\\)) simplemente usando la media muestral de \\(Y\\), \\(\\overline{y}\\), para ese valor dado de \\(X\\). Podemos denotar esta media muestral como \\(\\overline{y}|X\\). Una gráfica de \\(\\overline{y}|X\\) se conoce como una gráfica de promedios, ya que muestra la media observada de \\(Y\\) para distintos valores de \\(X\\). Figura 7.4: Gráfica de promedios con los promedios de cada observación de X marcados por una línea horizontal. Donde las líneas horizontales en los distintos niveles de \\(X\\) representan la media para este conjunto de datos. Si los valores de \\(\\overline{y}\\) en la gráfica de promedios encaja exactamente como una línea, entonces esa línea es la regresión linear y \\(\\mu_{Y|X}\\) es estimado a partir de \\(\\overline{y}|X\\). Sin embargo, usualmente los valores de \\(\\overline{y}\\) no son colineares. En este caso, la regresión es una versión suavizada de la gráfica de promedios, resultado en un modelo de ajuste en el cual todos los estimados de \\(\\mu_{Y|X}\\) caen en una línea. La ventaja de este proceso es que utilizamos toda la información disponible de todas las observaciones para estimar \\(\\mu_{Y|X}\\) en cualquier nivel de \\(X\\). Si aplicamos las fórmulas de regresión a nuestros datos de las ratas y las anfetaminas obtenemos que \\(b_0 = 99.3\\) y \\(b_1 = -9.01\\). Entonces el estimado de \\(\\mu_{Y|X=0}\\) es \\(99.3 \\space g/kg\\). Este valor es ligeramente distinto a \\(\\overline{y}|X = 0\\) de \\(100 \\space g/kg\\). Este nuevo valor hace uso de los 8 datos obtenidos cuando \\(X = 0\\) pero también de la tendencia lineal establecida por los otros 16 valores. De igual manera, \\(\\mu_{Y|X=2.5}\\) es \\(99.3 - 9.01 \\times 2.5 = 76.78 \\space g/kg\\), que difiere ligeramente del valor \\(\\overline{y}|X=2.5\\) de \\(75.5 \\space g/kg\\) y \\(\\mu_{Y|X=5}\\) que es \\(99.3-9.01 \\times 5 = 54.25 \\space g/kg\\), que difiere ligeramente de \\(\\overline{y}|X=5\\) que tiene un valor de \\(55.0 \\space g/kg\\). Este es, a grandes rasgos, el procedimiento por el cuál suavizamos una línea recta sobre un gráfico de promedios. 7.2.4 La suma de cuadrados de los residuales Ahora consideraremos un estadístico que describe la dispersión de los puntos respecto de la línea ajustada. Por cada valor \\(x_i\\) de nuestros datos, existe una predicción del valor de \\(Y\\) dado por \\(\\hat{y} = b_0 + b_1x_i\\). Para encontrar nuestro valor predicho, simplemente utilizamos la fórmula de nuestra regresión: \\[\\begin{equation} \\hat{y}_i = b_0 + b_1x_i \\tag{7.7} \\end{equation}\\] Además, asociado a cada observación, existe una cantidad llamada residual, definida como: \\[\\begin{equation} e_i = y_i - \\hat{y}_i \\tag{7.8} \\end{equation}\\] Figura 7.5: Residuales para los datos de longitud y peso de serpientes. En este gráfico con los residuales de nuestros datos de las serpientes vemos que las líneas punteadas van desde nuestro valor observado hasta la línea de la regresión. La magnitud del residual corresponde al valor absoluto de la línea vertical. Los valores predichos \\(\\hat{y}_i\\) se encuentran en color azul en nuestra figura 7.5. Lo que nos importa principalmente es qué tan cerca está el valor observado, \\(y_i\\), de su valor predicho, \\(\\hat{y}_i\\). Por esto medimos la distancia vertical de cada punto hasta la línea ajustada. Una medida que engloba a estos valores es la suma de cuadrados de los residuales o \\(SS(residuales)\\), que se obtiene de la siguiente manera: \\[\\begin{equation} SS(residuales) = \\sum_{i=1}^n(y_i - \\hat{y}_i)^2 = \\sum_{i=1}^n e_i^2 \\tag{7.9} \\end{equation}\\] La suma de cuadrados de los residuales será entonces más pequeña conforme mejor se ajuste la línea a nuestros datos. Veamos estos cálculos realizados con los datos de nuestras serpientes. Tabla 7.6: Residuales y valores predichos para los datos de serpientes \\(x\\) \\(y\\) \\(\\hat{y}\\) \\(y-\\hat{y}\\) \\((y-\\hat{y})^2\\) 60 136 130.42442 5.575581 31.087108 69 198 195.15116 2.848837 8.115873 66 194 173.57558 20.424419 417.156875 64 140 159.19186 -19.191861 368.327508 54 93 87.27326 5.726744 32.795599 67 172 180.76744 -8.767442 76.868037 59 116 123.23256 -7.232558 52.309897 65 174 166.38372 7.616279 58.007707 63 145 152.00000 -7.000000 49.000000 ## [1] 1093.669 Donde \\(x\\) es la longitud, \\(y\\) el peso, \\(\\hat{y}\\) es el valor predicho de \\(y\\) para un punto dado por \\(x\\), \\(y-\\hat{y}\\) son los residuales y \\((y-\\hat{y})^2\\) son los cuadrados de los residuales. Por lo tanto para nuestros datos \\(SS(residuales) = 1093.669\\). El criterio de mínimos cuadrados se basa en que la mejor línea recta es aquella que minimiza la suma de cuadrados de los residuales al mínimo. Debido a esto, la línea de regresión lineal ajustada también es conocida como la línea de mínimos cuadrados. 7.2.5 Desviación estándar de los residuales Esta es una medida derivada de la \\(SS(residuales)\\), es más fácil de interpretar y se representa como \\(s_e\\). \\[\\begin{equation} s_e = \\sqrt{\\frac{\\sum_{i=1}^n(y_i - \\hat{y}_i)^2}{n-2}} = \\sqrt{\\frac{\\sum_{i=1}^ne_i^2}{n-2}} = \\sqrt{\\frac{SS(residuales)}{n-2}} \\tag{7.10} \\end{equation}\\] Esta desviación estándar de los residuales nos dice que tan arriba o debajo de la línea de regresión tienden a estar. Para nuestros datos de las serpientes tenemos la siguiente \\(s_e\\): \\[\\begin{equation} s_e = \\sqrt{1093.669}{n-2} = 12.5 \\end{equation}\\] Por lo tanto, la desviación estándar de los residuales (también conocido como error estándar de los residuales) nos da como resultado 12.5 g/cm. De nuestros datos, esperamos que el 68% de ellos se encuentren a \\(\\pm1 s _e\\) de la línea de regresión (de manera similar, el 95% de ellos se encontrará a \\(\\pm2s_e\\)). ¿Cómo podemos hacer estos cálculos con R? Bueno, recordemos nuestra variable Serpientes que habíamos creado anteriormente. Serpientes ## Longitud Peso ## 1 60 136 ## 2 69 198 ## 3 66 194 ## 4 64 140 ## 5 54 93 ## 6 67 172 ## 7 59 116 ## 8 65 174 ## 9 63 145 A partir de esta variable podemos construir un modelo lineal utilizando la función lm(). Regresion &lt;- lm(Peso ~ Longitud, Serpientes) #Crea el modelo lineal summary(Regresion) ## ## Call: ## lm(formula = Peso ~ Longitud, data = Serpientes) ## ## Residuals: ## Min 1Q Median 3Q Max ## -19.192 -7.233 2.849 5.727 20.424 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -301.0872 60.1885 -5.002 0.001561 ** ## Longitud 7.1919 0.9531 7.546 0.000132 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 12.5 on 7 degrees of freedom ## Multiple R-squared: 0.8905, Adjusted R-squared: 0.8749 ## F-statistic: 56.94 on 1 and 7 DF, p-value: 0.0001321 Como podemos ver, nos da el valor de la desviación estándar residual, pero, ¿Cómo hacemos para ver los valores predichos? ¿O para ver los residuales? Para esto vamos a agregar dos columnas a nuestra variable con los datos de las serpientes. Utilizamos predict() para extraer los valores de \\(\\hat{y}\\) y residuals() para extraer los residuales. Serpientes$Predicho &lt;- predict(Regresion) #Guarda los valores predichos Serpientes$Residuales &lt;- residuals(Regresion) #Guarda los residuales Serpientes ## Longitud Peso Predicho Residuales ## 1 60 136 130.42442 5.575581 ## 2 69 198 195.15116 2.848837 ## 3 66 194 173.57558 20.424419 ## 4 64 140 159.19186 -19.191860 ## 5 54 93 87.27326 5.726744 ## 6 67 172 180.76744 -8.767442 ## 7 59 116 123.23256 -7.232558 ## 8 65 174 166.38372 7.616279 ## 9 63 145 152.00000 -7.000000 7.2.6 El coeficiente de determinación Cuando elevamos \\(r\\) al cuadrado, obtenemos información adicional de la regresión. El coeficiente de determinación, \\(r^2\\), describe la proporción de la varianza en \\(Y\\) que es explicada por la relación lineal entre \\(X\\) y \\(Y\\). El coeficiente de correlación \\(r\\) obedece la siguiente aproximación: \\[\\begin{equation} r^2 \\approx \\frac{s_y^2-s_e^2}{s_y^2} = 1 - \\frac{s_e^2}{s_y^2} \\end{equation}\\] El numerador \\(s_y^2 - s_e^2\\) puede ser interpretado como la varianza total en \\(Y\\) explicada por la línea de regresión. Si la línea de regresión se ajusta bien a los datos, entonces \\(s_e^2\\) será cercano a 0, por lo que el numerador será cercano a \\(s_y^2\\) y \\(r^2\\) será cercano a 1. Nuestro valor de \\(r^2\\) adquiere valores que van desde 0 a 1. Para nuestros datos de las serpientes, \\(r = 0.9437\\), por lo que \\(r^2 = 0.8906\\). Esto quiere decir que 89.06% de la varianza en el peso de las serpientes es explicada por esta relación. 7.3 El model lineal Podemos considerar realizar inferencia a partir de los datos sobre una población más grande. Una población condicional de \\(Y\\) es una población en la cual los valores de \\(Y\\) están asociados o fijos, dados un valor de \\(X\\). Dentro de una población condicional, podemos hablar de una distribución condicional de \\(Y\\). \\(\\mu_{Y|X}\\) corresponde a la media poblacional de \\(Y\\) para un valor dado de \\(X\\). \\(\\sigma_{Y|X}\\) corresponde a la desviación estándar poblacional para un valor dado de \\(X\\). Por ejemplo, en el caso de las ratas y las anfetaminas, \\(\\mu_{Y|X}\\) y \\(\\sigma_{Y|X}\\) representan la media y la desviación estándar de los valores de consumo de comida para las ratas dada la dosis \\(X\\) de anfetamina. Cabe aclarar que en estudios observacionales, las distribuciones condicionales corresponden a subpoblaciones más que a unidades experimentales. Para el modelo lineal, se le puede dar una interpretación paramétrica si se cumplen dos condiciones: - Linearidad: \\(Y = \\mu_{Y|X} + \\epsilon\\) donde \\(\\mu_{Y|X}\\) es la función lineal de \\(X\\), es decir, \\(\\mu_{Y|X} = \\beta_0 + \\beta_1X\\). Por ende \\(Y = \\beta_0 + \\beta_1X + \\epsilon\\). - Constancia en la desviación estándar: o sea, que \\(\\sigma_{Y|X}\\) no dependa de \\(X\\). Denotamos este valor constante como \\(\\sigma_\\epsilon\\). Por lo tanto, la fórmula general para el modelo lineal es: \\[\\begin{equation} Y = \\beta_0 + \\beta_1X + \\epsilon \\tag{7.11} \\end{equation}\\] Como en el caso del ANOVA, el término \\(\\epsilon\\) representa el error aleatorio. La desviación estándar condicional es la que determina la variabilidad de \\(Y\\) en un valor dado de \\(X\\), sin embargo, ya que el modelo lineal determina que la desviación estándar es la misma para todos los valores de \\(X\\), entonces la representamos como \\(\\sigma_\\epsilon\\) y nos referimos a ella como la desviación estándar del error aleatorio. 7.3.1 Estmiaciones del modelo lineal Al aplicar el modelo lineal estamos dispuestos a asumir que también aceptamos el modelo de submuestreo aleatorio, el cuál dice que para cada par de observaciones (\\(x, y\\)), consideramos al valor de \\(y\\) como si hubiese sido muestreado de manera aleatoria de la población condicional de los valores de \\(Y\\) asociados al valor \\(x\\) de \\(X\\). Bajo este modelo, los valores \\(b_0, b_1\\) y \\(s_e\\) calculados en la regresión lineal se interpretan como estimadores de los parámetros poblacionales: -\\(b_0\\) es un estimado de \\(\\beta_0\\) -\\(b_1\\) es un estimado de \\(\\beta_1\\) -\\(s_e\\) es un estimado de \\(\\sigma_\\epsilon\\) Gracias a la función summary(Regresion) y a los cálculos realizados anteriormente, sabemos que \\(b_0 = -301.08\\), \\(b_1 = 7.1916\\) y que \\(s_e = 12.5\\) para los datos de nuestras serpientes. summary(Regresion) ## ## Call: ## lm(formula = Peso ~ Longitud, data = Serpientes) ## ## Residuals: ## Min 1Q Median 3Q Max ## -19.192 -7.233 2.849 5.727 20.424 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -301.0872 60.1885 -5.002 0.001561 ** ## Longitud 7.1919 0.9531 7.546 0.000132 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 12.5 on 7 degrees of freedom ## Multiple R-squared: 0.8905, Adjusted R-squared: 0.8749 ## F-statistic: 56.94 on 1 and 7 DF, p-value: 0.0001321 La pendiente de la línea de regresión \\(b_0 = 7.1916\\) g/cm es un estimado de un parámetro morfológico (cambios en unidades de peso por cada unidad de longitud). Además, encontramos una variabilidad estimada de 12.5 g en el peso de las serpientes para un valor fijo de \\(X\\). 7.3.2 Interpolación en el modelo lineal Imaginemos que quisiéramos saber el peso de una hipotética serpiente que midiese 63.5 cm. Ya que en nuestros datos no tenemos ningún valor que nos de un estimado, tenemos que realizar una interpolación a partir de nuestra fórmula de regresión lineal. Para el caso de las serpientes, \\(\\hat{y} = (-301.08) + 7.1916 \\times 63.5 = 155.59\\), lo que corresponde a un promedio estimado de 155.9 g con una desviación estándar \\(s_e = 12.5\\) g (esto no quiere decir que el valor de \\(y\\) será 155.59 g, pero si obtenemos múltiples observaciones de este valor cuando \\(X = 63.5\\) en promedio, obtendremos 155.59 g). Cabe aclarar que lo que hicimos fue una interpolación porque, el dato estimado que queríamos saber se encuentra dentro del rango de nuestros datos. Cuando estimamos valores fuera del rango de nuestros datos, se le conoce como extrapolación. Aunque esto se recomienda evitar siempre que sea posible, ya que no hay garantía de que la linearidad se mantendrá fuera del conjunto de datos observados. La ventaja de realizar predicciones utilizando el modelo de la regresión lineal es que, este hace uso de todo el conjunto de datos que tenemos. Ejemplo: Supongamos que capturamos una hembra de la serpiente V. bertis y queremos saber cuál es su peso. No hemos medido a la serpiente aún, así que no conocemos su longitud. Para este caso, nuestro mejor estimado sería utilizar la media para el peso, \\(\\overline{y} = 152\\) g. Ejemplo: tras realizar la medición correspondiente, sabemos que la hembra de V. bertis mide 63 cm Tomando en cuenta esta información, ¿cuál es el mejor estimado de su peso? Sabemos por el grupo que capturamos anteriormente que el promedio del peso de las serpientes que miden 63 cm es, \\(\\overline{y}|x = 145\\) (ya que solamente capturamos una serpiente que midiera 63 cm). Ejemplo: los investigadores ajustan una línea para el conjunto de datos de V. bertis capturadas anteriormente, por lo que sabemos que la lína de regresión lineal de mínimos cuadrados es \\(Y = -301.08 - 7.1916X\\). Ahora que sabemos esto, podemos dar un estimado mucho mejor sobre cuál será el promedio del peso de las serpientes cuando miden 63 cm. \\(\\hat{y} = -301.08 + 7.1916 \\times 63 = 151.9905\\) cm. Esperamos que la última predicción sea la mejor, siempre y cuando creamos que existen una relación lineal entre las variables \\(Y\\) y \\(X\\). Además nos da resultados para valores que no se encuentren dentro del conjunto de datos. Es importante saber que esto será verdad siempre y cuando se mantenga la relación lineal de las variables. 7.4 Inferencia estadística respecto de \\(\\beta_1\\) En esta parte consideraremos la inferencia acerca de la verdadera pendiente de \\(\\beta_1\\) de la regresión lineal. Basado en que la condición de que la distribución condicional de la población \\(Y\\) para cada valor de \\(X\\) tiene una distribución normal. Esto quiere decir que en el modelo lineal \\(Y = \\beta_0 + \\beta_1X + \\epsilon\\), el valor de \\(\\epsilon\\) tiene una distribución normal. 7.4.1 El error estándar de \\(b_1\\) En el contexto del modelo lineal, \\(b_1\\) es un estimado de \\(\\beta_1\\) y como todo valor estimado, está sujeto a error de muestreo. La fórmula para calcularlo es la siguiente: \\[\\begin{equation} SE_{b_1} = \\frac{s_e}{s_x\\sqrt{n-1}} \\tag{7.12} \\end{equation}\\] Para los datos de las serpientes, tenemos que \\(n = 9\\), \\(s_x = 4.637\\), y que \\(s_e = 12.5\\). Entonces el error estándar de \\(b_0\\) sería: \\[\\begin{equation} SE_{b_1} = \\frac{12.5}{4.637 \\sqrt{9-1}} = 0.9531 \\end{equation}\\] Entonces, el error estándar de nuestra pendiente \\(b_1 = 0.9531\\). El valor de \\(b_1\\) depende de varios aspectos de los datos. \\(SE_{b_1}\\) depende de la dispersión de los datos sobre la regresión lineal (\\(s_e\\)) y en el tamaño de muestreo \\(n\\). Datos con menor dispersión sobre la regresión lineal (una \\(s_e\\) más pequeña) y tamaños de muestra más grandes, producen mejores estimadores de \\(\\beta_1\\). Además de esto, existe un tercer factor que afecta al \\(SE_{b_1}\\) y es la variabilidad de \\(X\\). Entre más dispresos estén los valores de \\(X\\) (mayor \\(s_x\\)), mayor será la precisión de nuestra estimación. Esto tiene implicaciones en el diseño experimental, ya que si podemos controlar la dispresión de los valores de \\(X\\) preferiríamos datos que minimicen el valor del \\(SE_{b_1}\\). 7.4.2 Intervalo de confianza para \\(\\beta_1\\) Un intervalo de confianza para \\(\\beta_1\\) se puede construir basándose en la distribución \\(t\\) de Student. Entonces, un intervalo de confianza al 95% sería: \\[\\begin{equation} b_1 \\pm t_{0.025}SE_{b_1} \\end{equation}\\] En dónde el valor crítico de \\(t\\) se determina por la distribución \\(t\\) de Student con los grados de libertad: \\[\\begin{equation} df = n -2 \\end{equation}\\] Para el caso de las serpientes, nuestro intervalo de confianza sería: \\[\\begin{equation} 7.1916 \\pm 2.365 \\times 0.9531 \\end{equation}\\] Por lo que nuestro intervalo sería \\((4.94, 9.45)\\) g/cm. Podemos calcular el intervalo de confianza en R con la función `confint(). confint(Regresion) ## 2.5 % 97.5 % ## (Intercept) -443.410309 -158.764110 ## Longitud 4.938183 9.445538 Donde obtenemos el intervalo de confianza tanto para \\(b_0\\) como para \\(b_1\\). En este caso nos interesa el de nuestra pendiente \\(b_1\\). 7.5 Prueba de hipótesis para \\(H_0: \\beta_1 = 0\\) En algunas investigaciones, no es una conclusión inevitable que exista una relación lineal entre \\(X\\) y \\(Y\\). Puede ser entonces importante considerar qu cualquier tendencia aparente surgió por azar, que sea ilusora y que solo refleje la variabilidad del muestreo. Entonces: \\[\\begin{equation} H_0: \\mu_{Y|X} \\space no \\space depende \\space de \\space X \\end{equation}\\] Lo que se puede traducir a \\(H_0: \\beta_1 = 0\\). Entonces nuestro estadístico de prueba sería: \\[\\begin{equation} t_s = \\frac{b_1 - 0}{SE_{b_1}} \\tag{7.13} \\end{equation}\\] Y de nuevo, los valores críticos se calculan con los grados de libertad, \\(df = n - 2\\). En R este proceso ya lo hemos realizado con la función summary() aplicada a nuestro modelo. El valor \\(0\\) en la fórmula (7.13) nos recuerda que estamos comparando el valor esperado bajo la \\(H_0\\) con el valor obtenido. summary(Regresion) ## ## Call: ## lm(formula = Peso ~ Longitud, data = Serpientes) ## ## Residuals: ## Min 1Q Median 3Q Max ## -19.192 -7.233 2.849 5.727 20.424 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -301.0872 60.1885 -5.002 0.001561 ** ## Longitud 7.1919 0.9531 7.546 0.000132 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 12.5 on 7 degrees of freedom ## Multiple R-squared: 0.8905, Adjusted R-squared: 0.8749 ## F-statistic: 56.94 on 1 and 7 DF, p-value: 0.0001321 En este caso podemos ver que el valor \\(t\\) ya está dado en el apartado de los coeficientes (\\(t_s = 7.546\\)), junto con nuestro valor-p denotado por el título Pr(&gt;|t|), que en este caso corresponde a \\(valor-p = 0.000132\\), por lo tanto rechazamos la \\(H_0\\) ya que \\(valor-p &lt; 0.05\\). Hay que interpretar esta prueba con cuidado. La prueba de hipótesis para \\(\\beta_1\\) no nos dice si existen una relación lineal entre \\(\\mu_{Y|X}\\) y \\(X\\), más bien, asume que el modelo linea es verdadero y nos dice si podemos concluir que la pendiente no es 0. 7.5.1 Gráficos de residuales Son bastante útiles para detectar anomalías en los datos (como algún patrón distinto al patrón lineal). Estos gráficos, conocidos como gráficos de residuales es lo que queda después de que removemos la tendencia lineal, por lo que esperamos que sean gráficos sin un patrón conciso. Por ejemplo, la figura 7.6 Figura 7.6: Gráfico de residuales mostrando la ausencia de curvatura o patrón para los datos de serpientes. De igual manera si las condiciones de normalidad se cumplen, deberíamos ver esto en un gráfico en el que se comparan los valores de los residuales con los valores \\(Z\\). Figura 7.7: Gráfico de residuales comparados con la valores Z teóricos para mostrar la normalidad. Como podemos ver también se sigue una distribución más o menos lineal cuando se compara con los valores \\(Z\\) teóricos. 7.6 Intervalos de confianza y de predicción En esta parte veremos la diferencia entre la predicción de la media del valor de \\(Y\\) para un valor \\(X\\) dado con obtener un valor específico de \\(Y\\) para un valor de \\(X\\) dado. Por ejemplo, ya sabemos que si queremos predecir la media del peso de una de las serpientes con una longitud de 63.5 cm la media de nuestras observaciones será 155.59 g. Sin embargo, ¿qué valor obtendríamos si quisieramos calcular un valor puntual? Pues la respuesta es que obtendríamos el mismo valor de 155.59 g. Utilizamos la regresión lineal de la misma manera, sin embargo, la precisión de nuestras predicciones son distinas. 7.6.1 Computando los intervalos Supongamos que queremos predecir \\(\\mu_{Y|X=x^*}\\) o \\(Y|X=x^*\\). Es decir, predicr la media o el valor actual de \\(Y\\) cuando el valor de \\(X = x^*\\). Las fórmulas para el intevalo de confianza (Fórmula @eq:ICreg) y el intervalo de la predicción (Fórmula @eq:ICreg) son: \\[\\begin{equation} \\hat{y} \\pm t_{0.025}s_e\\sqrt{\\frac{1}{n} + \\frac{(x^*-\\overline{x})^2}{(n-1)s_x^2}} \\tag{7.14} \\end{equation}\\] \\[\\begin{equation} \\hat{y} \\pm t_{0.025}s_e\\sqrt{1 + \\frac{1}{n} + \\frac{(x^*-\\overline{x})^2}{(n-1)s_x^2}} \\tag{7.15} \\end{equation}\\] Con el valor de \\(t_{0.025}\\) determinado por la distribución de \\(t\\) y los \\(df = n - 2\\). Aunque las fórmulas son muy parecidas, el \\(1\\) agregado debajo de la raíz cuadrada factoriza la variabilidad asociada cuando intentamos hacer una predicción individual. Figura 7.8: Intervalo de confianza al 95% (gris) e intervalo de la predicción al 95% (línea azul punteada) para los datos de serpientes. Como podemos ver en la figura 7.8, el intervalo de confianza es muchísimo más preciso que el intervalo de predicción, como se mencionó anteriormente. Para obtener los intervalos de confianza y de predicción en R podemos utilizar las siguientes funciones basadas en nuestro modelo lineal. Conf.int &lt;- predict(Regresion, interval = &quot;confidence&quot;) Conf.int ## fit lwr upr ## 1 130.42442 118.47544 142.3734 ## 2 195.15116 178.42057 211.8818 ## 3 173.57558 161.62660 185.5246 ## 4 159.19186 149.08515 169.2986 ## 5 87.27326 64.72396 109.8225 ## 6 180.76744 167.41336 194.1215 ## 7 123.23256 109.87848 136.5866 ## 8 166.38372 155.54939 177.2181 ## 9 152.00000 142.14777 161.8522 Pred.int &lt;- predict(Regresion, interval= &quot;prediction&quot;) ## Warning in predict.lm(Regresion, interval = &quot;prediction&quot;): predictions on current data refer to _future_ responses Pred.int ## fit lwr upr ## 1 130.42442 98.54375 162.3051 ## 2 195.15116 161.18778 229.1145 ## 3 173.57558 141.69492 205.4562 ## 4 159.19186 127.95495 190.4288 ## 5 87.27326 50.09706 124.4494 ## 6 180.76744 148.33397 213.2009 ## 7 123.23256 90.79908 155.6660 ## 8 166.38372 134.90386 197.8636 ## 9 152.00000 120.84450 183.1555 Si vemos los resultados, los valores del intervalo de confianza son más estrechos que los del intervalo de predicción. El apartado fit corresponde al valor predicho para cierto valor de \\(X\\). Con el argumento level podemos cambiar el nivel de % para nuestros intervalos. "],["bibliografia.html", "Bibliografia", " Bibliografia Allaire, J., Xie, Y., McPherson, J., Luraschi, J., Ushey, K., Atkins, A., Wickham, H., Cheng, J., Chang, W., &amp; Iannone, R. (2021). Rmarkdown: Dynamic documents for r. https://CRAN.R-project.org/package=rmarkdown Palacio, F. X., Apodaca, M. J., &amp; Crisci, J. V. (2020). Análisis multivariado para datos biológicos. Teoría y su aplicación utilizando el lenguaje r. Fundación Azara. R Core Team. (2020). R: A language and environment for statistical computing. R Foundation for Statistical Computing. https://www.R-project.org/ Samuels, M. L., Witmer, J. A., &amp; Schaffner, A. A. (2016). Statistics for the life sciences (5th Edition). Pearson. Triola, M. F. (2018). Estadística (12 Edición). Pearson. Xie, Y. (2014). Knitr: A comprehensive tool for reproducible research in R. In V. Stodden, F. Leisch, &amp; R. D. Peng (Eds.), Implementing reproducible computational research. Chapman; Hall/CRC. http://www.crcpress.com/product/isbn/9781466561595 Xie, Y. (2015). Dynamic documents with R and knitr (2nd ed.). Chapman; Hall/CRC. https://yihui.org/knitr/ Xie, Y. (2016). Bookdown: Authoring books and technical documents with R markdown. Chapman; Hall/CRC. https://github.com/rstudio/bookdown Xie, Y. (2020). Bookdown: Authoring books and technical documents with r markdown. https://github.com/rstudio/bookdown Xie, Y. (2021). Knitr: A general-purpose package for dynamic report generation in r. https://yihui.org/knitr/ Xie, Y., Allaire, J. J., &amp; Grolemund, G. (2018). R markdown: The definitive guide. Chapman; Hall/CRC. https://bookdown.org/yihui/rmarkdown Xie, Y., Dervieux, C., &amp; Riederer, E. (2020). R markdown cookbook. Chapman; Hall/CRC. https://bookdown.org/yihui/rmarkdown-cookbook "]]
