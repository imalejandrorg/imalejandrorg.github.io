[["probabilidad.html", "Lección 3 Probabilidad 3.1 Probabilidad frecuentista 3.2 Reglas de probabilidad 3.3 Curvas de densidad 3.4 Distribución binomial 3.5 Distribución de Poisson 3.6 Distribución normal 3.7 Distribución muestral", " Lección 3 Probabilidad La probabilidad es una cantidad numérica que expresa qué tan factible es que ocurra un evento. Normalmente es expresada como \\(P(A)\\), donde \\(A\\) es un evento aleatorio. Siempre se encuentra entre un rango de 0 y 1, o expresado en porcentaje, entre 0% y 100%. Existen distintos acercamientos a la probabilidad como el acercamiento frecuentista o el acercamiento bayesiano. En este caso, aprenderemos el enfoque frecuentista como una forma de asignar una probabilidad mensurable a un evento, es decir, la ocurrencia de el evento a la larga (qué tan frecuentemente ocurre). 3.1 Probabilidad frecuentista En este enfoque, la probabilidad de un evento se determina a través por el número de veces que el evento \\(A\\) ocurre en una serie de repeticiones indefinidamente largas. \\[\\begin{equation} P(A) = \\frac{num_{A}}{num_{T}} \\tag{3.1} \\end{equation}\\] Donde \\(num_{A}\\) es el número de veces que se repite el evento \\(A\\) y \\(num_{T}\\) es el número total de repeticiones. 3.2 Reglas de probabilidad Estas son algunas reglas que nos pueden servir para determinar la probabilidad de algunos eventos. La probabilidad de un evento \\(A\\) siempre se encuentra entre 0 y 1. La suma de probabilidades de los eventos tiene que ser igual a 1 (en caso de que sean complementarios). La probabilidad de que el evento \\(A\\) no ocurra es \\(1 - P(A)\\) y se denota con el símbolo \\(\\overline{A}_1\\), indicando que es el complemento de \\(A_1\\). A este tipo de eventos se les conoce como eventos complementarios. Si dos eventos \\(A_1\\) y \\(A_2\\) son eventos que no ocurren en conjunto, entonces: \\[ P(A_1 \\cup A_2) = P(A_1) + P(A_2) \\] Donde \\(\\cup\\) nos sirve para indicar que ocurre el evento \\(A_1\\) o el evento \\(A_2\\). En cambio, \\(\\cap\\) indica que ocurre el evento \\(A_1\\) y el evento \\(A_2\\) al mismo tiempo. Para dos eventos que ocurren en conjunto: \\[ P(A_1 \\cup A_2) = P(A_1) + P(A_2) - P(A_1 \\cap A_2) \\] Cabello Café Cabello Negro Cabello Rojo Total Ojos Cafés 400 300 20 720 Ojos Azules 800 200 50 1050 Total 1200 500 70 1770 En este caso, la probabilidad de que alguien tenga el cabello café (CC) o rojo (CR): \\[ P(CC \\cup CR) = P(CC) + P(CR) = 1200/1770 + 70 / 1770 \\] La probabilidad de que alguien tenga el cabello café: \\[ P(CC) = 1200/1770 \\] La probabilidad de tener ojos azules: \\[ P(OA) = 1050/1770 \\] La probabilidad de tener cabello negro y ojos azules \\(P(CN \\cap OA)\\) pueden ocurrir en conjunto ya que hay 200 personas con ojos azules y cabello negro. Entonces si queremos saber la probabilidad de que alguien tenga cabello negro u ojos azules: \\[ P(CN \\cup OA) = P(CN) + P(OA) - P(CN \\cap OA) = 500/1770 + 1050/1770 - 200/1770 = 1350/1770 \\] La fórmula de probabilidad condicional es la siguiente: \\[ P(A_1 | A_2) = \\frac{P(A_1 \\cap A_2)}{P(A_2)} \\] Si dos eventos, \\(A_1\\) y \\(A_2\\) son independientes, entonces: \\[ P(A_1 \\cap A_2) = P(A_1) \\times P(A_2) \\] Para cualquier evento \\(A_1\\) y \\(A_2\\): \\[ P(A_1 \\cap A_2) = P(A_1) \\times P(A_2 | A_1) \\] Por ejemplo, de la tabla anterior, ¿Cuál es la probabilidad de que una persona tenga cabello rojo y ojos cafés? En este caso: \\[ P(CR \\cap OC) = P(CR) \\times P(OC | CR) = 70/1770 \\times 20/70 = 20/1770 \\] Para dos eventos cualesquiera, \\(A_1\\) y \\(A_2\\): \\[ P(A_1) = P(A_2) \\times P(A_1 | A_2) + P(\\overline{A}_2) \\times P(A_1 | \\overline{A}_2 ) \\] Donde \\(\\overline{A}_2\\) es el complemento de \\(A_2\\), es decir \\(1 - A_2\\). 3.3 Curvas de densidad Una distribución de probabilidad corresponde a una descripción que da la probabilidad para cada valor de una variable aleatoria. La variable aleatoria puede ser discreta o puede ser continua, tal como vimos con los tipos de datos en la sección 2.1. Un histogramas de frecuencias relativas representa las proporciones de las observaciones en cada categoría, en lugar del total de observaciones. Sin embargo, para variables continuas normalmente se utilizan clases muy estrechas para representar al histograma como una curva de densidad. La coordenada \\(y\\) de una curva de densidad representa la escala de densidad y a menudo las frecuencias relativas se representan como áreas debajo de la curva. Algunos de los requisitos de una distribución de probabilidad son: Que exista una variable aleatoria numérica \\(x\\) y que sus valores estén asociados a una probabilidad. Que \\(\\sum P(x) = 1\\), donde \\(x\\) asume todos los los valores posibles. El área total bajo la curva de densidad debe ser igual a 1. Que la probabilidad de ver a \\(x_i\\) se encuentre entre 0 y 1. Es decir, \\(0 \\le P(x_i) \\le 1\\). Algunos ejemplos de distribuciones de probabilidad para variables discretas son: distribución binomial y distribución de Poisson. En el caso de variables continuas encontramos: distribución normal, distribución \\(t\\) de Student, distribución de F, distribución de Chi, etc. En la figura 3.1 podemos ver un histograma de frecuencias relativas junto a una curva de densidad. Figura 3.1: Curva de densidad junto con un histograma de frecuencias relativas. La probabilidad para una variable continua equivale al área debajo de la curva de densidad entre dos puntos. Si quisiéramos saber el valor de un punto exacto, obtendríamos de manera un tanto paradójica 0, ya que lo que equivale a la probabilidad es el área bajo la curva. Algunos de los parámetros de una distribución de probabilidad son: Media: \\(\\mu\\) Varianza: \\(\\sigma^2\\) Desviación estándar: \\(\\sigma\\) Tamaño de muestra: \\(n\\) 3.4 Distribución binomial La distribución binomial de una variable aleatoria es una distribución de probabilidad para una variable discreta, que cuenta el número de éxitos tras realizar \\(n\\) veces un experimento, cada intento debe ser independiente entre sí. Además, la probabilidad de éxito es fija entre cada intento y se denota con la letra \\(p\\). Lo primero que se nos viene a la mente con este tipo de distribuciones son lanzamientos de moneda, sin embargo, el comportamiento de muchos genes también sigue una distribución binomial. Normalmente, para que una variable aleatoria sea una variable aleatoria binomial se tienen que cumplir los siguientes requisitos: Resultados binarios: Solamente existen dos posibilidades para cada intento (éxito o fracaso, cara o cruz, dominante o recesivo, muerto o vivo, niño o niña, etc.). Intentos independientes: Cada prueba o intento deben ser independientes del anterior. El valor de \\(n\\) es fijo: Se sabe con antelación el número de pruebas \\(n\\). Mismo valor \\(p\\): En todos los casos, la probabilidad de éxito o fracaso no debe cambiar, es decir \\(p\\) debe permanecer constante. Resultados mutuamente excluyentes: Es decir, no se puede tener éxito y fracaso al mismo tiempo. Resultados colectivamente exhaustivos: Al menos uno de los dos resultados debe de ocurrir. La función de masa de probabilidad (PMF) o fórmula de la distribución binomial se indica en la fórmula (3.2). \\[\\begin{equation} P(X = k) = {\\binom{n}{k}}{p^{k}}{q^{n-k}} \\tag{3.2} \\end{equation}\\] Donde: \\(P(X = k)\\) = probabilidad de obtener \\(k\\) éxitos. \\(\\binom{n}{k}\\) = coeficiente binomial, que se calcula con la fórmula (3.3). \\(k\\) = número de éxitos. \\(p\\) = probabilidad de éxito. \\(q\\) = probabilidad de fracaso. \\(n\\) = número de pruebas. \\[\\begin{equation} _{n}C_{k} = \\binom{n}{k} = \\frac{n!}{k!(n-k)!} \\tag{3.3} \\end{equation}\\] 3.4.1 Media y desviación estándar de la distribución binomial Se pueden calcular tanto la media como la desviación estándar de una distribución binomial. En este caso la media es el número promedio de éxitos (\\(k\\)) y la desviación estándar es qué tan lejos de esta media caen el resto de los valores de la distribución. La media de una distribución binomial se calcula con la fórmula (3.4). \\[\\begin{equation} \\mu = {n}{p} \\tag{3.4} \\end{equation}\\] En cuanto a la desviación estándar se calcula con la siguiente fórmula (3.5). \\[\\begin{equation} \\sigma = \\sqrt{{n}{p}{(1-p)}} = \\sqrt{{n}{p}{q}} \\tag{3.5} \\end{equation}\\] En caso de querer obtener la varianza, simplemente tomamos la fórmula (3.5) sin aplicar la raíz cuadrada. \\[\\begin{equation} \\sigma^2 = {n}{p}{(1-p)} = {n}{p}{q} \\tag{3.6} \\end{equation}\\] Para utilizar la distribución binomial en R es bastante sencillo. Veamos el siguiente ejemplo. Ejemplo: Supongamos que analizamos a 5 individuos de una población en la que el 37% de las personas presentan un alelo mutante. Las probabilidades de las distintas configuraciones están dadas por la distribución binomial, en donde \\(n\\) = 5 y \\(p\\) = 0.37. ¿Cuál es la probabilidad de que exactamente 2 personas sean mutantes? Antes de continuar, hay que saber que existen 4 distintos comandos (o funciones) para la función binomial: dbinom() nos da un valor exacto de la distribución binomial en el punto indicado. pbinom() nos da la probabilidad acumulada de un evento. qbinom() toma el valor de probabilidad que le ponemos como primer argumento y nos da como regreso un número cuya probabilidad acumulada empate con el valor de la probabilidad (también conocida como función cuantil). rbinom() genera cierta cantidad de número aleatorios de acuerdo con la probabilidad y el número de pruebas realizadas. En este caso, queremos conocer la probabilidad exacta de que 2 personas sean mutantes. Para esto necesitamos la función dbinom(x, size, prob), donde el argumento x equivale al número de éxitos, size al número de ensayos y prob a la probabilidad de éxito. dbinom(2, 5, 0.37) ## [1] 0.3423143 Figura 3.2: Distribución de probabilidad binomial para el ejercicio de alelos mutantes. Ejemplo: En Estados Unidos, 85% de la población tiene sangre Rh positivo. Supongamos que tomamos 6 personas y contamos cuántos tiene Rh positivo. En este caso \\(Y\\) representará cuántas personas tienen Rh positivo dentro del grupo de 6. ¿Cuál es la probabilidad de \\(Y\\) = 4? ¿Y la probabilidad de que al menos 4 personas sean Rh positivo? ¿Y la probabilidad de que haya al menos 1 persona con Rh negativo? En este caso debemos utilizar dos comandos, dbinom() y pbinom(), con uno calcularemos \\(Y = 4\\) y con el otro \\(Y \\ge 4\\) (4 o más). Como queremos obtener \\(P(Y \\ge 4)\\), entonces: \\[ P(Y \\ge 4) = P(Y = 4) + P(Y=5) + P(Y=6) \\] Utilizamos el argumento lower.tail y lo ponemos en FALSE para indicar que vamos a trabajar con un valor mínimo de 4, para arriba. Para el caso de al menos una persona con Rh negativo \\(P(Y &lt; 6)\\), tenemos dos opciones. Usar dbinom() cinco veces para calcular el valor individual de: \\[ P(Y = 0) + P(Y = 1) + P(Y = 2) \\cdots + P(Y = 5) \\] O calcular \\(P(Y = 6)\\) y restarle a 1 este valor. Como vemos ambas operaciones dan los mismos resultados, pero es más sencilla la segunda. #Probabilidad de que 4 personas tengan Rh positivo. dbinom(4, 6, 0.85) ## [1] 0.1761771 Figura 3.3: Distribución binomial para los datos de Rh. En azul encontramos \\(P(X = 4)\\). #Probabilidad de que al menos 4 personas (pueden ser 4, 5 o 6) tengan Rh positivo. pbinom(3, 6, 0.85, lower.tail = FALSE) ## [1] 0.9526614 Figura 3.4: Distribución binomial para los datos de Rh. En azul encontramos la probabilidad al menos 4 personas sean Rh +. #Probabilidad de que al menos 1 persona tenga Rh negativo. dbinom(0, 6, 0.85) + dbinom(1, 6, 0.85) + dbinom(2, 6, 0.85) + dbinom(3, 6, 0.85) + dbinom(4, 6, 0.85) + dbinom(5, 6, 0.85) ## [1] 0.6228505 1 - dbinom(6, 6, 0.85) ## [1] 0.6228505 Figura 3.5: Distribución binomial para los datos de Rh. En azul se ve la probabilidad de que al menos una persona tenga Rh -. Como vemos en la distribución binomial, nuestra distribución de probabilidades está siempre representada por un histograma de probabilidades, a diferencia de las distribuciones de probabilidad de variables continuas que están representados por una curva de densidad. 3.5 Distribución de Poisson La distribución de Poisson es llamada así en honor al matemático francés Simeón Denis Poisson. Se utiliza para describir la probabilidad de variables discretas, al igual que la distribución binomial. Se utiliza para eventos aleatorios representados en un intervalo de tiempo determinado, aunque también es utilizada cuando el evento de interés se distribuye en un espacio plano o tridimensional. Nuestro suceso de interés (\\(X\\)) tiene valores discretos, mientras que nuestro intervalo tiene valores continuos. La fórmula de la distribución de Poisson (PMF) es: \\[\\begin{equation} P(X = k | \\lambda) = \\frac{e^{-\\lambda}\\lambda^k}{k!} \\tag{3.7} \\end{equation}\\] Donde: \\(P(X=k|\\lambda)\\) = número de observaciones \\(k\\) dado la media \\(\\lambda\\). \\(e\\) = número de Euler (aproximadamente 2.7183). \\(\\lambda\\) = parámetro de la distribución de Poisson, corresponde al promedio de veces que ocurre el evento aleatorio. Para que una variable sea considerada una variable aleatoria de Poisson se deben de cumplir los siguientes requisitos: Cada evento tiene que ser independiente. Las ocurrencias del evento tienen que ser aleatorias. El número de eventos en un intervalo de tiempo o espacio es infinito (\\(k = 0, 1, 2, \\cdots, \\infty\\)). La probabilidad de que un evento se presente en un intervalo de tiempo es proporcional al intervalo de tiempo o espacio. Se considera que la probabilidad de que dos eventos se presenten en la misma fracción de tiempo es tan pequeña que se puede considerar inexistente. 3.5.1 Media y desviación estándar de la distribución de Poisson Algo interesante de la distribución de Poisson es que la varianza y la media corresponden al mismo valor (\\(\\lambda\\)) por lo que la desviación estándar sería: \\[ \\sigma = \\sqrt{\\lambda} \\] Uno de los usos que tiene la distribución de Poisson es su aproximación a la distribución binomial cuando se cumplen las siguientes condiciones: Cuando \\(p &lt; 0.1\\) o \\(p &gt; 0.9\\). Cuando \\(n \\ge 20\\). Es todavía mejor si \\(n \\ge 100\\). Cuando \\(np \\le 10\\). Si tenemos estas condiciones, la distribución de Poisson es un muy buen aproximado a la distribución binomial. En estos casos, la media se obtendría de la misma manera que obtenemos la media de la distribución binomial. Debido a esta dependencia de probabilidades muy bajas o muy elevadas, frecuentemente se utiliza la distribución de Poisson para la evaluación de eventos raros. \\[\\begin{equation} \\mu = \\lambda = np \\tag{3.8} \\end{equation}\\] Ejemplo: En Escherichia coli, una célula de cada \\(10^9\\) muta para desarrollar resistencia a la estreptomicina. Si observamos \\(2 \\times 10^9\\) células, ¿Cuál es la probabilidad de que ninguna mute? ¿Cuál es la probabilidad de que al menos una mute? Como con la distribución binomial, R cuenta con funciones específicas para calcular los valores de una distribución de Poisson. dpois() nos da un valor exacto de la distribución de Poisson en el punto indicado. ppois() nos da la probabilidad acumulada de un evento. qpois() toma el valor de probabilidad que le ponemos como primer argumento y nos da como regreso un número cuya probabilidad acumulada empate con el valor de la probabilidad (también conocida como función cuantil). rpois() genera cierta cantidad de número aleatorios de acuerdo con la probabilidad y el número de pruebas realizadas. Como estamos hablando de probabilidades demasiado pequeñas, ya que tenemos que encontrar 1 mutante entre \\(10^9\\) células: \\[ p = \\frac{1}{10^9} = 0.000000001 \\] Para este caso, podemos utilizar la aproximación a la probabilidad binomial: \\[ \\mu = \\lambda = np = (2\\times10^9) \\times \\frac{1}{10^9} = 2 \\] Por lo tanto, \\(\\lambda = 2\\). Observemos que teníamos un valor de \\(n\\) enorme (\\(2 \\times 10^9\\)) y una \\(p\\) extremadamente pequeña (\\(\\frac{1}{10^9}\\)) y a pesar de eso, obtuvimos un valor de \\(\\lambda\\) relativamente pequeño, además de cumplir los supuestos para utilizar la distribución de Poisson como aproximación a la distribución binomial. #Probabilidad de que 1 célula mute. dpois(1, 2, log = F) ## [1] 0.2706706 Figura 3.6: Distribución de Poisson para datos de E. coli. En azul podemos ver la probabilidad de que 1 célula haya mutado. Por lo que la probabilidad de que al menos una célula mute dentro de \\(2 \\times 10^9\\) células de E. coli es de \\(P(X = 1) = 0.2707\\). Para el caso de que ninguna célula mute: #Probabilidad de que ninguna célula mute. dpois(0, 2, log = F) ## [1] 0.1353353 Figura 3.7: Distribución de Poisson para datos de E. coli. En azul podemos ver la probabilidad de que ninguna célula haya mutado. Por lo que \\(P(X = 0) = 0.1353\\). Si por el contrario, quisiéramos encontrar la probabilidad de que al menos una célula mute: #Probabilidad de que al menos una célula mute. 1 - dpois(0, 2, log = F) ## [1] 0.8646647 Por lo que \\(P(X \\ge 1) = 0.865\\). Figura 3.8: Distribución de Poisson para datos de E. coli. En azul podemos ver la probabilidad de que al menos 1 célula haya mutado (que corresponde a los valores mayores a 0. Como podemos ver en la figura 3.8, la probabilidad de obtener 7 o más células mutantes se vuelve prácticamente 0. 3.6 Distribución normal Una distribución normal corresponde a una curva en forma de campana o gaussiana, con ciertas características específicas. Se utiliza para representar la distribución de los valores de una variable \\(X\\), de dos maneras distintas: (1) como una aproximación a un histograma basado en los valores muestreados de la variable \\(X\\) o; (2) como una representación idealizada de la distribución poblacional de \\(X\\). Las curvas con distribución normal toman su forma por dos elementos muy importantes: la media, \\(\\mu\\) y su desviación estándar, \\(\\sigma\\). Cuando se tiene una curva con distribución normal, se expresa de la siguiente manera \\(X \\sim N(\\mu, \\sigma)\\). La fórmula de la distribución normal se encuentra en la fórmula (3.9). No se trata de cualquier curva simétrica, si no de una curva simétrica específica. \\[\\begin{equation} f(x) = \\frac{1}{{\\sigma}{\\sqrt{2 \\pi}}}e^{-\\frac{1}{2}(\\frac{x-\\mu}{\\sigma})^2} \\tag{3.9} \\end{equation}\\] La función \\(f(x)\\) se conoce como función de densidad de probabilidad (PDF) y expresa la altura de la curva como una función de la posición en el eje horizontal. El área bajo la curva acumulada se conoce como función de densidad acumulada (CDF). El centro de una curva normal es \\(x = \\mu\\), los puntos de inflexión están en \\(x = \\mu + \\sigma\\) y \\(x = \\mu - \\sigma\\). En principio la curva se extiende hasta el infinito, pero tres desviaciones estándar de la media hacia el valor negativo o positivo da como resultado valores demasiado pequeños. El ancho y alto de una curva normal están determinados por la desviación estándar \\(\\sigma\\). Figura 3.9: Forma de la distribución normal. Normalmente lo que nos interesa de una curva con distribución normal es el área debajo de la curva. Para esto utilizamos la escala estandarizada, en el cual el valor del eje horizontal se denomina valor \\(Z\\). La escala de \\(Z\\) mide las desviaciones estándar a partir de la media, por ejemplo, \\(z = 1\\) corresponde a una desviación estándar de la media. Para transformar nuestros datos a la escala \\(Z\\) simplemente aplicamos la fórmula (3.10) y a la variable \\(Z\\) se le conoce como una variable de distribución normal estándar, ya que se encuentra estandarizada y no importa en que valor se encuentren los datos originales (kg, °C, cm, mmHg, etc.), la variable \\(Z\\) es adimensional. \\[\\begin{equation} Z = \\frac{X - \\mu}{\\sigma} \\tag{3.10} \\end{equation}\\] Una vez que nuestras variables se encuentran estandarizadas en el valor \\(Z\\), podemos utilizar tablas de \\(Z\\) para realizar el cálculo debajo del área que corresponde al valor \\(Z\\) obtenido, aunque claro, también podemos hacer estos cálculos en R. Hay que tener en cuenta que para una curva de distribución normal estándar: 68% del área se encuentra entre \\(\\pm\\) 1 distribución estándar. 95% del área se encuentra entre \\(\\pm\\) 2 distribuciones estándar. 99.7% del área se encuentra entre \\(\\pm\\) 3 distribuciones estándar. De manera similar a la distribución binomial, en R encontramos comandos similares para la distribución normal, donde tenemos: dnorm() nos da un valor de densidad normal (PDF). pnorm() nos da un valor de densidad normal acumulado hasta cierto punto (área debajo de la curva, CDF). qnorm() toma el valor de densidad normal que le ponemos como primer argumento y nos da como regreso un número cuya densidad normal acumulada empate con el valor de densidad normal ingresado. rnorm() genera cierta cantidad de número aleatorios de acuerdo al valor de densidad normal. Intentemos resolver algunos ejercicios. Ejemplo: En una población de peces de la especie Pomolobus aestivalis, la longitud de los individuos sigue una distribución normal. La media de la longitud es de 54.0 mm, y la desviación estándar es de 4.5 mm2. ¿Qué porcentaje de los peces mide menos de 60 mm? ¿Qué porcentaje de los peces mide más de 51 mm? ¿Qué porcentaje de los peces miden entre 51 mm y 60 mm? Para responder la primer pregunta, debemos transformar nuestros datos a valores Z, ya que se encuentran en mm. Para esto aplicamos la fórmula (3.10). En R es una operación relativamente sencilla de hacer. (60 - 54)/(4.5) ## [1] 1.333333 Como podemos ver, el valor \\(Z\\) de 60 mm es igual a 1.33. Lo siguiente sería encontrar el área bajo la curva que corresponda a este valor Z. Para esto utilizamos la función pnorm(). pnorm(1.33, 0, 1, lower.tail = TRUE) ## [1] 0.9082409 Figura 3.10: Curva de distribución normal con el área sombreada correspondiente a un valor Z = 1.33 De nuevo, el argumento lower.tail = TRUE significa que encontrará la probabilidad acumulada de valores \\(Z\\) menores a 1.33 hasta 1.33. Como podemos ver la probabilidad de que un pez mida menos de 60 mm es de 90.82%. Para la segunda pregunta, primero debemos encontrar el valor de \\(Z\\) correspondiente a 51 mm y después basta con cambiar el argumento lower.tail a FALSE. (51 - 54)/(4.5) ## [1] -0.6666667 pnorm(-0.67, 0, 1, lower.tail = FALSE) ## [1] 0.7485711 Como podemos ver el resultado indica que 75.86% de los peces miden más de 51 mm. Bastante sencillo, ¿no? Figura 3.11: Curva de distribución normal con el área sombreada correspondiente a un valor Z = -0.67, pero partiendo desde el lado positivo de la curva. Ahora para la última pregunta, simplemente calculamos la probabilidad acumulada hasta nuestro valor \\(Z\\) más grande, que en este caso corresponde a 1.33 y le restamos la probabilidad acumulada del valor \\(Z\\) más pequeño, que corresponde -0.67. pnorm(1.33, 0, 1) - pnorm(-0.67, 0, 1) ## [1] 0.656812 Como resultado obtenemos que el 65.68% de los peces se encuentran en longitudes de entre 51 mm y 60 mm. Si nos damos cuenta, la distribución normal puede, de cierta manera, interpretarse como una distribución de probabilidad continua. Figura 3.12: Curva de distribución normal con el área sombreada correspondiente a un intervalo entre Z = -0.67 y Z = 1.33. En algunas ocasiones, queremos encontrar el valor \\(Z\\) correspondiente a un área bajo la curva determinada, para este tipo de ocasiones, utilizamos la función qnorm(). Del ejemplo anterior, supongamos que queremos encontrar el percentil 70 de la distribución de la longitud de los peces. Supongamos que este valor está representado por la variable \\(y\\). En otras palabras, queremos encontrar el valor tal que el 70% de las longitudes de los peces son menores que \\(y\\) y el 30% son mayores. qnorm(0.7, 0, 1) ## [1] 0.5244005 Como podemos ver, el valor \\(Z\\) correspondiente es 0.5244. Ahora, utilizando la fórmula (3.10) podemos realizar un despeje muy sencillo y obtener la fórmula \\(y = Z* \\sigma + \\mu = 0.5244 * 4.5 + 54 = 56.3\\). Esto quiere decir que 56.3 mm es el percentil 70 de la distribución de nuestros datos. Figura 3.13: Curva de distribución normal con el área sombreada correspondiente a un valor Z = -0.67, pero partiendo desde el lado positivo de la curva. 3.6.1 Pruebas de normalidad Ya que muchos procedimientos estadísticos se basan en datos provenientes de una población con distribución normal, es importante saber si nuestros datos siguen está distribución. Uno de los métodos más utilizados son los gráficos cuantil-cuantil, gráficos Q-Q o Q-Q plot. Veamos un ejemplo con datos de plantas que vienen incluidas en ggplot2. data(&quot;PlantGrowth&quot;) PlantGrowth ## weight group ## 1 4.17 ctrl ## 2 5.58 ctrl ## 3 5.18 ctrl ## 4 6.11 ctrl ## 5 4.50 ctrl ## 6 4.61 ctrl ## 7 5.17 ctrl ## 8 4.53 ctrl ## 9 5.33 ctrl ## 10 5.14 ctrl ## 11 4.81 trt1 ## 12 4.17 trt1 ## 13 4.41 trt1 ## 14 3.59 trt1 ## 15 5.87 trt1 ## 16 3.83 trt1 ## 17 6.03 trt1 ## 18 4.89 trt1 ## 19 4.32 trt1 ## 20 4.69 trt1 ## 21 6.31 trt2 ## 22 5.12 trt2 ## 23 5.54 trt2 ## 24 5.50 trt2 ## 25 5.37 trt2 ## 26 5.29 trt2 ## 27 4.92 trt2 ## 28 6.15 trt2 ## 29 5.80 trt2 ## 30 5.26 trt2 Como podemos ver, tenemos tres grupos, el control, tratamiento 1 y tratamiento 2. Vamos a enfocarnos solamente en los datos del grupo del tratamiento 1. Datos &lt;- PlantGrowth %&gt;% dplyr::select(starts_with(&quot;trt1&quot;), weight) Datos ## weight ## 1 4.17 ## 2 5.58 ## 3 5.18 ## 4 6.11 ## 5 4.50 ## 6 4.61 ## 7 5.17 ## 8 4.53 ## 9 5.33 ## 10 5.14 ## 11 4.81 ## 12 4.17 ## 13 4.41 ## 14 3.59 ## 15 5.87 ## 16 3.83 ## 17 6.03 ## 18 4.89 ## 19 4.32 ## 20 4.69 ## 21 6.31 ## 22 5.12 ## 23 5.54 ## 24 5.50 ## 25 5.37 ## 26 5.29 ## 27 4.92 ## 28 6.15 ## 29 5.80 ## 30 5.26 Recordemos que podemos utilizar la librería dplyr para extraer ciertos datos de nuestras matrices de datos. Lo que haremos ahora es utilizar el gráfico Q-Q para comprar los datos de nuestra muestra con unos datos teóricos que siguen una distribución normal. En caso de que nuestros datos se comporten de manera normal, deberíamos de tener casi una línea recta. Para esto necesitaremos la librería ggpubr. library(ggpubr) ggqqplot(Datos$weight) + xlab(&quot;Teórico&quot;) + ylab(&quot;Muestra&quot;) Figura 3.14: Gráfico Q-Q comparando una muestra de datos contra una muestra teórica con distribución normal. Otra opción sería realizar el gráfico de densidad, pero muchas veces es difícil identificar la forma de campana en cierto conjunto de datos, así que los gráficos Q-Q son de gran ayuda. Otra opción para realizar una prueba de normalidad conocida como prueba de Shapiro-Wilks (aunque no se recomienda que para \\(n &gt; 50\\)), pero es muy sensible a ligeras desviaciones de la normalidad, sobre todo con un tamaño de muestra grande. Es muy sencillo de realizar en R, simplemente utilizamos el comando shapiro_test() del paquete rstatix. En este caso, vamos a analizar si nuestros dos tratamientos y el control siguen una distribución normal. Para esto, vamos a hacer uso de la librería dplyr. Aunque también podemos utilizar la función shapiro.test() que viene incluida en R. library(rstatix) ## ## Attaching package: &#39;rstatix&#39; ## The following object is masked from &#39;package:stats&#39;: ## ## filter PlantGrowth %&gt;% group_by(group) %&gt;% shapiro_test(weight) #Agrupamos los datos por tipo de tratamiento y hacemos la prueba a la variable &quot;weight&quot;. ## # A tibble: 3 x 4 ## group variable statistic p ## &lt;fct&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 ctrl weight 0.957 0.747 ## 2 trt1 weight 0.930 0.452 ## 3 trt2 weight 0.941 0.564 ¿Cómo interpretamos los resultados y qué significa el valor-p? Bueno, ya que el control y los dos tratamientos tienen un \\(valor-p &gt; 0.05\\) decimos que se distribuyen de manera normal. Valores-p menores a 0.05 son indicativos fuertes de no-normalidad. Después de obtener un valor menor a 0.05, podríamos corroborar esto con un gráfico Q-Q o con un histograma para ver la forma de la distribución de nuestros datos. 3.7 Distribución muestral La variabilidad entre muestras aleatorias que provienen de una misma población se conoce como variabilidad de muestreo. Una distribución de probabilidad que caracteriza algún aspecto de la variabilidad de muestreo se conoce como distribución muestral. Usualmente los valores de una muestra se parecen a los de la población de la cuál provienen. Una distribución muestral nos indica qué tan cerca la resemblanza entre la muestra y la población es probable que sea. Normalmente tomamos solamente una muestra aleatoria de una población. Pero para visualizar la distribución muestral, necesitamos realizar un meta-estudio, que consiste en repetir de manera indefinida, réplicas del mismo estudio. Por ejemplo, si un estudio consiste en extraer una muestra aleatoria de tamaño \\(n\\) de una población, un meta-estudio consiste en repetir varias veces la extracción de una muestra de tamaño \\(n\\) de una población. Por lo tanto, las probabilidades relativas de una muestra aleatoria se pueden interpretar como frecuencias relativas en un meta-estudio. Conocer la distribución muestral nos permite hacer afirmaciones de probabilidad de otras posibles muestras. Una pregunta natural a realizar es, ¿Qué tan parecida es la media de la muestra \\(\\overline{x}\\) a la media de la población \\(\\mu\\)? Aunque con una sola muestra no podemos responder esta pregunta, si pensamos en un modelo de muestreo aleatorio y tomamos la media muestral como una variable \\(\\overline{X}\\), podemos hacer ciertas inferencias. Reformulamos nuestra pregunta a ¿Qué tan cerca de \\(\\mu\\) es probable que este \\(\\overline{X}\\)? Nuestra respuesta la encontramos en la distribución muestral de \\(\\overline{X}\\). Tenemos que tener en cuenta que, en promedio, la media de la distribución muestral \\(\\overline{X}\\) equivale a la media de la población \\(\\mu\\). Esto se ve mejor en la fórmula (3.11). \\[\\begin{equation} \\mu_{\\overline{X}} = \\mu \\tag{3.11} \\end{equation}\\] La fórmula de la desviación estándar de la muestra es un poco menos intuitiva, aunque si se analiza de manera detallada tiene sentido. \\[\\begin{equation} \\sigma_{\\overline{X}} = \\frac{\\sigma}{\\sqrt{n}} \\tag{3.12} \\end{equation}\\] Mientras el tamaño de muestra incrementa, la desviación estándar de \\(\\overline{X}\\) disminuye. Es decir para muestras más grandes existe menos variación. La forma está determinada por el tamaño de muestra y la naturaleza de la población. Si la población \\(X\\) se distribuye de manera normal, entonces la distribución muestral de \\(\\overline{X}\\) será también normal, sin importar el tamaño de nuestra \\(n\\). Además el teorema del límite central indica que si obtenemos una \\(n\\) suficientemente grande, la distribución muestral de \\(\\overline{X}\\) será aproximadamente normal incluso para muestras cuya población \\(X\\) no se distribuye de manera normal. Ejemplo: Supongamos que tenemos una población del Carbonerito Mexicano (Poecile sclateri) en la cuál el peso medio es de \\(\\mu = 11 g\\) y la desviación estándar \\(\\sigma = 1.2 g\\). Supongamos que tomamos una muestra aleatoria de seis aves. Dejemos que \\(\\overline{x}\\) represente la media del peso de las seis aves. Ya que sabemos que el peso de esta ave sigue una distribución normal en la población, también nuestras muestras seguirán una distribución normal. Para este caso, la media y la desviación estándar de nuestra muestra serán las siguientes: \\[\\begin{equation} \\mu_{\\overline{X}} = \\mu = 11 g\\\\ \\sigma_{\\overline{X}} = \\frac{\\sigma}{\\sqrt{n}} = \\frac{1.2}{\\sqrt{6}} = 0.49 g \\end{equation}\\] En este caso \\(\\mu_{\\overline{X}} = 11 g\\) y \\(\\sigma_{\\overline{X}} = 0.49 g\\). De tal manera que, en promedio la media de la muestra será 11 g, sin embargo, el 68% de las veces \\(\\overline{X}\\) se encontrará entre \\(11g \\pm 0.49g\\) y el 95% de las veces se encontrará entre \\(11g \\pm 0.98g\\). Figura 3.15: Distribución muestral proveniente de una población de Sclateri poecile. Esta distribución muestral expresa distintas posibilidades para los valores de \\(\\overline{X}\\). Supongamos que quisiéramos saber la probabilidad de que la media de una muestra de seis aves sea mayor a 11.5 g. Ya que nuestros datos son normales, podemos usar la transformación a valores \\(Z\\) para obtener nuestro resultado. \\[\\begin{equation} Z = \\frac{\\overline{x}-\\mu_\\overline{X}}{\\sigma_{\\overline{X}}} = \\frac{11.5 - 11}{0.49} = 1.0204 \\end{equation}\\] Ya que nuestro valor \\(Z = 1.0204\\), usamos la función pnorm() para encontrar nuestra área bajo la curva. pnorm(1.0204, mean = 0, sd = 1, lower.tail = FALSE) ## [1] 0.1537694 De hecho, ni siquiera es necesario realizar la transformación a valores \\(Z\\) en R ya que podemos modificar los parámetros de la función pnorm(). pnorm(11.5, mean = 11, sd = 0.49, lower.tail = FALSE) ## [1] 0.1537675 Debido a que hemos redondeado la desviación estándar el resultado es ligeramente distinto (cuestión de decimales). Entonces podemos concluir que: \\[\\begin{equation} P(\\overline{X} &gt; 11.5) = P(Z &gt; 1.0204) = 0.1538 \\approx 0.15 \\end{equation}\\] Figura 3.16: Probabilidad de que la media de una muestra de seis aves de la especie Poecile sclateri sea mayor a 11.5. Si eligiéramos muchas muestras aleatorias provenientes de esta población cerca del 15% de las muestras tendrían una media mayor a 11.5 g. El tamaño de la muestra tiene un efecto directo sobre la forma de nuestra curva. Básicamente, muestras más grandes dan un \\(\\sigma_\\overline{X}\\) menor, y por ende dan un menor error de muestreo. En seguida se muestran distintas gráficas con distintas \\(\\mu_\\overline{X}\\). Para este caso hipotético, \\(\\mu = 100, \\sigma = 40\\). Figura 3.17: Cambios en la forma de la curva dependientes de la desviación estándar muestral. Figura 3.18: Cambios en la forma de la curva dependientes de la desviación estándar muestral. Qué tan cerca esta \\(\\overline{X}\\) de \\(\\mu\\) depende del tamaño de la muestra \\(n\\). La media de una muestra grande no necesariamente está más cerca a la media poblacional que la media de una muestra pequeña, pero existe mayor probabilidad de que lo este. 3.7.1 Poblaciones, muestras y distribuciones muestrales Una vez llegado a este punto puede que exista confusión entre los valores de una población, de una muestra y de una distribución muestral. Para esto aclaremos los siguientes puntos, en torno a una variable \\(X\\). En una población, los estadísticos descriptivos como la media y la desviación estándar se representan por los siguientes símbolos: \\(\\mu\\): media poblacional. \\(\\sigma\\): desviación estándar poblacional. En una muestra, los mismos estadísticos se representan por los siguientes símbolos: \\(\\overline{x}\\): media muestral. \\(s\\): desviación estándar poblacional. En una distribución muestral lo que nosotros hacemos es repetir un muestreo indefinidas veces (meta-estudio) y de cada muestreo extraer la media muestral \\(\\overline{x}\\). Lo que representa la distribución muestral es una distribución de medias, en lugar de observaciones individuales. \\(\\mu_\\overline{X}\\): media de una distribución muestral. \\(\\sigma_\\overline{X}\\): desviación estándar de una distribución muestral. "]]
