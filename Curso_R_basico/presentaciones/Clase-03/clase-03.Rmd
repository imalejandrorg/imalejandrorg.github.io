---
title: "Curso de R b치sico"
author: "Alejandro Ruiz"
output:
  ioslides_presentation:
    widescreen: yes
    incremental: no
    transition: faster
    logo: O:/Im치genes/favicon_io/android-chrome-512x512.png
---

```{r setup, include=FALSE}
library(tidyverse)
library(patchwork)
library(kableExtra)
library(cowplot)
knitr::opts_chunk$set(echo = TRUE)
```

# Clase 3: Probabilidad

## Probabilidad {.build}

La **probabilidad** es una cantidad num칠rica que expresa qu칠 tan factible es que ocurra un evento. Se expresa como $P(A)$ donde $A$ es un evento aleatorio.

Siempre se encuentra en un rango de 0 a 1. Expresado como porcentaje entre 0% y 100%. Existen distintos acercamientos a la probabilidad como el acercamiento **frecuentista** o el acercamiento **bayesiano.**

En este caso, aprenderemos el enfoque frecuentista como una forma de asignar una probabilidad mensurable a un evento, es decir, la ocurrencia de el evento a la larga (qu칠 tan frecuentemente ocurre).

## Probabilidad frecuentista

En este enfoque, la probabilidad de un evento se determina a trav칠s por el n칰mero de veces que el evento $A$ ocurre en una serie de repeticiones indefinidamente largas.

$$
P(A) = \frac{Veces \space que \space ocurre \space A}{N칰mero \space total \space de \space repeticiones}
$$

# Reglas de probabilidad

## Reglas de probabilidad {.build}

1- La probabilidad de que un evento $A$ ocurra est치 siempre entre 0 y 1. 

$$
0 \le P(A) \le 1
$$

2- Siempre y cuando dos eventos sean complementarios, la suma de probabilidades debe ser igual a 1.

$$
P(A) + P(\bar{A}) = 1
$$

3- La probabilidad de que el evento $A$ no ocurra es...

$$
P(\bar{A}) = 1 - P(A)
$$

Donde $P(\bar{A})$ es el **complemento** de $P(A)$.

## Reglas de probabilidad {.build}

4- Si $A$ y $B$ son eventos **disjuntos** (no ocurren en conjunto):

$$
P(A \cup B) = P(A) + P(B)
$$

```{r echo=FALSE, fig.align='center'}
knitr::include_graphics("no_conjuntos.png", dpi = 120)
```

## Reglas de probabilidad {.build}

5- Si $A$ y $B$ son eventos que ocurren en **conjunto:**

$$
P(A \cup B) = P(A) + P(B) - P(A \cap B)
$$

Donde $P(A \cap B)$ es la probabilidad de que ocurra $A$ y $B$ al mismo tiempo.

```{r echo=FALSE, fig.align='center'}
knitr::include_graphics("cojnuntos.png", dpi = 140)
```

## Reglas de probabilidad {.build}

6- Si dos eventos son **independientes,** entonces:

$$
P(A \cap B) = P(A) \times P(B)
$$

## Reglas de probabilidad {.build}

7- Si dos eventos son **dependientes** entonces:

$$
P(A \cap B) = P(A) \times P(B|A)
$$

El valor 洧녞(洧냣|洧냢) corresponde a la probabilidad condicional, y nos dice cu치l es la probabilidad de que el evento $B$ ocurra dado que ha ocurrido ya el evento $A$. Para calcular la probabilidad condicional utilizamos:

$$
P(A|B) = \frac{P(A \cap B)}{P(B)} \\
P(B|A) = \frac{P(A \cap B)}{P(A)} \\
P(A|B) \ne P(B|A)
$$

## Reglas de probabilida {.build}

8- Para dos eventos cualesquiera:

$$
P(A) = P(B) \times P(A|B) + P(\bar{B}) \times P(A|\bar{B})
$$

## Curvas de densidad {.build}

Una **distribuci칩n de probabilidad** corresponde a una descripci칩n que da la probabilidad para cada valor de una variable aleatoria. La variable aleatoria puede ser **discreta** o puede ser **continua.**

Un **histogramas de frecuencias relativas** representa las proporciones de las observaciones en cada categor칤a. 

Para variables continuas normalmente se utilizan clases muy estrechas para representar al histograma como una **curva de densidad.** 

La coordenada $y$ de una curva de densidad representa la escala de densidad y a menudo las frecuencias relativas se representan como **치reas debajo de la curva.**

## Curva de densidad {.build}

Algunos de los requisitos de una distribuci칩n de probabilidad son:

- Que exista una variable aleatoria num칠rica $x$ y que sus valores est칠n asociados a una probabilidad.

- Que $\sum P(x) = 1$, donde $x$ asume todos los los valores posibles. El 치rea total bajo la curva de densidad debe ser igual a 1.

- Que la probabilidad de ver a $x_i$ se encuentre entre 0 y 1. Es decir, $0 \le P(x_i) \le 1$.

## Ejemplos

- **Distribuci칩n normal**

- **Distribuci칩n $t$ de Student** 

- **Distribuci칩n de $F$** 

- **Distribuci칩n de Chi**

## Ejemplos

```{r echo=FALSE, message=FALSE, warning=FALSE, paged.print=FALSE, fig.align='center'}
data <- data.frame("x" = rnorm(100000, mean = 0, sd = 1))
ggplot(data, aes(x = x)) +
  geom_histogram(aes(y=..density..), colour="black", fill="white") +
  geom_density(size = 0.6) +
  ylab("") +
  xlab("") +
  scale_y_continuous(expand = c(0,0), breaks = NULL) +
  theme_minimal_hgrid(color = "black")
```

## Par치metros de una distribuci칩n de probabilidad

- **Media:** $\mu$

- **Varianza:** $\sigma^2$

- **Desviaci칩n est치ndar:** $\sigma$

- **Tama침o de muestra:** $n$

# Distribuci칩n binomial

## Distribuci칩n binomial {.build}

- Distribuci칩n de probabilidad para una **variable discreta.**

- Cuenta el n칰mero de 칠xitos ($k$) tras realizar $n$ veces un experimento. 

- Cada *intento* debe ser independiente entre s칤. La probabilidad de 칠xito es fija entre cada intento y se denota con la letra $p$.

## Distribuci칩n binomial

```{r echo=FALSE, fig.align='center'}
knitr::include_graphics("ejemplos.png", dpi = 120)
```

## Variable aleatoria binomial

1.  **Resultados binarios:** Solamente existen dos posibilidades para cada intento (칠xito o fracaso, cara o cruz, dominante o recesivo, muerto o vivo, ni침o o ni침a, etc.).

2.  **Intentos independientes:** Cada prueba o intento deben ser independientes del anterior.

3.  **El valor de** $n$ es fijo: Se sabe con antelaci칩n el n칰mero de pruebas $n$.

4.  **Mismo valor** $p$: En todos los casos, la probabilidad de 칠xito o fracaso no debe cambiar, es decir $p$ debe permanecer constante.

5.  **Resultados mutuamente excluyentes:** Es decir, no se puede tener 칠xito y fracaso al mismo tiempo.

6.  **Resultados colectivamente exhaustivos:** Al menos uno de los dos resultados debe de ocurrir.

## Funci칩n de masa de probabilidad (PMF) {.smaller}

La **funci칩n de masa de probabilidad (PMF)** o f칩rmula de la distribuci칩n binomial.

$$
P(X = k) = {\binom{n}{k}}{p^{k}}{q^{n-k}}
$$

Donde:

-   $P(X = k)$ = probabilidad de obtener $k$ 칠xitos.

-   $\binom{n}{k}$ = coeficiente binomial.

-   $k$ = n칰mero de 칠xitos.

-   $p$ = probabilidad de 칠xito.

-   $q$ = probabilidad de fracaso.

-   $n$ = n칰mero de pruebas. 

## Coeficiente binomial

Corresponde al n칰mero de formas en las que se puede acomodar un conjunto dado.

$$
_{n}C_{k} = \binom{n}{k} = \frac{n!}{k!(n-k)!}
$$

## Media y desviaci칩n est치ndar de la distribuci칩n binomial {.build}

En este caso la media es el n칰mero promedio de 칠xitos ($k$) y la desviaci칩n est치ndar es qu칠 tan lejos de esta media caen el resto de los valores de la distribuci칩n.

$$
\mu = {n}{p}
$$

$$
\sigma = \sqrt{{n}{p}{(1-p)}} = \sqrt{{n}{p}{q}}
$$

Tambi칠n podemos calcular la varianza...

$$
\sigma^2 = {n}{p}{(1-p)} = {n}{p}{q}
$$

## Ejemplo

Supongamos que analizamos a 5 individuos de una poblaci칩n en la que el 37% de las personas presentan un alelo mutante. Las probabilidades de las distintas configuraciones est치n dadas por la distribuci칩n binomial, en donde $n$ = 5 y $p$ = 0.37. 쮺u치l es la probabilidad de que exactamente 2 personas sean mutantes?

Para esto utilizamos las funciones de R...

## Funciones de la distribuci칩n binomial

- **dbinom()** nos da un valor exacto de la distribuci칩n binomial en el punto indicado.

- **pbinom()** nos da la probabilidad acumulada de un evento.

- **qbinom()** toma el valor de probabilidad que le ponemos como primer argumento y nos da como regreso un n칰mero cuya probabilidad acumulada empate con el valor de la probabilidad (tambi칠n conocida como funci칩n cuantil).

- **rbinom()** genera cierta cantidad de n칰mero aleatorios de acuerdo con la probabilidad y el n칰mero de pruebas realizadas.

## 쮺u치l es la probabilidad de que exactamente 2 personas sean mutantes? {.build}

쯈u칠 funci칩n podr칤amos utilizar...?

**춰춰dbinom()!!**

## 쮺u치l es la probabilidad de que exactamente 2 personas sean mutantes? {.build}

En este caso $n = 5$, $p = 0.37$ y $k = 2$.

```{r}
dbinom(2, 5, 0.37)
```

## 쮺u치l es la probabilidad de que exactamente 2 personas sean mutantes?

```{r echo=FALSE, message=FALSE, warning=FALSE, paged.print=FALSE, fig.align='center'}

mutantes <- data.frame(n = 0:5, prob = dbinom(x = 0:5, size = 5, prob = 0.37))
mutantes$colour <- ifelse(mutantes$n == 2, "lightblue", "white")

ggplot(mutantes, aes(x = factor(n), y = prob, width = 0.9, fill = colour)) +
  geom_bar(stat = "identity", colour = "black", show.legend = F) +
  geom_text(aes(label = round(prob, 2), y = prob + 0.01), position = position_dodge(0.9), size = 3, vjust = 0) +
  labs(x = "Mutantes (x)", y = "Probabilidad") +
  scale_y_continuous(expand = c(0,0), limits = c(0,0.5)) +
  scale_fill_manual(values = c("lightblue", "white")) +
  theme_classic()

```

## Otro ejemplo...

En Estados Unidos, 85% de la poblaci칩n tiene sangre Rh positivo. Supongamos que tomamos 6 personas y contamos cu치ntos tiene Rh positivo. En este caso $Y$ representar치 cu치ntas personas tienen Rh positivo dentro del grupo de 6.

1. 쮺u치l es la probabilidad de $Y$ = 4?

2. 쯏 la probabilidad de que *al menos* 4 personas sean Rh positivo?

3. 쯏 la probabilidad de que haya *al menos* 1 persona con Rh negativo?

## Otro ejemplo... {.build}

Para el punto 1 buscamos una probabilidad **exacta**, $P(Y = 4)$. Es decir podemos utilizar **dbinom().**

```{r}
#Probabilidad de que 4 personas tengan Rh positivo.
dbinom(4, 6, 0.85)
```

```{r echo=FALSE, fig.align='center', fig.height=3, fig.width=5}

rh <- data.frame(rh = 0:6, prob = dbinom(x = 0:6, size = 6, prob = 0.85))
rh$colour <- ifelse(rh$rh == 4, "lightblue", "white")

ggplot(rh, aes(x = factor(rh), y = prob, width = 0.9, fill = colour)) +
 geom_bar(stat = "identity", colour = "black", show.legend = F) +
  geom_text(aes(label = round(prob, 2), y = prob + 0.01), position = position_dodge(0.9), size = 3, vjust = 0) +
  labs(x = "Personas con Rh +", y = "Probabilidad") +
  scale_y_continuous(expand = c(0,0), limits = c(0,0.5)) +
  scale_fill_manual(values = c("lightblue", "white")) +
  theme_classic()


```

## Otro ejemplo... {.build}

Para el punto 2, buscamos que *al menos* 4 personas tengan Rh +, es decir $P(Y \ge 4) = P(Y = 4) + P(Y=5) + P(Y=6)$.

```{r}
#Probabilidad de que al menos 4 personas (pueden ser 4, 5 o 6) tengan Rh positivo.
pbinom(3, 6, 0.85, lower.tail = FALSE)
```

```{r echo=FALSE, fig.align='center', fig.height=3, fig.width=5}

rh <- data.frame(rh = 0:6, prob = dbinom(x = 0:6, size = 6, prob = 0.85))
rh$colour <- ifelse(rh$rh >= 4, "lightblue", "white")

ggplot(rh, aes(x = factor(rh), y = prob, width = 0.9, fill = colour)) +
 geom_bar(stat = "identity", colour = "black", show.legend = F) +
  geom_text(aes(label = round(prob, 2), y = prob + 0.01), position = position_dodge(0.9), size = 3, vjust = 0) +
  labs(x = "Personas con Rh +", y = "Probabilidad") +
  scale_y_continuous(expand = c(0,0), limits = c(0,0.5)) +
  scale_fill_manual(values = c("lightblue", "white")) +
  theme_classic()

```


## Otro ejemplo... {.smaller .build}

Para el punto 3, buscamos que *al menos* una persona tenga Rh -. Por ende buscamos el complemento de nuestro evento, $P(Y = 0) + P(Y = 1) + P(Y = 2) \cdots + P(Y = 5)$, o lo que es lo mismo $1 - P(Y = 6)$. 


```{r}
#Probabilidad de que al menos 1 persona tenga Rh negativo.
dbinom(0, 6, 0.85) + dbinom(1, 6, 0.85) + dbinom(2, 6, 0.85) + dbinom(3, 6, 0.85) + dbinom(4, 6, 0.85) + dbinom(5, 6, 0.85)

1 - dbinom(6, 6, 0.85)

pbinom(5, 6, 0.85)
```

## Otro ejemplo... {.build}

```{r echo=FALSE, fig.align='center'}

rh <- data.frame(rh = 0:6, prob = dbinom(x = 0:6, size = 6, prob = 0.85))
rh$colour <- ifelse(rh$rh < 6, "lightblue", "white")

ggplot(rh, aes(x = factor(rh), y = prob, width = 0.9, fill = colour)) +
 geom_bar(stat = "identity", colour = "black", show.legend = F) +
  geom_text(aes(label = round(prob, 2), y = prob + 0.01), position = position_dodge(0.9), size = 3, vjust = 0) +
  labs(x = "Personas con Rh +", y = "Probabilidad") +
  scale_y_continuous(expand = c(0,0), limits = c(0,0.5)) +
  scale_fill_manual(values = c("lightblue", "white")) +
  theme_classic()

```

# Distribuci칩n de Poisson

## Distribuci칩n de Poisson {.build}

Llamada as칤 en honor al matem치tico franc칠s **Sime칩n Denis Poisson.** Para eventos aleatorios representados en un intervalo de tiempo determinado, aunque tambi칠n es utilizada cuando el evento de inter칠s se distribuye en un espacio plano o tridimensional.

```{r echo=FALSE, fig.align='center'}
knitr::include_graphics("ejemplos2.png", dpi = 150)
```


Nuestro suceso de inter칠s ($X$) tiene valores discretos, mientras que nuestro intervalo tiene valores continuos.

## Funci칩n de masa de probabilidad (PMF) {.build}

La f칩rmula de la distribuci칩n de Poisson (PMF) es...

$$
P(X = k | \lambda) = \frac{e^{-\lambda}\lambda^k}{k!}
$$

Donde:

* $P(X=k|\lambda)$ = n칰mero de observaciones $k$ dado la media $\lambda$.

* $e$ = n칰mero de Euler (aproximadamente 2.7183). 

* $\lambda$ = par치metro de la distribuci칩n de Poisson, corresponde al promedio de veces que ocurre el evento aleatorio.

## Variable aleatoria de Poisson 

Para que una variable sea considerada una **variable aleatoria de Poisson** se deben de cumplir los siguientes requisitos:

1. Cada evento tiene que ser independiente.

2. Las ocurrencias del evento tienen que ser aleatorias.

3. El n칰mero de eventos en un intervalo de tiempo o espacio es infinito ($k = 0, 1, 2, \cdots, \infty$).

4. La probabilidad de que un evento se presente en un intervalo de tiempo es proporcional al intervalo de tiempo o espacio.

5. Se considera que la probabilidad de que dos eventos se presenten en la misma fracci칩n de tiempo es tan peque침a que se puede considerar inexistente.

## Media y desviaci칩n est치ndar de la distribuci칩n de Poisson {.build}

La varianza y la media corresponden al mismo valor ($\lambda$) por lo que la desviaci칩n est치ndar ser칤a...

$$
\sigma = \sqrt{\lambda}
$$

## Aproximaci칩n a la distribuci칩n binomial

Cuando se cumplen las siguientes condiciones:

1. Cuando $p < 0.1$ o $p > 0.9$.

2. Cuando $n \ge 20$. Es todav칤a mejor si $n \ge 100$.

3. Cuando $np \le 10$.

Utilizada para **eventos raros.**

$$
\mu =  \lambda = np
$$

## Ejemplo... {.build}

En *Escherichia coli*, una c칠lula de cada $10^9$ muta para desarrollar resistencia a la estreptomicina. Si observamos $2 \times 10^9$ c칠lulas... 

1. 쮺u치l es la probabilidad de que ninguna mute, $P(X = 0|\lambda)$?

2. 쮺u치l es la probabilidad de que al menos una mute, $P(X \ge 1|\lambda)$?

## Ejemplo... {.build}

Estamos hablando de probabilidades demasiado peque침as, ya que tenemos que encontrar 1 mutante entre $10^9$ c칠lulas:

$$
p = \frac{1}{10^9} = 0.000000001
$$

Para este caso, podemos utilizar la aproximaci칩n a la probabilidad binomial:

$$
\mu = \lambda = np = (2\times10^9) \times \frac{1}{10^9} = 2 \\
$$

Por lo tanto, $\lambda = 2$. Observemos que ten칤amos un valor de $n$ enorme ($2 \times 10^9$) y una $p$ extremadamente peque침a ($\frac{1}{10^9}$) y a pesar de eso, obtuvimos un valor de $\lambda$ relativamente peque침o.

## Funciones de la distribuci칩n de Poisson

Como con la distribuci칩n binomial, **R** cuenta con funciones espec칤ficas para calcular los valores de una distribuci칩n de Poisson.

- **dpois()** nos da un valor exacto de la distribuci칩n de Poisson en el punto indicado.

- **ppois()** nos da la probabilidad acumulada de un evento.

- **qpois()** toma el valor de probabilidad que le ponemos como primer argumento y nos da como regreso un n칰mero cuya probabilidad acumulada empate con el valor de la probabilidad (tambi칠n conocida como funci칩n cuantil).

- **rpois()** genera cierta cantidad de n칰mero aleatorios de acuerdo con la probabilidad y el n칰mero de pruebas realizadas.

## Ejemplo... {.build}

Para el primer punto, $P(X = 0|\lambda)$...

```{r}
#Probabilidad de que ninguna c칠lula mute.
dpois(0, 2)
```

```{r, echo=FALSE, fig.align='center', message=FALSE, warning=FALSE, paged.print=FALSE, fig.height=3, fig.width=5}

Pois <- data.frame(muts = 0:9, prob = dpois(0:9, 2, log = F))
Pois$colour <- ifelse(Pois$muts == 0, "lightblue", "white")

ggplot(Pois, aes(x = factor(muts), y = prob, width = 0.9, fill = colour)) +
  geom_bar(stat = "identity", colour = "black", show.legend = F) +
  geom_text(aes(label = round(prob, 2), y = prob + 0.01), position = position_dodge(0.9), size = 3, vjust = 0) +
  labs(x = "C칠lulas mutantes", y = "Probabilidad") +
  scale_y_continuous(expand = c(0,0), limits = c(0,0.3)) +
  scale_fill_manual(values = c("lightblue", "white")) +
  theme_classic()

```

## Ejemplo... {.build}

Para el punto 2, $P(X \ge 1|\lambda)$...

```{r}
#Probabilidad de que al menos una c칠lula mute.
1 - dpois(0, 2)
```

```{r echo=FALSE, fig.align='center', fig.height=3, fig.width=5, message=FALSE, warning=FALSE, paged.print=FALSE}

Pois <- data.frame(muts = 0:9, prob = dpois(0:9, 2, log = F))
Pois$colour <- ifelse(Pois$muts >= 1, "lightblue", "white")

ggplot(Pois, aes(x = factor(muts), y = prob, width = 0.9, fill = colour)) +
  geom_bar(stat = "identity", colour = "black", show.legend = F) +
  geom_text(aes(label = round(prob, 2), y = prob + 0.01), position = position_dodge(0.9), size = 3, vjust = 0) +
  labs(x = "C칠lulas mutantes", y = "Probabilidad") +
  scale_y_continuous(expand = c(0,0), limits = c(0,0.3)) +
  scale_fill_manual(values = c("lightblue", "white")) +
  theme_classic()

```

# Distribuci칩n normal

## Distribuci칩n normal {.build}

Corresponde a una curva en *forma de campana* o *gaussiana*, con ciertas caracter칤sticas espec칤ficas.

Se utiliza para representar la distribuci칩n de los valores de una variable $X$, de dos maneras distintas: 

* Como una aproximaci칩n a un histograma basado en los valores muestreados de la variable $X$.

* Como una representaci칩n idealizada de la distribuci칩n poblacional de $X$.

## Distribuci칩n normal {.build}

Las curvas con distribuci칩n normal toman su forma por dos elementos muy importantes: la **media**, $\mu$ y su **desviaci칩n est치ndar**, $\sigma$.

Cuando se tiene una curva con distribuci칩n normal, se expresa de la siguiente manera $X \sim N(\mu, \sigma)$.

No se trata de cualquier curva sim칠trica, si no de una curva sim칠trica *espec칤fica,* eso lo veremos en su **funci칩n de densidad de probabilidad (PDF).**

## Funci칩n de densidad de probabilidad (PDF) {.build}

Expresa la altura de la curva como una funci칩n de la posici칩n en el eje horizontal. El 치rea bajo la curva acumulada se conoce como **funci칩n de densidad acumulada (CDF).**

$$
f(x) = \frac{1}{{\sigma}{\sqrt{2 \pi}}}e^{-\frac{1}{2}(\frac{x-\mu}{\sigma})^2}
$$

El centro de una curva normal es $x = \mu$, los puntos de inflexi칩n est치n en $x = \mu + \sigma$ y $x = \mu - \sigma$.

En principio la curva se extiende hasta el infinito, pero tres desviaciones est치ndar de la media hacia el valor negativo o positivo da como resultado valores demasiado peque침os. El ancho y alto de una curva normal est치n determinados por la desviaci칩n est치ndar $\sigma$.

## Distribuci칩n normal

```{r echo=FALSE, fig.align='center', message=FALSE, warning=FALSE, paged.print=FALSE}
ggplot(data.frame(x = c(-4, 4)), aes(x = x)) +
  stat_function(fun = dnorm, size = 0.6) +
  scale_y_continuous(breaks = NULL) +
  ylab("") +
  xlab("") +
  scale_y_continuous(expand = c(0,0), breaks = NULL) +
  scale_x_discrete(limit = c(-3:3), labels = c(expression(-3*sigma), expression(-2*sigma), expression(-sigma), expression(mu), expression(sigma), expression(2*sigma), expression(3*sigma))) +
  theme_minimal_hgrid(color = "black")
```

## Distribuci칩n normal {.build}

Normalmente lo que nos interesa de una curva con distribuci칩n normal es el **치rea debajo de la curva.** Para esto utilizamos la **escala estandarizada,** en el cual el valor del eje horizontal se denomina **valor $Z$.**

La escala de $Z$ mide las desviaciones est치ndar a partir de la media, por ejemplo, $z = 1$ corresponde a una desviaci칩n est치ndar de la media.

```{r echo=FALSE, fig.align='center', message=FALSE, warning=FALSE, paged.print=FALSE, fig.height=3}
ggplot(data.frame(x = c(-4, 4)), aes(x = x)) +
  stat_function(fun = dnorm, size = 0.6) +
  scale_y_continuous(breaks = NULL) +
  ylab("") +
  xlab("") +
  scale_y_continuous(expand = c(0,0), breaks = NULL) +
  scale_x_discrete(limit = c(-3:3), labels = c("-3 z", "-2 z", "-z", expression(mu), "z", "2 z", "3 z")) +
  theme_minimal_hgrid(color = "black")
```

## Escala de $Z$ {.build}

Para transformar nuestros datos a la escala $Z$ simplemente aplicamos la f칩rmula...

$$
Z = \frac{X - \mu}{\sigma}
$$

La variable $Z$ se le conoce como una variable de **distribuci칩n normal est치ndar,** ya que se encuentra estandarizada y no importa en que valor se encuentren los datos originales (kg, 춿C, cm, mmHg, etc.), la variable $Z$ es *adimensional.*

## Escala de $Z$ {.build .smaller}

Una vez que nuestras variables se encuentran estandarizadas en el valor $Z$, podemos utilizar **tablas de $Z$** para realizar el c치lculo debajo del 치rea que corresponde al valor $Z$ obtenido, aunque claro, tambi칠n podemos hacer estos c치lculos en **R.**

```{r echo=FALSE, fig.align='center'}
knitr::include_graphics("tabla_z.png", dpi = 150)
```

## Propiedades de una curva normal est치ndar

-  68% del 치rea se encuentra entre $\pm$ 1 distribuci칩n est치ndar.

-  95% del 치rea se encuentra entre $\pm$ 2 distribuciones est치ndar.

-  99.7% del 치rea se encuentra entre $\pm$ 3 distribuciones est치ndar.

## Funciones de la distribuci칩n normal

- **dnorm()** nos da un valor de densidad normal (PDF).

- **pnorm()** nos da un valor de densidad normal acumulado hasta cierto punto (치rea debajo de la curva, CDF).

- **qnorm()** toma el valor de densidad normal que le ponemos como primer argumento y nos da como regreso un n칰mero cuya densidad normal acumulada empate con el valor de densidad normal ingresado.

- **rnorm()** genera cierta cantidad de n칰mero aleatorios de acuerdo al valor de densidad normal.

## Ejemplo... {.build}

En una poblaci칩n de peces de la especie *Pomolobus aestivalis*, la longitud de los individuos sigue una distribuci칩n normal. La media de la longitud es de $\bar{x} = 54.0 \space mm$, y la desviaci칩n est치ndar es de $s = 4.5 \space mm^2$. 

1. 쯈u칠 porcentaje de los peces mide menos de 60 mm?

2. 쯈u칠 porcentaje de los peces mide m치s de 51 mm?

3. 쯈u칠 porcentaje de los peces miden entre 51 mm y 60 mm?

## 쯈u칠 porcentaje de los peces mide menos de 60 mm? {.build}

Para responder la primer pregunta, debemos transformar nuestros datos a valores $Z$, ya que se encuentran en mm.

```{r}
(60 - 54)/(4.5)
```

Ya que $z = 1.33$, ahora buscamos el 치rea bajo la curva con la funci칩n **pnorm().**

```{r}
pnorm(1.33, 0, 1, lower.tail = TRUE)
```

## 쯈u칠 porcentaje de los peces mide menos de 60 mm? {.build .smaller}

Como podemos ver la probabilidad de que un pez mida menos de 60 mm es de **90.82%.**

```{r echo=FALSE, message=FALSE, warning=FALSE, paged.print=FALSE, fig.align='center'}
ggplot(data.frame(x = c(-4, 4)), aes(x = x)) +
  stat_function(fun = dnorm, geom = "area", fill = "lightblue", xlim = c(-4, 1.33)) +
  stat_function(fun = dnorm, size = 0.6) +
  geom_segment(aes(x = 1.33, y = 0, xend = 1.33, yend = 0.165), linetype = "dashed") +
  #geom_vline(aes(xintercept = 1.33), linetype = "dashed", size = 0.35) +
  geom_text(aes(x = 2.2, label = "츼rea = 0.9082", y = 0.26), color = "red", vjust = -1.2, size = 4.5) +
  geom_text(aes(x = 2.25, label = "z = 1.33", y = 0.26), color = "black", vjust = -3, size = 4.5) +
  xlim(-4, 4) +
  ylab("") +
  xlab("") +
  scale_y_continuous(expand = c(0,0), breaks = NULL) +
  scale_x_discrete(limit = c(-3:3)) +
  theme_minimal_hgrid(color = "black")
```

## 쯈u칠 porcentaje de los peces mide m치s de 51 mm? {.build}

Para la segunda pregunta, primero debemos encontrar el valor de $z$ correspondiente a 51 mm y despu칠s basta con cambiar el argumento **lower.tail** a **FALSE.**

```{r}
(51 - 54)/(4.5)

pnorm(-0.67, 0, 1, lower.tail = FALSE)
```

## 쯈u칠 porcentaje de los peces mide m치s de 51 mm? {.build .smaller}

El resultado indica que **75.86%** de los peces miden m치s de 51 mm. Bastante sencillo, 쯡o?

```{r echo=FALSE, message=FALSE, warning=FALSE, paged.print=FALSE, fig.align='center'}
ggplot(data.frame(x = c(-4, 4)), aes(x = x)) +
  stat_function(fun = dnorm, geom = "area", fill = "lightblue", xlim = c(-0.67, 4)) +
  stat_function(fun = dnorm, size = 0.6) +
  geom_segment(aes(x = -0.67, y = 0, xend = -0.67, yend = 0.319), linetype = "dashed") +
  #geom_vline(aes(xintercept = -0.67), linetype = "dashed", size = 0.35) +
  geom_text(aes(x = -1.8, label = "츼rea = 0.7486", y = 0.26), color = "red", vjust = -1.2, size = 4.5) +
  geom_text(aes(x = -1.7, label = "Z = -0.67", y = 0.26), color = "black", vjust = -3, size = 4.5) +
  xlim(-4, 4) +
  ylab("") +
  xlab("") +
  scale_y_continuous(expand = c(0,0), breaks = NULL) +
  scale_x_discrete(limit = c(-3:3)) +
  theme_minimal_hgrid(color = "black")
```

## 쯈u칠 porcentaje de los peces miden entre 51 mm y 60 mm? {.build}

Simplemente calculamos la probabilidad acumulada hasta nuestro valor $z$ m치s grande, que en este caso corresponde a 1.33 y le restamos la probabilidad acumulada del valor $z$ m치s peque침o, que corresponde -0.67.

```{r}
pnorm(1.33, 0, 1) - pnorm(-0.67, 0, 1)
```

## 쯈u칠 porcentaje de los peces miden entre 51 mm y 60 mm? {.build .smaller}

Como resultado obtenemos que el 65.68% de los peces se encuentran en longitudes de entre 51 mm y 60 mm.

```{r echo=FALSE, message=FALSE, warning=FALSE, paged.print=FALSE, fig.align='center'}
ggplot(data.frame(x = c(-4, 4)), aes(x = x)) +
  stat_function(fun = dnorm, geom = "area", fill = "lightblue", xlim = c(-0.67, 1.33)) +
  stat_function(fun = dnorm, size = 0.6) +
  geom_segment(aes(x = -0.67, y = 0, xend = -0.67, yend = 0.319), linetype = "dashed") +
  geom_segment(aes(x = 1.33, y = 0, xend = 1.33, yend = 0.165), linetype = "dashed") +
  #geom_vline(aes(xintercept = -0.67), linetype = "dashed", size = 0.35) +
  #geom_vline(aes(xintercept = 1.33), linetype = "dashed", size = 0.35) +
  geom_text(aes(x = -1.8, label = "츼rea = 0.6568", y = 0.26), color = "red", vjust = -1.2, size = 4.5) +
  geom_text(aes(x = -1.7, label = "Z = -0.67", y = 0.26), color = "black", vjust = -3, size = 4.5) +
  geom_text(aes(x = 1.9, label = "Z = 1.33", y = 0.26), color = "black", vjust = -3, size = 4.5) +
  xlim(-4, 4) +
  ylab("") +
  xlab("") +
  scale_y_continuous(expand = c(0,0), breaks = NULL) +
  scale_x_discrete(limit = c(-3:3)) +
  theme_minimal_hgrid(color = "black")
```

## La otra interpretaci칩n de la distribuci칩n normal {.build}

Si nos damos cuenta, la distribuci칩n normal puede, de cierta manera, interpretarse tambi칠n como una **distribuci칩n de probabilidad continua.**

```{r echo=FALSE, message=FALSE, warning=FALSE, paged.print=FALSE, fig.align='center'}
ggplot(data.frame(x = c(-4, 4)), aes(x = x)) +
  stat_function(fun = dnorm, geom = "area", fill = "lightblue", xlim = c(-4, 1.33)) +
  stat_function(fun = dnorm, size = 0.6) +
  geom_segment(aes(x = 1.33, y = 0, xend = 1.33, yend = 0.165), linetype = "dashed") +
  xlim(-4, 4) +
  ylab("") +
  xlab("") +
  scale_y_continuous(expand = c(0,0), breaks = NULL) +
  scale_x_discrete(limit = c(-3:3)) +
  theme_minimal_hgrid(color = "black")
```

## qnorm() {.build}

Supongamos que queremos encontrar el percentil 70 de la distribuci칩n de la longitud de los peces. Supongamos que este valor est치 representado por la variable $y$. En otras palabras, queremos encontrar el valor tal que el 70% de las longitudes de los peces son menores que $y$ y el 30% son mayores.

```{r}
qnorm(0.7, 0, 1)
```

Como podemos ver, el valor $z$ correspondiente es $z = 0.5244$. Podemos realizar un despeje muy sencillo y obtener la f칩rmula $y = Z \times \sigma + \mu = 0.5244 \times 4.5 + 54 = 56.3$. Esto quiere decir que 56.3 mm es el percentil 70 de la distribuci칩n de nuestros datos.

## qnorm () {.build}

```{r echo=FALSE, fig.align='center', message=FALSE, warning=FALSE, paged.print=FALSE}
ggplot(data.frame(x = c(-4, 4)), aes(x = x)) +
  stat_function(fun = dnorm, geom = "area", fill = "lightblue", xlim = c(0.5244, 4)) +
  stat_function(fun = dnorm, size = 0.6) +
  geom_segment(aes(x = 0.5244, y = 0, xend = 0.5244, yend = 0.347), linetype = "dashed") +
  geom_text(aes(x = 1.8, label = "츼rea = 0.3", y = 0.26), color = "red", vjust = -1.2, size = 4.5) +
  geom_text(aes(x = 2.18, label = "Z = 0.5244", y = 0.26), color = "black", vjust = -3, size = 4.5) +
  xlim(-4, 4) +
  ylab("") +
  xlab("") +
  scale_y_continuous(expand = c(0,0), breaks = NULL) +
  scale_x_discrete(limit = c(-3:3)) +
  theme_minimal_hgrid(color = "black")
```

# Pruebas de normalidad

## Pruebas de normalidad {.build}

Ya que muchos procedimientos estad칤sticos se basan en datos provenientes de una poblaci칩n con distribuci칩n normal, es importante saber si nuestros datos siguen est치 distribuci칩n.

Uno de los m칠todos m치s utilizados son los **gr치ficos cuantil-cuantil**, **gr치ficos Q-Q** o **Q-Q plot**. Veamos un ejemplo con datos de plantas que vienen incluidas en **ggplot2.**

## Pruebas de normalidad {.build}

```{r}
data("PlantGrowth")
PlantGrowth
```

## Gr치ficos Q-Q {.build}

Vamos a enfocarnos solamente en los datos del grupo control.

```{r}
control <- PlantGrowth %>% dplyr::filter(group == "ctrl")
```

Ahora vamos a hacer uso de la librer칤a **ggpubr.**

## Gr치ficos Q-Q {.build}

```{r echo=TRUE, fig.align='center', message=FALSE, warning=FALSE, paged.print=FALSE, fig.height=4, fig.width=5}
library(ggpubr)

ggqqplot(control$weight) +
  labs(x = "Muestra", y = "Te칩rico")
```

## Prueba de Shapiro-Wilks {.build}

Otra opci칩n para realizar una prueba de normalidad conocida como **prueba de Shapiro-Wilks** (aunque no se recomienda que para $n > 50$). Es muy sensible a ligeras desviaciones de la normalidad, sobre todo con un tama침o de muestra grande.

```{r}
#Indicamos nuestra variable con el s칤mbolo de $.
shapiro.test(control$weight)
```

쯈u칠 pasa con el valor p?

## Prueba de Shapiro-Wilks {.build}

Valores p menores a 0.05 son indicativos fuertes de no-normalidad. 

Despu칠s de obtener un valor menor a 0.05, podr칤amos corroborar esto con un gr치fico Q-Q o con un histograma para ver la forma de la distribuci칩n de nuestros datos.

```{r echo=FALSE, message=FALSE, warning=FALSE, paged.print=FALSE, fig.align='center', fig.height=3}
ggplot(control, aes(x = weight)) +
  geom_histogram(binwidth = 0.5, color = "grey20", fill = "lightblue") +
  labs(x = "Peso", y = "Frecuencia") +
  theme_classic()
```

# Distribuci칩n muestral

## Distribuci칩n muestral {.build}

La variabilidad entre muestras aleatorias que provienen de una misma poblaci칩n se conoce como **variabilidad de muestreo.**

Una distribuci칩n de probabilidad que caracteriza alg칰n aspecto de la variabilidad de muestreo se conoce como **distribuci칩n muestral.**

Usualmente los valores de una muestra se parecen a los de la poblaci칩n de la cu치l provienen.

Una distribuci칩n muestral nos indica qu칠 tan cerca la resemblanza entre la muestra y la poblaci칩n es probable que sea.

## Distribuci칩n muestral {.build}

Imaginemos que muestreamos de una poblaci칩n de *Chondestes grammacus.*

```{r echo=FALSE, message=FALSE, warning=FALSE, paged.print=FALSE, fig.align='center'}
knitr::include_graphics("dist_muestral.png", dpi = 120)
```

## Distribuci칩n muestral

Ahora imaginemos que hacemos el muestreo 50 veces... O tal vez 100, 쯆 200?

```{r echo=FALSE, message=FALSE, warning=FALSE, paged.print=FALSE, fig.align='center'}
knitr::include_graphics("dist_muestral.png", dpi = 120)
```

## Distribuci칩n muestral {.build}

Normalmente tomamos solamente una muestra aleatoria de una poblaci칩n. Pero para visualizar la distribuci칩n muestral, necesitamos realizar un **meta-estudio,** que consiste en repetir de manera indefinida, r칠plicas del mismo estudio.

Si un estudio consiste en extraer una muestra aleatoria de tama침o $n$ de una poblaci칩n, un meta-estudio consiste en repetir varias veces la extracci칩n de una muestra de tama침o $n$ de una poblaci칩n.

Las probabilidades relativas de una muestra aleatoria se pueden interpretar como frecuencias relativas en un meta-estudio. Conocer la distribuci칩n muestral nos permite hacer afirmaciones de probabilidad de otras posibles muestras.

## 쯈u칠 tan parecidas son $\overline{X}$ y $\mu$?

Si pensamos en un modelo de muestreo aleatorio y tomamos la media muestral como una variable $\overline{X}$, podemos hacer ciertas inferencias.

Reformulamos nuestra pregunta a 쯈u칠 tan cerca de $\mu$ es probable que este $\overline{X}$? Nuestra respuesta la encontramos en la **distribuci칩n muestral de $\overline{X}$.**

En promedio, la **media de la distribuci칩n muestral** $\overline{X}$ equivale a la media de la poblaci칩n $\mu$.

$$
\mu_{\overline{X}} = \mu
$$

## La desviaci칩n est치ndar de una distribuci칩n muestral {.build .smaller}

La f칩rmula de la **desviaci칩n est치ndar de la muestra** es un poco menos intuitiva, aunque si se analiza de manera detallada tiene sentido.

$$
\sigma_{\overline{X}} = \frac{\sigma}{\sqrt{n}}
$$

Mientras el tama침o de muestra incrementa, la desviaci칩n est치ndar de $\overline{X}$ disminuye. Es decir para muestras m치s grandes existe menos variaci칩n.

La **forma** est치 determinada por el tama침o de muestra y la naturaleza de la poblaci칩n. Si la poblaci칩n $X$ se distribuye de manera normal, entonces la distribuci칩n muestral de $\overline{X}$ ser치 tambi칠n normal, sin importar el tama침o de nuestra $n$.

## Teorema del limite central {.build}

Indica que si obtenemos una $n$ suficientemente grande, la distribuci칩n muestral de $\overline{X}$ ser치 aproximadamente normal incluso para muestras cuya poblaci칩n $X$ no se distribuye de manera normal.

## Ejemplo con los gorriones arlequ칤n (*Chondestes grammacus*) {.build}

Supongamos que tenemos una poblaci칩n del Gorri칩n Arlequ칤n (*Chondestes grammacus*) en la cu치l el peso medio es de $\mu = 11 \space g$ y la desviaci칩n est치ndar $\sigma = 1.2 \space g$.

Tomamos una muestra aleatoria de seis aves ($n = 6$). Dejemos que $\overline{x}$ represente la media del peso de las seis aves. Ya que sabemos que el peso de esta ave sigue una distribuci칩n normal en la poblaci칩n, tambi칠n nuestras muestras seguir치n una distribuci칩n normal.

$$
\mu_{\overline{X}} = \mu = 11 \space g\\
\sigma_{\overline{X}} = \frac{\sigma}{\sqrt{n}} = \frac{1.2}{\sqrt{6}} = 0.49 \space g
$$

## Ejemplo con los gorriones arlequ칤n {.build} 

En este caso $\mu_{\overline{X}} = 11 \space g$ y $\sigma_{\overline{X}} = \space 0.49 g$. De tal manera que, en promedio la media de la muestra ser치 11 g, sin embargo, el 68% de las veces $\overline{X}$ se encontrar치 entre $11 \space g \pm 0.49 \space g$ y el 95% de las veces se encontrar치 entre $11 \space g \pm 0.98g$.

```{r echo=FALSE, message=FALSE, warning=FALSE, paged.print=FALSE, fig.align='center', fig.height=3}
ggplot(data.frame(x = c(9, 13)), aes(x = x)) +
  stat_function(fun = dnorm, args = list(mean = 11, sd = 0.49), size = 0.6) +
  geom_vline(xintercept = 11, linetype = "dashed") +
  xlim(9, 13) +
  ylab("") +
  xlab("Media de la muestra (g)") +
  scale_y_continuous(expand = c(0,0), breaks = NULL) +
  scale_x_continuous(breaks = c(9.53, 10.02, 10.51, 11, 11.49, 11.98, 12.47)) +
  theme_minimal_hgrid(color = "black")
```

## Ejemplo con los gorriones arlequ칤n {.build} 

Esta distribuci칩n muestral expresa distintas posibilidades para los valores de $\overline{X}$.

Supongamos que quisi칠ramos saber la probabilidad de que la media de una muestra de seis aves sea mayor a 11.5 g. Ya que nuestros datos son
normales, podemos usar la transformaci칩n a valores $z$ para obtener nuestro resultado.

$$
z = \frac{\overline{x}-\mu_\overline{X}}{\sigma_{\overline{X}}} = \frac{11.5 \space g - 11 \space g}{0.49 \space g} = 1.0204
$$

## Ejemplo con los gorriones arlequ칤n {.build} 

Ya que nuestro valor $z = 1.0204$, usamos la funci칩n **pnorm()** para encontrar nuestra 치rea bajo la curva.

```{r}
pnorm(1.0204, mean = 0, sd = 1, lower.tail = FALSE)
```

De hecho, ni siquiera es necesario realizar la transformaci칩n a valores $z$ en **R** ya que podemos modificar los par치metros de la funci칩n **pnorm()**.

```{r}
pnorm(11.5, mean = 11, sd = 0.49, lower.tail = FALSE)
```

## Ejemplo con los gorriones arlequ칤n {.build .smaller} 

Si eligi칠ramos muchas muestras aleatorias provenientes de esta poblaci칩n cerca del 15% de las muestras tendr칤an una media mayor a 11.5 g. 

$$
P(\overline{X} > 11.5) = P(Z > 1.0204) = 0.1538 \approx 0.15
$$

```{r echo=FALSE, message=FALSE, warning=FALSE, paged.print=FALSE, fig.align='center', fig.height=3}

ggplot(data.frame(x = c(9, 13)), aes(x = x)) +
  stat_function(fun = dnorm, args = list(mean = 11, sd = 0.49), geom = "area", fill = "lightblue", xlim = c(11.5,13)) +
  stat_function(fun = dnorm, args = list(mean = 11, sd = 0.49), size = 0.6) +
  geom_segment(aes(x = 11.5, y = 0, xend = 11.5, yend = 0.484), linetype = "dashed") +
  #geom_vline(aes(xintercept = 11.5), linetype = "dashed", size = 0.35) +
  geom_text(aes(x = 12, label = "츼rea = 0.1538", y = 0.4), color = "red", vjust = -0.8, size = 4.5) +
  geom_text(aes(x = 12.08, label = "z = 1.0204", y = 0.4), color = "black", vjust = -2.6, size = 4.5) +
  xlim(9, 13) +
  ylab("") +
  xlab("Media de la muestra (g)") +
  scale_y_continuous(expand = c(0,0), breaks = NULL) +
  scale_x_continuous(breaks = c(9.53, 10.02, 10.51, 11, 11.49, 11.98, 12.47)) +
  theme_minimal_hgrid(color = "black")

```

## El tama침o de la muestra {.build}

El **tama침o de la muestra** tiene un efecto directo sobre la forma de nuestra curva. B치sicamente, muestras m치s grandes dan un $\sigma_\overline{X}$ menor, y por ende dan un menor error de muestreo. 

En seguida se muestran distintas gr치ficas con distintas $\mu_\overline{X}$. Para este caso hipot칠tico, $\mu = 100, \sigma = 40$.

```{r echo=FALSE, message=FALSE, warning=FALSE, paged.print=FALSE, fig.align='center', fig.height=3}
library(patchwork)
p1 <- ggplot(data.frame(x = c(40, 160)), aes(x = x)) +
  stat_function(fun = dnorm, args = list(mean = 100, sd = 20), size = 0.6) +
  geom_text(aes(x = 150, y = 0.015, label = "n == 4"), parse = TRUE) +
  geom_text(aes(x = 150, y = 0.0135, label = "sigma[bar(X)] == 20"), parse = TRUE) +
  xlim(40, 160) +
  ylab("") +
  xlab("") +
  scale_y_continuous(expand = c(0,0), breaks = NULL) +
  theme_minimal_hgrid(color = "black")

p2 <- ggplot(data.frame(x = c(40, 160)), aes(x = x)) +
  stat_function(fun = dnorm, args = list(mean = 100, sd = 10), size = 0.6) +
  geom_text(aes(x = 140, y = 0.029, label = "n == 16"), parse = TRUE) +
  geom_text(aes(x = 140, y = 0.026, label = "sigma[bar(X)] == 10"), parse = TRUE) +
  xlim(40, 160) +
  ylab("") +
  xlab("") +
  scale_y_continuous(expand = c(0,0), breaks = NULL) +
  theme_minimal_hgrid(color = "black")

p1 + p2
```

## El tama침o de la muestra {.build}

Qu칠 tan cerca esta $\overline{X}$ de $\mu$ depende del tama침o de la muestra $n$.

La media de una muestra grande no necesariamente est치 m치s cerca a la media poblacional que la media de una muestra peque침a, pero existe mayor probabilidad de que lo este.

## Poblaciones, muestras y distribuciones muestrales {.build .smaller}

En una **poblaci칩n**, los estad칤sticos descriptivos como la media y la desviaci칩n est치ndar se representan por los siguientes s칤mbolos:

- $\mu$: media poblacional.   

- $\sigma$: desviaci칩n est치ndar poblacional.

En una **muestra**, los mismos estad칤sticos se representan por los siguientes s칤mbolos:

- $\overline{x}$: media muestral.

- $s$: desviaci칩n est치ndar poblacional.

En una **distribuci칩n muestral** lo que nosotros hacemos es repetir un muestreo indefinidas veces y de cada muestreo extraer la media muestral $\overline{x}$. La distribuci칩n muestral representa una distribuci칩n de medias.

- $\mu_\overline{X}$: media de una distribuci칩n muestral.

- $\sigma_\overline{X}$: desviaci칩n est치ndar de una distribuci칩n muestral.

## Poblaciones, muestras y distribuciones muestrales {.build} 

```{r echo=FALSE}
knitr::include_graphics("dist_muestra2.png")
```

